<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <meta name="description" content="这里是 Hilda 的个人博客，与你一起发现更大的世界 | 要做一个有 swag 的程序员">
    <meta name="keywords" content="Hilda">
    <meta name="theme-color" content="#000000">

    <!-- Open Graph -->
    <meta property="og:title"
        content="【AI思想启蒙14】深度学习第4篇-一文吃透神经网络训练的 8 大核心难题  - Hilda的博客 | Your genius girlfriend's blog">
    
    <meta property="og:type" content="article">
    <meta property="og:description" content="

">
    
    <meta property="article:published_time" content=" 2025-11-25T00:00:00Z">
    
    
    <meta property="article:author" content="Hilda">
    
    
    <meta property="article:tag" content="AI思想启蒙">
    
    
    <meta property="og:image" content="https://kirsten-1.github.io">
    <meta property="og:url" content="https://kirsten-1.github.io/2025/11/25/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9914/">
    <meta property="og:site_name" content="Hilda的博客 | Your genius girlfriend's blog">

    <title>【AI思想启蒙14】深度学习第4篇-一文吃透神经网络训练的 8 大核心难题  - Hilda的博客 | Your genius girlfriend's blog</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">

    <!-- Canonical URL -->
    <link rel="canonical" href="https://kirsten-1.github.io/2025/11/25/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9914/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href=" /css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href=" /css/hux-blog.min.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet"
        type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>

    <!-- Google AdSense -->
    <script data-ad-client="ca-pub-6487568398225121" async
        src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->

    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="/">Hilda</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div id="huxblog_navbar">
                <div class="navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="/">Home</a>
                        </li>
                        
                        
                        
                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="/archive/">Archive</a>
                        </li>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <li class="search-icon">
                            <a href="javascript:void(0)">
                                <i class="fa fa-search"></i>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <script>
        // Drop Bootstarp low-performance Navbar
        // Use customize navbar with high-quality material design animation
        // in high-perf jank-free CSS3 implementation
        var $body = document.body;
        var $toggle = document.querySelector('.navbar-toggle');
        var $navbar = document.querySelector('#huxblog_navbar');
        var $collapse = document.querySelector('.navbar-collapse');

        var __HuxNav__ = {
            close: function () {
                $navbar.className = " ";
                // wait until animation end.
                setTimeout(function () {
                    // prevent frequently toggle
                    if ($navbar.className.indexOf('in') < 0) {
                        $collapse.style.height = "0px"
                    }
                }, 400)
            },
            open: function () {
                $collapse.style.height = "auto"
                $navbar.className += " in";
            }
        }

        // Bind Event
        $toggle.addEventListener('click', function (e) {
            if ($navbar.className.indexOf('in') > 0) {
                __HuxNav__.close()
            } else {
                __HuxNav__.open()
            }
        })

        /**
         * Since Fastclick is used to delegate 'touchstart' globally
         * to hack 300ms delay in iOS by performing a fake 'click',
         * Using 'e.stopPropagation' to stop 'touchstart' event from 
         * $toggle/$collapse will break global delegation.
         * 
         * Instead, we use a 'e.target' filter to prevent handler
         * added to document close HuxNav.  
         *
         * Also, we use 'click' instead of 'touchstart' as compromise
         */
        document.addEventListener('click', function (e) {
            if (e.target == $toggle) return;
            if (e.target.className == 'icon-bar') return;
            __HuxNav__.close();
        })
    </script>
    <!-- Search -->
<div class="search-page">
  <div class="search-icon-close-container">
    <span class="search-icon-close">
      <i class="fa fa-chevron-down"></i>
    </span>
  </div>
  <div class="search-main container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <form></form>
        <input type="text" id="search-input" placeholder="$ grep...">
        </form>
        <div id="search-results" class="mini-post-list"></div>
      </div>
    </div>
  </div>
</div>

    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-2015.jpg" width="0" height="0"> -->

<!-- Post Header -->



<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/post-bg-2015.jpg');
        background: ;
    }

    
</style>




<header class="intro-header" >

    <div class="header-mask"></div>
    
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/archive/?tag=AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%99" title="AI思想启蒙">AI思想启蒙</a>
                        
                    </div>
                    <h1>【AI思想启蒙14】深度学习第4篇-一文吃透神经网络训练的 8 大核心难题 </h1>
                    
                    <h2 class="subheading">小批量SGD无偏但有噪声，适中batch size平衡精度与效率；小batch噪声助逃尖锐最小值/鞍点；动量抑震荡、加速，Adam自适应+偏差修正最优；BatchNorm稳定分布、允许大学习率，但小batch失效；L2正则防过拟合，L1具有剪枝效果。</h2>
                    <span class="meta">Posted by Hilda on November 25, 2025</span>
                </div>
            </div>
        </div>
    </div>
</header>







<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>

<h1 id="1梯度下降基础问题">1.梯度下降基础问题</h1>

<p>回顾梯度下降：在训练神经网络时，我们的目标是最小化整个训练集上的<strong>总损失函数</strong> \(\mathcal{L}(W)\)：</p>

\[\mathcal{L}(W) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{(i)}(W)\]

<p>其中\(W\) 是所有模型的权重，\(N\) 是训练集中的总样本数。</p>

<p><strong>目标是计算总梯度的平均值：</strong> \(\frac{\partial \mathcal{L}}{\partial W} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial \mathcal{L}_{(i)}}{\partial W}\)。</p>

<p>由于N往往非常大（百万甚至上亿），每次迭代都计算全部N个样本的梯度是<strong>不可行</strong>且<strong>耗时</strong>的。因此，我们使用 SGD 的小批量变体。</p>

<h2 id="11小批量随机梯度下降-mini-batch-sgd-的梯度">1.1小批量随机梯度下降 (Mini-Batch SGD) 的梯度</h2>

<p><strong>小批量 SGD 的梯度</strong></p>

\[\frac{\partial \text{Loss}_{SGD}}{\partial W} = \frac{1}{\text{batch\_size}} \sum_{i \in \text{batch}} \frac{\partial \text{Loss}_{(i)}}{\partial W}\]

<blockquote>
  <p>为了最大程度的应用GPU等硬件资源，batch size通常为32，64，128，256等等数值。</p>
</blockquote>

<p>在每次迭代中，我们从训练集中随机抽取一个大小为 \(B = \text{batch\_size}\) 的子集（一个批次）。**我们用这个小批次样本的平均梯度，来近似整个训练集的总平均梯度。另外，为了充分利用 GPU 等硬件的并行计算能力，通常需要增加 Batch Size。</p>

<p>小批量 SGD 的成功依赖于<strong>无偏估计原理</strong>：通过随机抽样，我们只需要计算一小部分样本的平均梯度，就能以极高的计算效率，沿着<strong>统计学上正确</strong>的方向进行优化。这使得深度神经网络的大规模训练成为可能。</p>

<h2 id="12统计上相等的原理无偏估计">1.2统计上相等的原理（无偏估计）</h2>

\[E\left(\frac{1}{\text{batch\_size}} \sum_{i \in \text{batch}} \frac{\partial \text{Loss}_{(i)}}{\partial W}\right) = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial \text{Loss}_{(i)}}{\partial W}\]

<p>左式中\(E(\cdot)\) 代表<strong>期望值（Expected Value）</strong>。左侧是小批量梯度的<strong>期望值</strong>。</p>

<p>右侧是整个训练集的<strong>真实平均梯度</strong>（即批量梯度下降 BGD 的梯度）。</p>

<p>这个等式表明，虽然任何一个<strong>单独的小批量梯度</strong>都可能与真实的总梯度有所偏差（这是随机性造成的），但如果我们对所有可能的小批量梯度取平均（即求期望），那么这个平均值<strong>精确等于</strong>真实的总梯度。</p>

<p>在统计学中，一个估计量（这里是小批量梯度）的期望值如果等于它所估计的真实参数（这里是真实总梯度），那么这个估计量就是<strong>无偏的（Unbiased）</strong>。每次迭代我们都在向<strong>正确的方向</strong>前进，即使每一步都有噪声。由于期望值是正确的，只要我们进行足够多的迭代，这些噪声就会相互抵消，最终会收敛到真实的总梯度方向附近。</p>

<ul>
  <li>批量梯度下降 (BGD):<strong>梯度最精确</strong>，但计算成本极高，无法用于大模型。适用于小规模/理论分析</li>
  <li>纯随机梯度下降 (SGD):只选取一个进行梯度更新。<strong>梯度噪声最大</strong>，但计算成本最低，易于跳出局部极小值。</li>
  <li>小批量 SGD (Mini-Batch):<strong>平衡</strong>了计算效率和梯度精度，是工业界和研究中最常用的方法。</li>
</ul>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124182229547.png" alt="image-20251124182229547" /></p>

<p>batch size越大，梯度的方差越小，因为随机性越弱。</p>

<hr />

<h1 id="2batch-size-的作用影响以及常见的训练问题">2.<strong>Batch Size</strong> 的作用、影响以及常见的训练问题</h1>

<p>当总样本数 N=10000 个时，如果使用批量大小 \(B=\text{batch\_size}\) 进行训练，那么遍历完所有样本所需执行的梯度更新次数（即一次 Epoch 完成的更新次数）是：</p>

\[\text{更新次数} = \frac{N}{B} = \frac{10000}{\text{batch\_size}}\]

<p>batchsize越小，更新越频繁,网络调整方向的频率越高,这种频繁更新使得模型在单位时间内看到更多不同方向的梯度信息，理论上可以更快地响应损失函数的变化。</p>

<p>注意：</p>

<ul>
  <li>Batch Size 过小（梯度过于随机），每次更新的梯度只根据极少数样本计算得出。根据我们之前讨论的<strong>无偏估计</strong>原理，虽然它的<strong>期望值</strong>是正确的，但<strong>方差（噪声）</strong>极大。梯度方向具有<strong>高度随机性</strong>，与真实的整体梯度方向差异过大。这使得优化路径非常曲折和不稳定，就像在一个摇摇晃晃的船上行走，难以稳定收敛。</li>
  <li>Batch Size 过大，虽然梯度方差小，更新方向更精确，但更新频率低。此外，大 Batch Size 可能会导致模型更容易收敛到<strong>尖锐的局部极小值</strong>（泛化能力差），并且需要更多的内存资源。</li>
</ul>

<hr />

<p>在机器学习训练中，如果 Loss 曲线出现剧烈震荡而不是平滑下降，通常是以下原因造成的：</p>

<p>1.如果 <code class="language-plaintext highlighter-rouge">batchsize=1</code>（或非常小），每次更新的梯度噪声太大。每次参数更新都是对<strong>单个样本（或极少数样本）的妥协</strong>，而不是对全局损失的优化。这导致参数在不同训练样本的需求之间来回摇摆不定，Loss 曲线表现为剧烈的<strong>锯齿状震荡</strong>。</p>

<p>2.训练样本没有 Shuffle（打乱），例如，前 5000 个样本都是猫的图片，后 5000 个样本都是狗的图片），而没有进行打乱（Shuffle）在训练的前半段 Epoch 中，Batch 中可能全都是猫。模型会将所有参数朝向识别“猫”的方向更新。在训练的后半段 Epoch 中，Batch 中可能全都是狗。模型会突然发现之前的参数方向是错的，于是将参数剧烈地调整朝向识别“狗”的方向。这种剧烈的、周期性的方向调整，会在 Loss 曲线中表现为明显的周期性、大幅度的震荡或周期性的剧烈上升，严重影响训练的稳定性和收敛性。</p>

<h1 id="3逃离尖锐极小值">3.逃离尖锐极小值</h1>

<p>深度神经网络的损失函数通常是<strong>非凸的</strong>。这意味着损失曲面非常复杂，存在大量的局部极小值（Local Minima）和鞍点（Saddle Points），使得找到全局最优解变得极其困难。</p>

<p>【1】尖锐最小值的危害：参数的微小变化就会导致损失急剧增加，导致模型在未见过的数据（测试集）上表现不佳，即<strong>损失了泛化能力</strong>。</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124184218811.png" alt="image-20251124184218811" style="zoom:50%;" /></p>

<p>【2】到达局部尖锐最小值的原因：大 Batch Size 减小了梯度中的噪声，这使得模型更容易收敛到<strong>尖锐的局部极小值</strong> 。LB 方法的梯度<strong>噪声（方差）极小</strong>，路径平滑精确。这使其在优化过程中缺乏<strong>探索性</strong>，一旦进入尖锐最小值的“吸引盆地”（Basin of Attraction），就<strong>无法逃脱</strong>。</p>

<p>实践发现，使用大批量（LB，large batch）方法训练出的模型，虽然在<strong>训练集上的损失值（Training Loss）</strong>与小批量（Small-Batch, SB）方法相似，但在<strong>测试集上的性能（泛化能力）</strong>明显更差。这就是所谓的<strong>泛化差距</strong>。</p>

<p>LB 方法缺乏 SB 方法的<strong>探索性</strong>，倾向于收敛到靠近<strong>初始点</strong>的最小值。SB 和 LB 方法收敛到<strong>本质上不同</strong>的最小值，这些最小值具有不同的泛化特性。LB 方法缺乏泛化能力的原因在于它们倾向于收敛到<strong>尖锐最小值（Sharp Minimizers）</strong>。</p>

<p>SB 方法的梯度具有<strong>大量噪声</strong>。这种噪声就像一个持续的<strong>扰动</strong>，能够帮助优化过程跳出尖锐、狭窄的谷底，从而探索到更宽广、更平坦的最小值区域。</p>

<p>LB 方法的梯度<strong>过于精确和稳定</strong>，缺乏 SB 方法中噪声所提供的<strong>跳出和探索</strong>能力。这导致 LB 方法最终停留在一个对参数微小变化不鲁棒的解上。</p>

<hr />

<p>看一些实践的数据：</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194657465.png" alt="image-20251124194657465" /></p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194708249.png" alt="image-20251124194708249" /></p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194818816.png" alt="image-20251124194818816" /></p>

<hr />

<p><strong>早期停止（Early Stopping）</strong>是一种常用的正则化技术，它通过在验证集损失开始上升时停止训练，来防止模型过拟合。</p>

<p>对于大批量方法产生的<strong>泛化差距（Generalization Gap）</strong>，简单地使用早期停止并不能有效地弥补。这意味着问题不是出在训练时间太长，而是出在<strong>收敛到的那个最小值本身的性质</strong>有问题。存在<strong>比训练时间</strong>更深层次的原因（即最小值的<strong>尖锐度</strong>）导致泛化能力差。</p>

<h2 id="31最小值的尖锐度测量">3.1最小值的尖锐度测量</h2>

<p>最小值的尖锐度在理论上是通过损失函数在最小值点 \(x\) 处的<strong>海森矩阵 (\(\nabla^2 f(x)\))</strong> 的<strong>特征值</strong>来表征的。特征值越大，曲率越大，最小值越尖锐。但是在深度学习应用中，模型的参数 \(n\) 数量巨大，计算和分解海森矩阵的成本<strong>高到无法承受（prohibitive cost）</strong>。</p>

<p>新的尖锐性度量：基于<strong>敏感性（Sensitivity）</strong>的度量方法。这种方法的核心是：通过<strong>探索解 \(x\) 周围的一个小邻域</strong>，并计算函数 \(f\)（即损失函数）在该邻域内能达到的<strong>最大值</strong>。这个最大值越高，说明损失函数在该点越敏感，即越尖锐。比如计算在 \(x\) 点附近<strong>损失函数值的相对增幅</strong>。增幅越大，说明在该点稍有偏离损失就会大幅增加，即<strong>越尖锐</strong>。</p>

<p>当然衡量尖锐度的方法肯定有很多，可以查阅更多论文了解。例如：https://arxiv.org/pdf/1609.04836</p>

<hr />

<p>适度的噪声被认为是<strong>有益的</strong>。这种噪声帮助优化算法在损失曲面上<strong>跳出</strong>尖锐的局部极小值和鞍点，引导模型收敛到<strong>平坦的最小值（Flat Minima）</strong>区域，从而提高模型的<strong>泛化能力</strong>。</p>

<h1 id="4鞍点问题-优化过程中的隐形障碍">4.鞍点问题-优化过程中的隐形障碍</h1>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124201234427.png" alt="image-20251124201234427" /></p>

<p>深度神经网络的损失曲面（Loss Surface）实际上是一个<strong>混沌的高维地形</strong> 。</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124203051499.png" alt="image-20251124203051499" style="zoom:50%;" /></p>

<p>在这个高维地形中，最常见的障碍物不是<strong>局部极小值（Local Minima）</strong>，而是<strong>鞍点（Saddle Points）</strong>。理解鞍点是理解深度学习优化器设计原理的关键。</p>

<h2 id="41鞍点的数学定义">4.1鞍点的数学定义</h2>

<p>第一：在鞍点处，损失函数 \(f\) 的<strong>梯度</strong> \(\nabla f\) 等于零 (\(\nabla f = 0\))。—-&gt;这会导致优化过程停滞。</p>

<p>第二：函数的<strong>曲率（Curvature）</strong>是<strong>混合的</strong>。在某些方向上，曲面是<strong>向上弯曲</strong>的（像最小值）；在另一些方向上，曲面是<strong>向下弯曲</strong>的（像最大值）。</p>

<blockquote>
  <p><strong>示例函数 \(f(x, y) = x^2 - y^2\)：</strong></p>

  <ul>
    <li><strong>梯度：</strong> \(\nabla f = (2x, -2y)\)。在原点 \((0, 0)\) 处，梯度为0。</li>
    <li><strong>曲率（通过海森矩阵的特征值）：</strong> 特征值为 \(\{2, -2\}\)。由于特征值<strong>符号混合</strong>（一个正，一个负），原点 \((0, 0)\) 是一个鞍点。</li>
    <li>在多变量微积分中，<strong>海森矩阵（Hessian Matrix）就是二阶导数的矩阵表示</strong>。海森矩阵是多变量函数<strong>二阶导数的完整集合</strong>，它以矩阵的形式体现了函数在各个方向上的弯曲程度。</li>
  </ul>
</blockquote>

<p>总结：鞍点是损失曲面上梯度为零的点，但它在某些方向上是最小值，在另一些方向上是最大值。在深度网络中，鞍点比局部极小值更普遍。</p>

<h2 id="42为什么鞍点是一个问题">4.2为什么鞍点是一个问题</h2>

<p>鞍点是训练深度神经网络时导致训练效率低下的主要原因。</p>

<p>在鞍点附近，<strong>梯度（Gradient）几乎为零</strong> (\(\nabla f \approx 0\))。结果就是<strong>收敛停滞（convergence stalls）</strong>，模型在很长时间内都不会有显著进展。当优化器陷入鞍点时，损失值停止下降，但并没有达到一个理想的低值。从业者可能会误认为训练已经<strong>“完成”</strong>，因为损失已经不再变化，但实际上优化器只是<strong>被困在一个平坦的平台（flat plateau）</strong>上。</p>

<p>在低维参数空间中，极小值（Minima）和极大值（Maxima）是主要的驻点。但在<strong>高维空间</strong>（例如拥有数百万个参数的 DNN）中，<strong>鞍点的数量会远超局部极小值</strong>（Dauphin et al., 2014）。这意味着在训练过程中，优化器遇到鞍点的概率远高于遇到局部极小值。</p>

<h2 id="43如何推断陷入鞍点">4.3如何推断陷入鞍点</h2>

<p>由于直接计算鞍点过于复杂，实践中我们通过观察训练动态来<strong>推断</strong>模型是否被鞍点困住:</p>

<ul>
  <li>损失值长时间不下降，但并未收敛到接近零的低值。</li>
  <li>在多个训练步骤中，梯度的<strong>范数（Norm）</strong>（即梯度的整体大小）接近于零。</li>
  <li>模型参数在鞍点周围轻微“徘徊”（hover），但整体上没有朝着损失更低的方向移动。</li>
  <li>理论上，可以通过计算<strong>海森矩阵（Hessian）的特征值</strong>。如果特征值<strong>正负混合</strong>，则可以确定该点是鞍点。然而，正如前面所解释的，海森矩阵对于大型网络<strong>计算上不可行（infeasible）</strong>。深度学习框架（如 TensorFlow, PyTorch）<strong>不会检查海森矩阵</strong>，而是依靠优化器的动态特性来自动逃离陷阱。</li>
</ul>

<h2 id="44逃离鞍点的策略">4.4逃离鞍点的策略</h2>

<p>现代优化算法的核心设计目标就是提供逃离鞍点所需的“推力”或“方向感”。</p>

<p>(1)小批量 SGD 带来的随机梯度噪声（方差）是一种天然的<strong>扰动</strong>。</p>

<p>(2) <strong>现代优化器（如 Adam, RMSprop）</strong>通过引入动量（Momentum）和自适应学习率机制，能够更有力地推开优化过程，有效<strong>逃离鞍点</strong>。</p>

<p>动量为参数更新添加了<strong>速度（velocity）</strong>，这是基于历史梯度的积累。即使在鞍点梯度消失时，动量也能凭借惯性<strong>带着参数向前冲</strong>，从而越过平坦区域。</p>

<p>这类优化器（Adam、RMSProp）会为<strong>每个参数</strong>动态地缩放（调整）学习率。在鞍点处，优化器能识别出<strong>平坦方向</strong>（梯度变化慢），并给予这些方向<strong>相对较大的步长</strong>，而对<strong>尖锐方向</strong>（梯度变化快）则给予较小步长，从而<strong>加速逃离</strong>。</p>

<p>通过精心调整和衰减学习率，可以防止优化器一开始就以极快的速度冲过低损失区域，避免陷入鞍点后因学习率过低而无法逃脱。</p>

<p>(3)正则化有助于减少网络的冗余和退化，从而<strong>重塑（reshapes）</strong>损失曲面。这可以减少鞍点的数量或使鞍点更容易被逃离。</p>

<h2 id="45pytorch如何解决鞍点问题">4.5PyTorch如何解决鞍点问题</h2>

<p>现代优化器逃离鞍点的策略，都是为了在梯度接近于零的平坦区域提供额外的<strong>推力</strong>和<strong>方向感</strong>。</p>

<p>（1）动量 SGD (SGD with Momentum)：本身SGD有噪声，一定程度上可以缓解鞍点的问题。但是还可以引入一个<strong>惯性项（Momentum Term）</strong>来解决传统 SGD 在鞍点附近和狭长曲面上的震荡和停滞问题。</p>

<p>优化器维护一个<strong>速度缓冲 <code class="language-plaintext highlighter-rouge">buf</code></strong>（或称为动量项），它存储了过去梯度的指数加权平均值。当优化器遇到鞍点时 ，虽然当前的梯度很小，但由于 <code class="language-plaintext highlighter-rouge">buf</code> 中累积了之前的梯度信息，<strong>动量会带着参数继续向前推进</strong>。</p>

<p><strong><code class="language-plaintext highlighter-rouge">buf = momentum * buf + grad</code></strong></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">momentum</code> 是动量系数，通常接近 1 (如 0.9或 0.99)。</li>
  <li><strong>新速度</strong> (<code class="language-plaintext highlighter-rouge">buf</code>) 是<strong>旧速度</strong>按动量衰减后的值，加上<strong>当前梯度</strong> (<code class="language-plaintext highlighter-rouge">grad</code>)。</li>
  <li>在鞍点处，连续的微小梯度会被累加，形成一个较大的速度向量，从而提供逃逸所需的推力。</li>
</ul>

<p>参数更新直接使用累积的速度 (<code class="language-plaintext highlighter-rouge">buf</code>)，而不是原始梯度。</p>

<p>（2）Adam 优化器 (Adaptive Moment Estimation)：目前工业界最流行的优化器之一，它结合了 <strong>动量</strong> 和 <strong>自适应学习率</strong> 的双重优势，使其在逃离鞍点方面表现出色。</p>

<h1 id="5学习的震荡问题">5.学习的震荡问题</h1>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124211118448.png" alt="image-20251124211118448" style="zoom:50%;" /></p>

<p>损失函数曲面是狭长的，损失函数对不同参数的敏感度差异很大，参数被迫在山谷的<strong>两侧来回剧烈跳跃（Oscillation）</strong></p>

<p>导致的问题：大部分的学习步长被浪费在无意义的、垂直于目标方向的来回跳动上。只有微小的进步是沿着谷底向真正的最小值方向进行的。如果我们尝试使用较大的学习率来加速沿着谷底的进展，那么在陡峭的垂直方向上，参数就会<strong>发散（Diverge）</strong>，无法收敛。整个训练过程的学习率不得不受到最陡峭方向的限制，因此平坦方向的学习速度就会非常慢。</p>

<hr />

<p>解决：比如<strong>动量（Momentum）：</strong> 通过累积历史梯度，震荡方向的梯度（它们相互抵消）被抑制，而沿着谷底方向的梯度（它们持续累加）被放大，从而加速沿着谷底的进展。</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213049497.png" alt="image-20251124213049497" style="zoom:50%;" /></p>

\[\begin{aligned}&amp;\boldsymbol{g}=\frac{\partial\mathrm{Loss}}{\partial\boldsymbol{W}}\\&amp;\boldsymbol{v}_{t}=\alpha\boldsymbol{v}_{t-1}+\varepsilon\boldsymbol{g}\\&amp;\boldsymbol{W}_{t}=\boldsymbol{W}_{t-1}-\boldsymbol{v}_{t}\end{aligned}\]

<blockquote>
  <p>这个方法说白了就是考虑当前梯度和上一次的梯度，做对冲</p>

  <p>如果方向不一致，那么就会对冲，更新步幅会比较稳定</p>

  <p>如果方向一致，那么步子会越迈越大，更加加快学习速度—-&gt;巧妙！</p>

  <p>动量法—-&gt;既解决学习震荡的问题，又解决学习慢的问题</p>
</blockquote>

<p>首先SGD 的第一步，计算当前小批量样本（Mini-Batch）在当前权重 \(\boldsymbol{W}_{t-1}\) 处的平均梯度 \(\boldsymbol{g}\)。</p>

<p>新速度 \(\boldsymbol{v}_t\) 是由两部分组成的:一般推荐\(\alpha=0.9\)，\(\varepsilon=0.1\)</p>

<p>1.<strong>历史速度的延续 (\(\alpha\boldsymbol{v}_{t-1}\)):</strong> 参数带着上一步的惯性继续前进。\(\alpha\) 越大，惯性越强。</p>

<p>2.<strong>当前梯度的推动 (\(\varepsilon\boldsymbol{g}\)):</strong> 当前的梯度信息指导参数朝新的方向移动， \(\varepsilon\) 控制这个推动力的大小。</p>

<p>最后，更新权重，权重 \(\boldsymbol{W}\) 是沿着新的<strong>速度向量 \(\boldsymbol{v}_t\)</strong> 的反方向进行更新。</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213112882.png" alt="image-20251124213112882" style="zoom:50%;" /></p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213143374.png" alt="image-20251124213143374" style="zoom:50%;" /></p>

<p>Sebastian Ruder 在他的论文中简明扼要地描述了动量的影响：”动量项在梯度指向相同的维度上会增加，而在梯度改变方向的维度上会减少更新。因此，我们可以加快收敛速度，减少振荡”</p>

<h1 id="6adagrad学习因子设定问题学习率">6.AdaGrad学习因子设定问题（学习率）</h1>

<p>AdaGrad（Adaptive Gradient Algorithm，自适应学习算法）是一种革命性的优化器，它首次实现了对<strong>每个参数</strong>的<strong>独立学习率调整</strong>。它的核心动机是解决不同参数对梯度的敏感度差异问题。</p>

<p>在训练过程中，权重向量的不同分量（即不同的参数）可能会有极端的梯度差异。某些参数（例如对应频繁出现特征的权重）的梯度值相对稳定且适中。这是因为它们在每次更新时都使用了大量的样本信息。另一些参数（例如对应不常出现特征的权重，即稀疏数据）的梯度值可能极其巨大且不稳定。这是因为这些参数只在少数几次迭代中被更新，导致偶尔出现的信号被过分放大，造成梯度爆炸。</p>

<p>另外，传统的 SGD 或带动量的 SGD 对所有参数使用单一的全局学习率 \(\alpha\)。如果 \(\alpha\) 设置得太小，那些梯度小的稀疏参数就更新得太慢；如果 \(\alpha\) 设置得太大，那些梯度大的参数就会剧烈震荡甚至发散。</p>

<p>AdaGrad 通过为权重向量的<strong>每个组件（每个参数）独立地调整学习率</strong>来解决上述问题。</p>

<p>如果某个参数的历史梯度<strong>积累值很大</strong> $\rightarrow$ 它的学习率应该<strong>变小</strong>。</p>

<p>如果某个参数的历史梯度<strong>积累值很小</strong> $\rightarrow$ 它的学习率应该<strong>变大</strong>。</p>

<p>这样，那些梯度大的稀疏参数的学习率会被迅速抑制，稳定了训练；而那些梯度小的参数能获得更大的推动力，加速了学习，从而有助于解决<strong>梯度消失和梯度爆炸</strong>问题。</p>

<p>AdaGrad 通过累积<strong>过去所有迭代中梯度的平方</strong>来实现自适应调整。</p>

<ul>
  <li>
    <p>累积量 \(v_t\)： 在每一步 \(t\)，AdaGrad 会计算一个累积变量 \(v_t\)（在公式中通常记为 \(G_t\) 或 \(V_t\)），它是从训练开始到当前步为止，所有历史梯度平方的元素级求和。</p>

\[\boldsymbol{v}_t = \boldsymbol{v}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t\]

    <p>（其中 \(\odot\) 表示元素级乘法，\(\boldsymbol{g}_t\) 是当前梯度）</p>
  </li>
  <li>
    <p>更新方式： AdaGrad 将原始的学习率 \(\alpha\) 划分为 \(\sqrt{v_t}\)，作为每个参数的新的、自适应的学习率</p>

\[\boldsymbol{W}_{t} = \boldsymbol{W}_{t-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_t }+ \epsilon} \odot \boldsymbol{g}_t\]

    <ul>
      <li><strong>分母 \(\sqrt{\boldsymbol{v}_t}\)：</strong> 作为一个<strong>动态的正则项</strong>，其值越大，该参数的有效学习率就越小。</li>
      <li><strong>\(\epsilon\)：</strong> 一个很小的正数，防止分母为零。</li>
    </ul>
  </li>
</ul>

<p>最大的优势在于<strong>无需手动调参</strong>（尤其是学习率 \(\alpha\)），因为算法在训练过程中会根据历史数据自动进行调整。非常适合处理<strong>稀疏数据</strong>（如自然语言处理中的词嵌入），能让罕见特征获得更大的更新步长。</p>

<blockquote>
  <p>或者说，学习的越多，W更新的越多，学习因子就大。</p>
</blockquote>

<p>最大的问题是 <strong>\(v_t\) 是一个持续累积正数</strong>的变量，它只会单调递增。这意味着分母 \(\sqrt{\boldsymbol{v}_t}\) 会越来越大，导致<strong>有效学习率会随着训练的进行而不断、单调地衰减</strong>。在训练的后期，有效学习率变得<strong>极低</strong>，使得模型难以进行有效的参数更新，导致算法<strong>收敛速度非常慢</strong>，甚至提前停止学习。这个局限性直接促成了后续改进算法如 <strong>RMSprop</strong> 和 <strong>Adam</strong> 的诞生。</p>

<h1 id="7rmsprop和adam">7.RMSprop和Adam</h1>

<h2 id="71rmsprop优化器">7.1RMSprop优化器</h2>

<p><strong>RMSProp (Root Mean Square Propagation)</strong> 优化器，它作为对 AdaGrad 的改进，克服了 AdaGrad 的主要缺陷，由 Geoffrey Hinton 在其 Coursera 课程中提出的，它的主要目标是解决 AdaGrad 学习率单调递减导致后期收敛缓慢的问题。</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124222159540.png" alt="image-20251124222159540" style="zoom:50%;" /></p>

<p>在AdaGrad中，二阶矩（即\(v_t\)）是简单的累积相乘，\(v_t\) 单调递增，学习率无限衰减。学到最后，就学的越来越少了。</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124222520938.png" alt="image-20251124222520938" style="zoom:50%;" /></p>

<p>而RMS prop中，二阶矩（即\(v_t\)）是<strong>指数移动平均 (EMA)</strong>：\(v_t = \beta \cdot v_{t-1} + (1-\beta) \cdot g_t^2\)。\(\beta\) 控制历史信息衰减，学习率可以<strong>保持稳定</strong>（通常接近 1，如 0.9）。</p>

<blockquote>
  <p>公式只涉及 \(t\) 和 \(t-1\) 时刻的量，为什么说它和“久远”的历史信息有关？指数移动平均（EMA）的“记忆”机制</p>

  <p>因为\(\boldsymbol{v}_{t-1}\) 本身就包含了久远的历史信息。</p>

\[\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha \boldsymbol{v}_{t-1}\]

  <p>而\(\boldsymbol{v}_{t-1} = (1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha \boldsymbol{v}_{t-2}\)</p>

  <p>代入得到：\(\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha \left[ (1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha \boldsymbol{v}_{t-2} \right]\)</p>

\[\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha(1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha^2 \boldsymbol{v}_{t-2}\]

  <p>如果将这个过程一直追溯到训练开始（\(t=0\)），可以看到 \(\boldsymbol{v}_t\) 实际上是<strong>所有历史梯度平方 \(\boldsymbol{g}_i^2\) 的加权和</strong>：</p>

\[\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha(1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha^2(1-\alpha)\boldsymbol{g}_{t-2}^2 + \alpha^3(1-\alpha)\boldsymbol{g}_{t-3}^2 + \dots\]

  <p>因此，说 \(\boldsymbol{v}_t\) 具有<strong>“记忆”</strong>功能，并由 \(\alpha\) 控制这个记忆的<strong>“遗忘速度”</strong>。</p>
</blockquote>

<p>AdaGrad 与 RMSProp 维持历史信息的方式对比：</p>

<p>AdaGrad是一个<strong>无限期、不衰减的累加过程</strong>。每一个历史梯度平方 \(\boldsymbol{g}_i^2\) 对当前的 \(v_t\) 都拥有<strong>相同的、永久的权重</strong>。\(v_t\) 的值是一个<strong>非递减序列</strong>，它永远不会忘记过去的梯度。随着训练的不断进行，分母 \(\sqrt{\boldsymbol{v}_t}\) 会<strong>无限增大</strong>，导致有效学习率 \(\frac{\alpha}{\sqrt{\boldsymbol{v}_t}}\) 不可避免地、<strong>无限地趋向于零</strong>。在训练后期，即使遇到一个非常重要的、需要大步长更新的梯度，模型也因为学习率太小而无法做出有效响应。</p>

<p>RMSProp是一个<strong>有选择性、指数衰减的平均过程</strong>。通过 \(\beta\) 系数，<strong>久远的梯度信息权重会指数级地衰减</strong>。模型只“记住”最近一段时间内的梯度平均大小。<strong>\(v_t\) 的值会收敛到近期梯度平方的平均值附近</strong>，<strong>不会无限增大</strong>。</p>

<h2 id="72时间累积自适应梯度">7.2时间累积+自适应梯度</h2>

<p>可以把动量 SGD 和 RMSprop 结合起来，公式如下。</p>

\[v_{t}=\rho_{1} v_{t-1}+(1-\rho_{1}) g\]

\[r_{t}=\rho_{2} r_{t-1}+(1-\rho_{2})\langle g, g\rangle\]

\[w_{t}=w_{t-1}-\alpha \frac{v_{t}}{\delta+\sqrt{r_{t}}}\]

<p>\(r_t\)是自适应的学习因子。</p>

<p>不足：t早期冷启动的时候，v和r都会整体偏小</p>

<p>解释：在训练开始时，优化器通常将 \(\boldsymbol{v}_0\) 和 \(\boldsymbol{r}_0\) 初始化为<strong>零向量</strong>。由于 \(\rho_1\) 和 \(\rho_2\) 通常被设置成接近 1的值（例如 \(\rho_1=0.9\) 或 \(\rho_2=0.999\)），这意味着历史信息的权重很大，而当前信息的权重 \((1-\rho)\) 很小。所以在训练的最初几个时间步中，几乎从零开始，导致 \(\boldsymbol{v}_t\) 和 \(\boldsymbol{r}_t\) 的值在训练初期会<strong>系统性地偏小</strong>，尤其是在 \(t\) 很小时。它们的真实期望值应该比计算出来的值要大。</p>

<p>这种低估偏差导致模型<strong>初始的学习步长过小</strong>。</p>

<p>解决方案：偏差修正 (Bias Correction),就是改进版的adam优化器</p>

<h2 id="73adam优化器">7.3Adam优化器</h2>

<p>为了解决这种冷启动偏差，Adam 引入了<strong>偏差修正项</strong>。其思想是利用 \(\boldsymbol{v}_t\) 和 \(\boldsymbol{r}_t\) 的期望值，对它们进行放大，从而消除初始化偏差。</p>

<p>修正后的一阶矩（动量）：</p>

\[\hat{\boldsymbol{v}}_{t} = \frac{\boldsymbol{v}_{t}}{1-\rho_{1}^{t}}\]

<p>修正后的二阶矩（方差）：</p>

\[\hat{\boldsymbol{r}}_{t} = \frac{\boldsymbol{r}_{t}}{1-\rho_{2}^{t}}\]

<hr />

<p>当 \(t\) 较小（如 \(t=1, 2\)）时，\(1-\rho^t\) 的值接近于零。用这个接近零的值作除数，会<strong>极大地放大</strong>原始的 \(v_t\) 和 \(r_t\)。这抵消了它们在初期被低估的偏差。</p>

<p>随着 \(t\) 增大（例如 \(t \approx 50\)），\(\rho^t\) 会趋近于0，分母 \(1-\rho^t\) 趋近于1。此时，\(\hat{v}_t \approx v_t\)，偏差修正项<strong>自动消失</strong>，优化器回归到标准的 EMA 更新。</p>

<hr />

<p><strong>最终的 Adam 更新公式：</strong></p>

\[\boldsymbol{w}_{t} = \boldsymbol{w}_{t-1}-\alpha \frac{\hat{\boldsymbol{v}}_{t}}{\delta+\sqrt{\hat{\boldsymbol{r}}_{t}}}\]

<p>通过引入这个偏差修正机制，Adam 确保了在训练初期（冷启动阶段）参数更新的方向和大小是准确的，极大地提高了模型的<strong>训练稳定性</strong>。</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124232804313.png" alt="image-20251124232804313" style="zoom:50%;" /></p>

<h1 id="8归一化normalization和正则化">8.归一化Normalization和正则化</h1>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124233107638.png" alt="image-20251124233107638" style="zoom:50%;" /></p>

<h2 id="81为什么要归一化">8.1为什么要归一化</h2>

<p>特征尺度的巨大差异，比如<strong>年龄 (Age)：</strong> 范围 \([0, 65]\)，<strong>薪水 (Salary)：</strong> 范围 \([0, 100,000]\)。</p>

<p>模型学习到的<strong>权重（Weights）</strong>在更新时会受到这种尺度差异的强烈影响,显然年龄相关的权重则需要更大的调整才能跟上薪水权重的影响。</p>

<p>另外，当输入特征尺度不一致时，会导致以下问题：</p>

<p>（1）震荡</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124211118448.png" alt="image-20251124211118448" style="zoom:50%;" /></p>

<p>（2）优化器（如梯度下降）必须沿着最陡峭的方向前进，导致它在陡峭的维度上<strong>剧烈震荡</strong>。为了防止在陡峭方向上超调（overshoot）导致发散，我们被迫使用非常低的学习率。结果是，模型沿着平坦的方向进展缓慢，学习过程变慢，收敛效率低下。</p>

<h2 id="82输入归一化的方法">8.2输入归一化的方法</h2>

<p><strong>Z-score 标准化</strong>或 <strong>Standard Scaling</strong></p>

<p>显然，归一化之后，损失曲面被“解扭曲”，变得更加<strong>圆形和对称</strong>。优化器可以沿着更直接的路径向最小值前进，不再需要处理剧烈的震荡。我们可以使用<strong>更高的学习率</strong>而不会导致发散，从而实现<strong>更快的收敛</strong>和<strong>更稳定的训练</strong>。</p>

<p>既然这个解决方案如此巧妙，为什么我们不将网络中每一层的激活值都进行归一化呢？</p>

<h2 id="83批归一化激活值归一化">8.3批归一化(激活值归一化)</h2>

<p>在<strong>每一层</strong>的激活值（在激活函数之前或之后）都进行归一化，确保每一层的输入分布保持稳定，从而进一步加速训练和提高模型性能。</p>

<p>批量归一化（批归一化，Batch Normalization）的提出是为了解决 <strong>内部协变量漂移</strong> 问题。</p>

<p>【1】内部协变量漂移 (Internal Covariate Shift, ICS)</p>

<p>简单来说，在神经网络训练过程中，前一层参数的每一次更新，都会导致该层输出的<strong>激活值分布发生改变</strong>。紧随其后的下一层需要<strong>不断适应</strong>这种不断变化的输入分布，这使得模型难以收敛，导致<strong>收敛速度变慢</strong>和<strong>训练不稳定</strong>。</p>

<p>【2】BN：BN 的思想是像对输入层进行归一化一样，对网络<strong>每一层</strong>的激活值进行归一化，以控制其分布。</p>

<p>BN 在一个 <strong>小批量 (Mini-Batch)</strong> 上计算激活值的均值和方差，对激活值进行中心化（减去 \(\mu\)）和缩放（除以 \(\sigma\)），即完成标准化。</p>

<p>为了不让网络完全丢失归一化之前的表示能力（例如，如果 Sigmoid/Tanh 函数的输入总是被强制归一化到0附近，它们将无法利用非线性区域），BN 引入了两个<strong>可学习的参数</strong>：</p>

<ul>
  <li><strong>缩放因子 (\(\gamma\)):</strong> 用于<strong>缩放 (Scaling)</strong> 归一化后的分布。</li>
  <li><strong>平移因子 (\(\beta\)):</strong> 用于<strong>平移 (Shifting)</strong> 归一化后的分布。</li>
</ul>

<p>这两个参数会和网络权重一起，通过<strong>反向传播</strong>进行学习和优化。它们允许网络自主决定最佳的分布，例如，如果网络需要一个非零均值或非单位方差的分布来提高性能，\(\gamma\) 和 \(\beta\) 可以通过学习恢复这种分布。</p>

\[\text{BN}_{\gamma, \beta}(x_i) = \gamma \cdot \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} + \beta\]

<p>【3】BN 的主要优势</p>

<p>由于网络中每一层的输入分布保持固定，后续层不再需要适应不断变化的输入，从而允许我们使用<strong>更高的学习率</strong>，<strong>加快收敛速度</strong>。BN解决了 ICS，使训练过程更加稳定。</p>

<p>基于 Mini-Batch 的统计量引入了<strong>随机的噪声</strong>，类似于 Dropout 机制，有助于防止过拟合，因此 BN 可以作为一种正则化技术。</p>

<p>【4】 BN 的局限性</p>

<p>（1）对批量大小 (Batch Size) 的依赖，Batch Size太大导致参数更新慢，容易陷入尖锐最小值；BN 假设当前 Batch 的 \(\mu\) 和 \(\sigma^2\) 是对整个数据集 \(\mu\) 和 \(\sigma^2\) 的良好估计。当 <strong>Batch Size 太小</strong>时，这个估计非常不准确，引入的噪声过大，导致性能<strong>急剧下降</strong>。如下图，batch size过小，错误率越高：</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125112038931.png" alt="image-20251125112038931" style="zoom:50%;" /></p>

<p>（2）测试/推理时间 (Testing Time) 的不一致性</p>

<p>BN 的训练和测试过程需要不同的计算方式。在测试或推理时（例如自动驾驶汽车只输入<strong>单帧图像</strong>），无法再计算有效的 Batch 统计量。网络必须使用在训练期间预先计算和存储的<strong>全局平均值和全局方差</strong>（通过对训练过程中所有 Batch 的 \(\mu\) 和 \(\sigma^2\) 进行移动平均估计）。如果训练时的全局统计量与实际测试数据分布存在较大偏差，就会导致 <strong>“在训练和测试中结果不一致”</strong> 的问题。</p>

<p>这些局限性，特别是对 Batch Size 的强依赖，促使社区开发了诸如 <strong>层归一化 (Layer Normalization)</strong>、<strong>实例归一化 (Instance Normalization)</strong> 和 <strong>组归一化 (Group Normalization)</strong> 等替代方法，来避免对 Batch 的依赖。</p>

<p>推荐阅读：https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf</p>

<h2 id="84正则化">8.4正则化</h2>

\[\mathcal{L}_{\text{L2}}(\boldsymbol{W}) = \mathcal{L}(\boldsymbol{W}) + \frac{\lambda}{2} \sum_{i} W_i^2\]

\[\mathcal{L}_{\text{L1}}(\boldsymbol{W}) = \mathcal{L}(\boldsymbol{W}) + \lambda \sum_{i} |W_i|\]

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125124738477.png" alt="image-20251125124738477" style="zoom:50%;" /></p>

<p>神经网络比较灵活，不同的层可以使用不同的L1或L2正则。且上方公式的\(\lambda\)不同层可以选择不同的权重。</p>

<p>思考，对于正则化公式的\(\lambda\)，神经网络输入层和输出层哪边的\(\lambda\)更大比较好？</p>

<blockquote>
  <p>答案：对网络深层的权重施加更强的正则化。(答案不唯一，也有人认为浅层应设置更大的值)</p>

  <p>浅层（靠近输入数据）的权重主要负责提取数据的<strong>低级特征</strong>，例如边缘、纹理、颜色等基础模式。这些低级特征往往是<strong>通用且稳定的</strong>，并且在数据集中是<strong>普遍存在</strong>的。对这些基础特征提取器施加<strong>过强的正则化（过大的 \(\lambda\)）</strong>，可能会<strong>过度抑制</strong>网络学习这些基础模式的能力，导致模型性能下降。因此，我们倾向于使用<strong>较小</strong>的 \(\lambda\)。</p>

  <p>深层（靠近输出层）的权重主要负责将前一层提取的<strong>高级语义特征</strong>（例如，物体的特定组合、复杂的概念）映射到最终的<strong>决策或预测</strong>。这些深层权重往往更容易捕捉到训练集中的<strong>偶然噪声和特定模式</strong>，从而导致<strong>过拟合</strong>。它们负责网络最复杂的逻辑判断。为了避免这些高层决策逻辑<strong>过度依赖</strong>训练集中的微小波动或噪声，我们需要对深层权重施加<strong>更强的约束（更大的 \(\lambda\)）</strong>，鼓励它们保持较小的数值，从而提高模型的<strong>泛化能力</strong>。</p>

  <p>在实际应用中，最佳的 \(\lambda\) 仍然需要通过<strong>交叉验证（Cross-Validation）</strong>在特定的数据集和模型结构上进行实验确定</p>
</blockquote>

<p>L1正则的剪枝效果：因为L1正则化倾向于使不重要的权重<strong>精确地等于零</strong>。可以剔除冗余特征，实现<strong>特征选择</strong>。在实际的深度学习训练中，我们通常选择 <strong>L2 正则化</strong>（即权重衰减），因为它能平滑地控制模型复杂度，更好地防止过拟合。只有当我们特别需要<strong>稀疏解</strong>或进行<strong>特征选择</strong>时，才会考虑使用 L1 正则化。</p>

<p><img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125130337611.png" alt="image-20251125130337611" style="zoom:50%;" /></p>



                <hr style="visibility: hidden;">
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2025/11/24/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9913/" data-toggle="tooltip" data-placement="top" title="【AI思想启蒙13】深度学习第3篇-度量学习 ">
                        Previous<br>
                        <span>【AI思想启蒙13】深度学习第3篇-度量学习 </span>
                        </a>
                    </li>
                    
                    
                </ul>
                <hr style="visibility: hidden;">

                

                
            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                


<section>
    
        <hr class="hidden-sm hidden-xs">
    
    <h5><a href="/archive/">FEATURED TAGS</a></h5>
    <div class="tags">
        
        
        
        
        
        
                <a data-sort="0188" 
                    href="/archive/?tag=%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB"
                    title="书籍阅读"
                    rel="3">书籍阅读</a>
        
                <a data-sort="0150" 
                    href="/archive/?tag=%E7%AE%97%E6%B3%95"
                    title="算法"
                    rel="41">算法</a>
        
                <a data-sort="0173" 
                    href="/archive/?tag=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDAI%E5%9F%BA%E7%A1%80"
                    title="人工智能AI基础"
                    rel="18">人工智能AI基础</a>
        
                <a data-sort="0174" 
                    href="/archive/?tag=python%E5%9F%BA%E7%A1%80"
                    title="python基础"
                    rel="17">python基础</a>
        
                <a data-sort="0176" 
                    href="/archive/?tag=%E5%8A%9B%E6%89%A3"
                    title="力扣"
                    rel="15">力扣</a>
        
                <a data-sort="0177" 
                    href="/archive/?tag=AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%99"
                    title="AI思想启蒙"
                    rel="14">AI思想启蒙</a>
        
                <a data-sort="0178" 
                    href="/archive/?tag=numpy"
                    title="numpy"
                    rel="13">numpy</a>
        
                <a data-sort="0178" 
                    href="/archive/?tag=pandas"
                    title="pandas"
                    rel="13">pandas</a>
        
                <a data-sort="0179" 
                    href="/archive/?tag=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE"
                    title="机器学习-吴恩达"
                    rel="12">机器学习-吴恩达</a>
        
                <a data-sort="0180" 
                    href="/archive/?tag=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"
                    title="机器学习算法"
                    rel="11">机器学习算法</a>
        
                <a data-sort="0185" 
                    href="/archive/?tag=matplotlib"
                    title="matplotlib"
                    rel="6">matplotlib</a>
        
                <a data-sort="0188" 
                    href="/archive/?tag=spring6"
                    title="spring6"
                    rel="3">spring6</a>
        
                <a data-sort="0189" 
                    href="/archive/?tag=%E7%AE%97%E6%B3%95%E9%A2%98%E5%BA%93"
                    title="算法题库"
                    rel="2">算法题库</a>
        
                <a data-sort="0189" 
                    href="/archive/?tag=Java%E5%9F%BA%E7%A1%80"
                    title="Java基础"
                    rel="2">Java基础</a>
        
                <a data-sort="0189" 
                    href="/archive/?tag=java%E9%9B%86%E5%90%88"
                    title="java集合"
                    rel="2">java集合</a>
        
                <a data-sort="0189" 
                    href="/archive/?tag=ollama%E5%B7%A5%E5%85%B7"
                    title="ollama工具"
                    rel="2">ollama工具</a>
        
                <a data-sort="0189" 
                    href="/archive/?tag=python%E7%88%AC%E8%99%AB"
                    title="python爬虫"
                    rel="2">python爬虫</a>
    </div>
</section>


                <!-- Friends Blog -->
                
<hr>
<h5>FRIENDS</h5>
<ul class="list-inline">
  
  <li><a href="https://m.freebuf.com/">freebuf</a></li>
  
  <li><a href="https://xz.aliyun.com/">先知</a></li>
  
  <li><a href="https://www.sec-in.com/All">sec-in</a></li>
  
  <li><a href="https://paper.seebug.org/">see-bug</a></li>
  
  <li><a href="https://twitter.com/Concurr21486093">我的推特</a></li>
  
  <li><a href="https://pdai.tech/">pdai</a></li>
  
  <li><a href="https://nodejs.org/dist/">nodejs index of dist</a></li>
  
  <li><a href="#">公众号：小东方不败</a></li>
  
</ul>

            </div>
        </div>
    </div>
</article>

<!-- add support for mathjax by voleking-->









<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'right',
          // icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- SNS Link -->
                


<ul class="list-inline text-center">


  
  
  <li>
    <a href="https://twitter.com/Concurr21486093">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
  <li>
    <a target="_blank" href="https://www.zhihu.com/people/Hilda">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa  fa-stack-1x fa-inverse">知</i>
      </span>
    </a>
  </li>
  
  
  
  
  <li>
    <a target="_blank" href="https://github.com/kirsten-1">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
</ul>

                <p class="copyright text-muted">
                    Copyright &copy; Hilda 2025
                    <br>
                    Powered by <a href="https://kirsten-1.github.io/">hilda Blog</a>
<!--                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="100px"-->
<!--                        height="20px"-->
<!--                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true">-->
<!--                    </iframe>-->
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Simple Jekyll Search -->
<script src="/js/simple-jekyll-search.min.js"></script>

<!-- Service Worker -->

<script src="/js/snackbar.js "></script>
<script src="/js/sw-registration.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
        var d = document, t = 'script',
            o = d.createElement(t),
            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
        s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->







<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function () {
        var $nav = document.querySelector("nav");
        if ($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->



<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog(selector) {

        // interop with multilangual 
        if ('' == 'true') {
            _containerSelector = 'div.post-container.active'
        } else {
            _containerSelector = 'div.post-container'
        }

        // init
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        // clean
        $(selector).html('')

        // appending
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Multi-Lingual -->


<!-- Simple Jekyll Search -->
<script>
    // https://stackoverflow.com/questions/1912501/unescape-html-entities-in-javascript
    function htmlDecode(input) {
        var e = document.createElement('textarea');
        e.innerHTML = input;
        // handle case of empty input
        return e.childNodes.length === 0 ? "" : e.childNodes[0].nodeValue;
    }

    SimpleJekyllSearch({
        searchInput: document.getElementById('search-input'),
        resultsContainer: document.getElementById('search-results'),
        json: '/search.json',
        searchResultTemplate: '<div class="post-preview item"><a href="{url}"><h2 class="post-title">{title}</h2><h3 class="post-subtitle">{subtitle}</h3><hr></a></div>',
        noResultsText: 'No results',
        limit: 50,
        fuzzy: false,
        // a hack to get escaped subtitle unescaped. for some reason, 
        // post.subtitle w/o escape filter nuke entire search.
        templateMiddleware: function (prop, value, template) {
            if (prop === 'subtitle' || prop === 'title') {
                if (value.indexOf("code")) {
                    return htmlDecode(value);
                } else {
                    return value;
                }
            }
        }
    });

    $(document).ready(function () {
        var $searchPage = $('.search-page');
        var $searchOpen = $('.search-icon');
        var $searchClose = $('.search-icon-close');
        var $searchInput = $('#search-input');
        var $body = $('body');

        $searchOpen.on('click', function (e) {
            e.preventDefault();
            $searchPage.toggleClass('search-active');
            var prevClasses = $body.attr('class') || '';
            setTimeout(function () {
                $body.addClass('no-scroll');
            }, 400)

            if ($searchPage.hasClass('search-active')) {
                $searchClose.on('click', function (e) {
                    e.preventDefault();
                    $searchPage.removeClass('search-active');
                    $body.attr('class', prevClasses);  // from closure 
                });
                $searchInput.focus();
            }
        });
    });
</script>


<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
