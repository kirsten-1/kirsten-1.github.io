---
layout: post
title: "【AI思想启蒙14】深度学习第4篇-一文吃透神经网络训练的 8 大核心难题 "
subtitle: "小批量SGD无偏但有噪声，适中batch size平衡精度与效率；小batch噪声助逃尖锐最小值/鞍点；动量抑震荡、加速，Adam自适应+偏差修正最优；BatchNorm稳定分布、允许大学习率，但小batch失效；L2正则防过拟合，L1具有剪枝效果。"
date: 2025-11-25
author: "Hilda"
header-img: "img/post-bg-2015.jpg"
tags:
- AI思想启蒙
---

<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


# 1.梯度下降基础问题

回顾梯度下降：在训练神经网络时，我们的目标是最小化整个训练集上的**总损失函数** $$\mathcal{L}(W)$$：

$$\mathcal{L}(W) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{(i)}(W)$$

其中$$W$$ 是所有模型的权重，$$N$$ 是训练集中的总样本数。

**目标是计算总梯度的平均值：** $$\frac{\partial \mathcal{L}}{\partial W} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial \mathcal{L}_{(i)}}{\partial W}$$。

由于N往往非常大（百万甚至上亿），每次迭代都计算全部N个样本的梯度是**不可行**且**耗时**的。因此，我们使用 SGD 的小批量变体。

## 1.1小批量随机梯度下降 (Mini-Batch SGD) 的梯度

**小批量 SGD 的梯度**

$$\frac{\partial \text{Loss}_{SGD}}{\partial W} = \frac{1}{\text{batch\_size}} \sum_{i \in \text{batch}} \frac{\partial \text{Loss}_{(i)}}{\partial W}$$

> 为了最大程度的应用GPU等硬件资源，batch size通常为32，64，128，256等等数值。

在每次迭代中，我们从训练集中随机抽取一个大小为 $$B = \text{batch\_size}$$ 的子集（一个批次）。**我们用这个小批次样本的平均梯度，来近似整个训练集的总平均梯度。另外，为了充分利用 GPU 等硬件的并行计算能力，通常需要增加 Batch Size。

小批量 SGD 的成功依赖于**无偏估计原理**：通过随机抽样，我们只需要计算一小部分样本的平均梯度，就能以极高的计算效率，沿着**统计学上正确**的方向进行优化。这使得深度神经网络的大规模训练成为可能。

## 1.2统计上相等的原理（无偏估计）

$$E\left(\frac{1}{\text{batch\_size}} \sum_{i \in \text{batch}} \frac{\partial \text{Loss}_{(i)}}{\partial W}\right) = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial \text{Loss}_{(i)}}{\partial W}$$

左式中$$E(\cdot)$$ 代表**期望值（Expected Value）**。左侧是小批量梯度的**期望值**。

右侧是整个训练集的**真实平均梯度**（即批量梯度下降 BGD 的梯度）。

这个等式表明，虽然任何一个**单独的小批量梯度**都可能与真实的总梯度有所偏差（这是随机性造成的），但如果我们对所有可能的小批量梯度取平均（即求期望），那么这个平均值**精确等于**真实的总梯度。

在统计学中，一个估计量（这里是小批量梯度）的期望值如果等于它所估计的真实参数（这里是真实总梯度），那么这个估计量就是**无偏的（Unbiased）**。每次迭代我们都在向**正确的方向**前进，即使每一步都有噪声。由于期望值是正确的，只要我们进行足够多的迭代，这些噪声就会相互抵消，最终会收敛到真实的总梯度方向附近。

- 批量梯度下降 (BGD):**梯度最精确**，但计算成本极高，无法用于大模型。适用于小规模/理论分析
- 纯随机梯度下降 (SGD):只选取一个进行梯度更新。**梯度噪声最大**，但计算成本最低，易于跳出局部极小值。
- 小批量 SGD (Mini-Batch):**平衡**了计算效率和梯度精度，是工业界和研究中最常用的方法。

![image-20251124182229547](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124182229547.png)



batch size越大，梯度的方差越小，因为随机性越弱。

----

# 2.**Batch Size** 的作用、影响以及常见的训练问题

当总样本数 N=10000 个时，如果使用批量大小 $$B=\text{batch\_size}$$ 进行训练，那么遍历完所有样本所需执行的梯度更新次数（即一次 Epoch 完成的更新次数）是：

$$\text{更新次数} = \frac{N}{B} = \frac{10000}{\text{batch\_size}}$$



batchsize越小，更新越频繁,网络调整方向的频率越高,这种频繁更新使得模型在单位时间内看到更多不同方向的梯度信息，理论上可以更快地响应损失函数的变化。

注意：

- Batch Size 过小（梯度过于随机），每次更新的梯度只根据极少数样本计算得出。根据我们之前讨论的**无偏估计**原理，虽然它的**期望值**是正确的，但**方差（噪声）**极大。梯度方向具有**高度随机性**，与真实的整体梯度方向差异过大。这使得优化路径非常曲折和不稳定，就像在一个摇摇晃晃的船上行走，难以稳定收敛。
- Batch Size 过大，虽然梯度方差小，更新方向更精确，但更新频率低。此外，大 Batch Size 可能会导致模型更容易收敛到**尖锐的局部极小值**（泛化能力差），并且需要更多的内存资源。

----

在机器学习训练中，如果 Loss 曲线出现剧烈震荡而不是平滑下降，通常是以下原因造成的：

1.如果 `batchsize=1`（或非常小），每次更新的梯度噪声太大。每次参数更新都是对**单个样本（或极少数样本）的妥协**，而不是对全局损失的优化。这导致参数在不同训练样本的需求之间来回摇摆不定，Loss 曲线表现为剧烈的**锯齿状震荡**。

2.训练样本没有 Shuffle（打乱），例如，前 5000 个样本都是猫的图片，后 5000 个样本都是狗的图片），而没有进行打乱（Shuffle）在训练的前半段 Epoch 中，Batch 中可能全都是猫。模型会将所有参数朝向识别“猫”的方向更新。在训练的后半段 Epoch 中，Batch 中可能全都是狗。模型会突然发现之前的参数方向是错的，于是将参数剧烈地调整朝向识别“狗”的方向。这种剧烈的、周期性的方向调整，会在 Loss 曲线中表现为明显的周期性、大幅度的震荡或周期性的剧烈上升，严重影响训练的稳定性和收敛性。





# 3.逃离尖锐极小值

深度神经网络的损失函数通常是**非凸的**。这意味着损失曲面非常复杂，存在大量的局部极小值（Local Minima）和鞍点（Saddle Points），使得找到全局最优解变得极其困难。

【1】尖锐最小值的危害：参数的微小变化就会导致损失急剧增加，导致模型在未见过的数据（测试集）上表现不佳，即**损失了泛化能力**。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124184218811.png" alt="image-20251124184218811" style="zoom:50%;" />

【2】到达局部尖锐最小值的原因：大 Batch Size 减小了梯度中的噪声，这使得模型更容易收敛到**尖锐的局部极小值** 。LB 方法的梯度**噪声（方差）极小**，路径平滑精确。这使其在优化过程中缺乏**探索性**，一旦进入尖锐最小值的“吸引盆地”（Basin of Attraction），就**无法逃脱**。

实践发现，使用大批量（LB，large batch）方法训练出的模型，虽然在**训练集上的损失值（Training Loss）**与小批量（Small-Batch, SB）方法相似，但在**测试集上的性能（泛化能力）**明显更差。这就是所谓的**泛化差距**。

LB 方法缺乏 SB 方法的**探索性**，倾向于收敛到靠近**初始点**的最小值。SB 和 LB 方法收敛到**本质上不同**的最小值，这些最小值具有不同的泛化特性。LB 方法缺乏泛化能力的原因在于它们倾向于收敛到**尖锐最小值（Sharp Minimizers）**。

SB 方法的梯度具有**大量噪声**。这种噪声就像一个持续的**扰动**，能够帮助优化过程跳出尖锐、狭窄的谷底，从而探索到更宽广、更平坦的最小值区域。

LB 方法的梯度**过于精确和稳定**，缺乏 SB 方法中噪声所提供的**跳出和探索**能力。这导致 LB 方法最终停留在一个对参数微小变化不鲁棒的解上。

----

看一些实践的数据：

![image-20251124194657465](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194657465.png)

![image-20251124194708249](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194708249.png)

![image-20251124194818816](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194818816.png)

-----

**早期停止（Early Stopping）**是一种常用的正则化技术，它通过在验证集损失开始上升时停止训练，来防止模型过拟合。

对于大批量方法产生的**泛化差距（Generalization Gap）**，简单地使用早期停止并不能有效地弥补。这意味着问题不是出在训练时间太长，而是出在**收敛到的那个最小值本身的性质**有问题。存在**比训练时间**更深层次的原因（即最小值的**尖锐度**）导致泛化能力差。

## 3.1最小值的尖锐度测量

最小值的尖锐度在理论上是通过损失函数在最小值点 $$x$$ 处的**海森矩阵 ($$\nabla^2 f(x)$$)** 的**特征值**来表征的。特征值越大，曲率越大，最小值越尖锐。但是在深度学习应用中，模型的参数 $$n$$ 数量巨大，计算和分解海森矩阵的成本**高到无法承受（prohibitive cost）**。

新的尖锐性度量：基于**敏感性（Sensitivity）**的度量方法。这种方法的核心是：通过**探索解 $$x$$ 周围的一个小邻域**，并计算函数 $$f$$（即损失函数）在该邻域内能达到的**最大值**。这个最大值越高，说明损失函数在该点越敏感，即越尖锐。比如计算在 $$x$$ 点附近**损失函数值的相对增幅**。增幅越大，说明在该点稍有偏离损失就会大幅增加，即**越尖锐**。

当然衡量尖锐度的方法肯定有很多，可以查阅更多论文了解。例如：https://arxiv.org/pdf/1609.04836







----



适度的噪声被认为是**有益的**。这种噪声帮助优化算法在损失曲面上**跳出**尖锐的局部极小值和鞍点，引导模型收敛到**平坦的最小值（Flat Minima）**区域，从而提高模型的**泛化能力**。







# 4.鞍点问题-优化过程中的隐形障碍

![image-20251124201234427](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124201234427.png)



深度神经网络的损失曲面（Loss Surface）实际上是一个**混沌的高维地形** 。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124203051499.png" alt="image-20251124203051499" style="zoom:50%;" />

在这个高维地形中，最常见的障碍物不是**局部极小值（Local Minima）**，而是**鞍点（Saddle Points）**。理解鞍点是理解深度学习优化器设计原理的关键。

## 4.1鞍点的数学定义

第一：在鞍点处，损失函数 $$f$$ 的**梯度** $$\nabla f$$ 等于零 ($$\nabla f = 0$$)。---->这会导致优化过程停滞。

第二：函数的**曲率（Curvature）**是**混合的**。在某些方向上，曲面是**向上弯曲**的（像最小值）；在另一些方向上，曲面是**向下弯曲**的（像最大值）。

> **示例函数 $$f(x, y) = x^2 - y^2$$：**
>
> - **梯度：** $$\nabla f = (2x, -2y)$$。在原点 $$(0, 0)$$ 处，梯度为0。
> - **曲率（通过海森矩阵的特征值）：** 特征值为 $$\{2, -2\}$$。由于特征值**符号混合**（一个正，一个负），原点 $$(0, 0)$$ 是一个鞍点。
> - 在多变量微积分中，**海森矩阵（Hessian Matrix）就是二阶导数的矩阵表示**。海森矩阵是多变量函数**二阶导数的完整集合**，它以矩阵的形式体现了函数在各个方向上的弯曲程度。

总结：鞍点是损失曲面上梯度为零的点，但它在某些方向上是最小值，在另一些方向上是最大值。在深度网络中，鞍点比局部极小值更普遍。

## 4.2为什么鞍点是一个问题

鞍点是训练深度神经网络时导致训练效率低下的主要原因。

在鞍点附近，**梯度（Gradient）几乎为零** ($$\nabla f \approx 0$$)。结果就是**收敛停滞（convergence stalls）**，模型在很长时间内都不会有显著进展。当优化器陷入鞍点时，损失值停止下降，但并没有达到一个理想的低值。从业者可能会误认为训练已经**“完成”**，因为损失已经不再变化，但实际上优化器只是**被困在一个平坦的平台（flat plateau）**上。

在低维参数空间中，极小值（Minima）和极大值（Maxima）是主要的驻点。但在**高维空间**（例如拥有数百万个参数的 DNN）中，**鞍点的数量会远超局部极小值**（Dauphin et al., 2014）。这意味着在训练过程中，优化器遇到鞍点的概率远高于遇到局部极小值。

## 4.3如何推断陷入鞍点

由于直接计算鞍点过于复杂，实践中我们通过观察训练动态来**推断**模型是否被鞍点困住:

- 损失值长时间不下降，但并未收敛到接近零的低值。
- 在多个训练步骤中，梯度的**范数（Norm）**（即梯度的整体大小）接近于零。
- 模型参数在鞍点周围轻微“徘徊”（hover），但整体上没有朝着损失更低的方向移动。
- 理论上，可以通过计算**海森矩阵（Hessian）的特征值**。如果特征值**正负混合**，则可以确定该点是鞍点。然而，正如前面所解释的，海森矩阵对于大型网络**计算上不可行（infeasible）**。深度学习框架（如 TensorFlow, PyTorch）**不会检查海森矩阵**，而是依靠优化器的动态特性来自动逃离陷阱。

## 4.4逃离鞍点的策略

现代优化算法的核心设计目标就是提供逃离鞍点所需的“推力”或“方向感”。

(1)小批量 SGD 带来的随机梯度噪声（方差）是一种天然的**扰动**。

(2) **现代优化器（如 Adam, RMSprop）**通过引入动量（Momentum）和自适应学习率机制，能够更有力地推开优化过程，有效**逃离鞍点**。

动量为参数更新添加了**速度（velocity）**，这是基于历史梯度的积累。即使在鞍点梯度消失时，动量也能凭借惯性**带着参数向前冲**，从而越过平坦区域。

这类优化器（Adam、RMSProp）会为**每个参数**动态地缩放（调整）学习率。在鞍点处，优化器能识别出**平坦方向**（梯度变化慢），并给予这些方向**相对较大的步长**，而对**尖锐方向**（梯度变化快）则给予较小步长，从而**加速逃离**。

通过精心调整和衰减学习率，可以防止优化器一开始就以极快的速度冲过低损失区域，避免陷入鞍点后因学习率过低而无法逃脱。

(3)正则化有助于减少网络的冗余和退化，从而**重塑（reshapes）**损失曲面。这可以减少鞍点的数量或使鞍点更容易被逃离。





## 4.5PyTorch如何解决鞍点问题

现代优化器逃离鞍点的策略，都是为了在梯度接近于零的平坦区域提供额外的**推力**和**方向感**。

（1）动量 SGD (SGD with Momentum)：本身SGD有噪声，一定程度上可以缓解鞍点的问题。但是还可以引入一个**惯性项（Momentum Term）**来解决传统 SGD 在鞍点附近和狭长曲面上的震荡和停滞问题。

优化器维护一个**速度缓冲 `buf`**（或称为动量项），它存储了过去梯度的指数加权平均值。当优化器遇到鞍点时 ，虽然当前的梯度很小，但由于 `buf` 中累积了之前的梯度信息，**动量会带着参数继续向前推进**。

**`buf = momentum * buf + grad`**

- `momentum` 是动量系数，通常接近 1 (如 0.9或 0.99)。
- **新速度** (`buf`) 是**旧速度**按动量衰减后的值，加上**当前梯度** (`grad`)。
- 在鞍点处，连续的微小梯度会被累加，形成一个较大的速度向量，从而提供逃逸所需的推力。

参数更新直接使用累积的速度 (`buf`)，而不是原始梯度。

（2）Adam 优化器 (Adaptive Moment Estimation)：目前工业界最流行的优化器之一，它结合了 **动量** 和 **自适应学习率** 的双重优势，使其在逃离鞍点方面表现出色。



# 5.学习的震荡问题

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124211118448.png" alt="image-20251124211118448" style="zoom:50%;" />

损失函数曲面是狭长的，损失函数对不同参数的敏感度差异很大，参数被迫在山谷的**两侧来回剧烈跳跃（Oscillation）**

导致的问题：大部分的学习步长被浪费在无意义的、垂直于目标方向的来回跳动上。只有微小的进步是沿着谷底向真正的最小值方向进行的。如果我们尝试使用较大的学习率来加速沿着谷底的进展，那么在陡峭的垂直方向上，参数就会**发散（Diverge）**，无法收敛。整个训练过程的学习率不得不受到最陡峭方向的限制，因此平坦方向的学习速度就会非常慢。

---

解决：比如**动量（Momentum）：** 通过累积历史梯度，震荡方向的梯度（它们相互抵消）被抑制，而沿着谷底方向的梯度（它们持续累加）被放大，从而加速沿着谷底的进展。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213049497.png" alt="image-20251124213049497" style="zoom:50%;" />

$$\begin{aligned}&\boldsymbol{g}=\frac{\partial\mathrm{Loss}}{\partial\boldsymbol{W}}\\&\boldsymbol{v}_{t}=\alpha\boldsymbol{v}_{t-1}+\varepsilon\boldsymbol{g}\\&\boldsymbol{W}_{t}=\boldsymbol{W}_{t-1}-\boldsymbol{v}_{t}\end{aligned}$$

> 这个方法说白了就是考虑当前梯度和上一次的梯度，做对冲
>
> 如果方向不一致，那么就会对冲，更新步幅会比较稳定
>
> 如果方向一致，那么步子会越迈越大，更加加快学习速度---->巧妙！
>
> 动量法---->既解决学习震荡的问题，又解决学习慢的问题

首先SGD 的第一步，计算当前小批量样本（Mini-Batch）在当前权重 $$\boldsymbol{W}_{t-1}$$ 处的平均梯度 $$\boldsymbol{g}$$。

新速度 $$\boldsymbol{v}_t$$ 是由两部分组成的:一般推荐$$\alpha=0.9$$，$$\varepsilon=0.1$$

1.**历史速度的延续 ($$\alpha\boldsymbol{v}_{t-1}$$):** 参数带着上一步的惯性继续前进。$$\alpha$$ 越大，惯性越强。

2.**当前梯度的推动 ($$\varepsilon\boldsymbol{g}$$):** 当前的梯度信息指导参数朝新的方向移动， $$\varepsilon$$ 控制这个推动力的大小。

最后，更新权重，权重 $$\boldsymbol{W}$$ 是沿着新的**速度向量 $$\boldsymbol{v}_t$$** 的反方向进行更新。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213112882.png" alt="image-20251124213112882" style="zoom:50%;" />

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213143374.png" alt="image-20251124213143374" style="zoom:50%;" />

Sebastian Ruder 在他的论文中简明扼要地描述了动量的影响："动量项在梯度指向相同的维度上会增加，而在梯度改变方向的维度上会减少更新。因此，我们可以加快收敛速度，减少振荡"









# 6.AdaGrad学习因子设定问题（学习率）

AdaGrad（Adaptive Gradient Algorithm，自适应学习算法）是一种革命性的优化器，它首次实现了对**每个参数**的**独立学习率调整**。它的核心动机是解决不同参数对梯度的敏感度差异问题。

在训练过程中，权重向量的不同分量（即不同的参数）可能会有极端的梯度差异。某些参数（例如对应频繁出现特征的权重）的梯度值相对稳定且适中。这是因为它们在每次更新时都使用了大量的样本信息。另一些参数（例如对应不常出现特征的权重，即稀疏数据）的梯度值可能极其巨大且不稳定。这是因为这些参数只在少数几次迭代中被更新，导致偶尔出现的信号被过分放大，造成梯度爆炸。

另外，传统的 SGD 或带动量的 SGD 对所有参数使用单一的全局学习率 $$\alpha$$。如果 $$\alpha$$ 设置得太小，那些梯度小的稀疏参数就更新得太慢；如果 $$\alpha$$ 设置得太大，那些梯度大的参数就会剧烈震荡甚至发散。

AdaGrad 通过为权重向量的**每个组件（每个参数）独立地调整学习率**来解决上述问题。

如果某个参数的历史梯度**积累值很大** $\rightarrow$ 它的学习率应该**变小**。

如果某个参数的历史梯度**积累值很小** $\rightarrow$ 它的学习率应该**变大**。

这样，那些梯度大的稀疏参数的学习率会被迅速抑制，稳定了训练；而那些梯度小的参数能获得更大的推动力，加速了学习，从而有助于解决**梯度消失和梯度爆炸**问题。

AdaGrad 通过累积**过去所有迭代中梯度的平方**来实现自适应调整。

- 累积量 $$v_t$$： 在每一步 $$t$$，AdaGrad 会计算一个累积变量 $$v_t$$（在公式中通常记为 $$G_t$$ 或 $$V_t$$），它是从训练开始到当前步为止，所有历史梯度平方的元素级求和。

  $$\boldsymbol{v}_t = \boldsymbol{v}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t$$

  （其中 $$\odot$$ 表示元素级乘法，$$\boldsymbol{g}_t$$ 是当前梯度）

- 更新方式： AdaGrad 将原始的学习率 $$\alpha$$ 划分为 $$\sqrt{v_t}$$，作为每个参数的新的、自适应的学习率

  $$\boldsymbol{W}_{t} = \boldsymbol{W}_{t-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_t }+ \epsilon} \odot \boldsymbol{g}_t$$

  - **分母 $$\sqrt{\boldsymbol{v}_t}$$：** 作为一个**动态的正则项**，其值越大，该参数的有效学习率就越小。
  - **$$\epsilon$$：** 一个很小的正数，防止分母为零。

最大的优势在于**无需手动调参**（尤其是学习率 $$\alpha$$），因为算法在训练过程中会根据历史数据自动进行调整。非常适合处理**稀疏数据**（如自然语言处理中的词嵌入），能让罕见特征获得更大的更新步长。

> 或者说，学习的越多，W更新的越多，学习因子就大。

最大的问题是 **$$v_t$$ 是一个持续累积正数**的变量，它只会单调递增。这意味着分母 $$\sqrt{\boldsymbol{v}_t}$$ 会越来越大，导致**有效学习率会随着训练的进行而不断、单调地衰减**。在训练的后期，有效学习率变得**极低**，使得模型难以进行有效的参数更新，导致算法**收敛速度非常慢**，甚至提前停止学习。这个局限性直接促成了后续改进算法如 **RMSprop** 和 **Adam** 的诞生。

# 7.RMSprop和Adam

## 7.1RMSprop优化器

**RMSProp (Root Mean Square Propagation)** 优化器，它作为对 AdaGrad 的改进，克服了 AdaGrad 的主要缺陷，由 Geoffrey Hinton 在其 Coursera 课程中提出的，它的主要目标是解决 AdaGrad 学习率单调递减导致后期收敛缓慢的问题。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124222159540.png" alt="image-20251124222159540" style="zoom:50%;" />

在AdaGrad中，二阶矩（即$$v_t$$）是简单的累积相乘，$$v_t$$ 单调递增，学习率无限衰减。学到最后，就学的越来越少了。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124222520938.png" alt="image-20251124222520938" style="zoom:50%;" />

而RMS prop中，二阶矩（即$$v_t$$）是**指数移动平均 (EMA)**：$$v_t = \beta \cdot v_{t-1} + (1-\beta) \cdot g_t^2$$。$$\beta$$ 控制历史信息衰减，学习率可以**保持稳定**（通常接近 1，如 0.9）。

> 公式只涉及 $$t$$ 和 $$t-1$$ 时刻的量，为什么说它和“久远”的历史信息有关？指数移动平均（EMA）的“记忆”机制
>
> 因为$$\boldsymbol{v}_{t-1}$$ 本身就包含了久远的历史信息。
>
> $$\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha \boldsymbol{v}_{t-1}$$
>
> 而$$\boldsymbol{v}_{t-1} = (1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha \boldsymbol{v}_{t-2}$$
>
> 代入得到：$$\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha \left[ (1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha \boldsymbol{v}_{t-2} \right]$$
>
> $$\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha(1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha^2 \boldsymbol{v}_{t-2}$$
>
> 如果将这个过程一直追溯到训练开始（$$t=0$$），可以看到 $$\boldsymbol{v}_t$$ 实际上是**所有历史梯度平方 $$\boldsymbol{g}_i^2$$ 的加权和**：
>
> $$\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha(1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha^2(1-\alpha)\boldsymbol{g}_{t-2}^2 + \alpha^3(1-\alpha)\boldsymbol{g}_{t-3}^2 + \dots$$
>
> 因此，说 $$\boldsymbol{v}_t$$ 具有**“记忆”**功能，并由 $$\alpha$$ 控制这个记忆的**“遗忘速度”**。

AdaGrad 与 RMSProp 维持历史信息的方式对比：

AdaGrad是一个**无限期、不衰减的累加过程**。每一个历史梯度平方 $$\boldsymbol{g}_i^2$$ 对当前的 $$v_t$$ 都拥有**相同的、永久的权重**。$$v_t$$ 的值是一个**非递减序列**，它永远不会忘记过去的梯度。随着训练的不断进行，分母 $$\sqrt{\boldsymbol{v}_t}$$ 会**无限增大**，导致有效学习率 $$\frac{\alpha}{\sqrt{\boldsymbol{v}_t}}$$ 不可避免地、**无限地趋向于零**。在训练后期，即使遇到一个非常重要的、需要大步长更新的梯度，模型也因为学习率太小而无法做出有效响应。

RMSProp是一个**有选择性、指数衰减的平均过程**。通过 $$\beta$$ 系数，**久远的梯度信息权重会指数级地衰减**。模型只“记住”最近一段时间内的梯度平均大小。**$$v_t$$ 的值会收敛到近期梯度平方的平均值附近**，**不会无限增大**。

## 7.2时间累积+自适应梯度

可以把动量 SGD 和 RMSprop 结合起来，公式如下。

$$v_{t}=\rho_{1} v_{t-1}+(1-\rho_{1}) g$$

$$r_{t}=\rho_{2} r_{t-1}+(1-\rho_{2})\langle g, g\rangle$$

$$w_{t}=w_{t-1}-\alpha \frac{v_{t}}{\delta+\sqrt{r_{t}}}$$

$$r_t$$是自适应的学习因子。

不足：t早期冷启动的时候，v和r都会整体偏小

解释：在训练开始时，优化器通常将 $$\boldsymbol{v}_0$$ 和 $$\boldsymbol{r}_0$$ 初始化为**零向量**。由于 $$\rho_1$$ 和 $$\rho_2$$ 通常被设置成接近 1的值（例如 $$\rho_1=0.9$$ 或 $$\rho_2=0.999$$），这意味着历史信息的权重很大，而当前信息的权重 $$(1-\rho)$$ 很小。所以在训练的最初几个时间步中，几乎从零开始，导致 $$\boldsymbol{v}_t$$ 和 $$\boldsymbol{r}_t$$ 的值在训练初期会**系统性地偏小**，尤其是在 $$t$$ 很小时。它们的真实期望值应该比计算出来的值要大。

这种低估偏差导致模型**初始的学习步长过小**。

解决方案：偏差修正 (Bias Correction),就是改进版的adam优化器

## 7.3Adam优化器

为了解决这种冷启动偏差，Adam 引入了**偏差修正项**。其思想是利用 $$\boldsymbol{v}_t$$ 和 $$\boldsymbol{r}_t$$ 的期望值，对它们进行放大，从而消除初始化偏差。

修正后的一阶矩（动量）：

$$\hat{\boldsymbol{v}}_{t} = \frac{\boldsymbol{v}_{t}}{1-\rho_{1}^{t}}$$

修正后的二阶矩（方差）：

$$\hat{\boldsymbol{r}}_{t} = \frac{\boldsymbol{r}_{t}}{1-\rho_{2}^{t}}$$

---

当 $$t$$ 较小（如 $$t=1, 2$$）时，$$1-\rho^t$$ 的值接近于零。用这个接近零的值作除数，会**极大地放大**原始的 $$v_t$$ 和 $$r_t$$。这抵消了它们在初期被低估的偏差。

随着 $$t$$ 增大（例如 $$t \approx 50$$），$$\rho^t$$ 会趋近于0，分母 $$1-\rho^t$$ 趋近于1。此时，$$\hat{v}_t \approx v_t$$，偏差修正项**自动消失**，优化器回归到标准的 EMA 更新。

---

**最终的 Adam 更新公式：**

$$\boldsymbol{w}_{t} = \boldsymbol{w}_{t-1}-\alpha \frac{\hat{\boldsymbol{v}}_{t}}{\delta+\sqrt{\hat{\boldsymbol{r}}_{t}}}$$

通过引入这个偏差修正机制，Adam 确保了在训练初期（冷启动阶段）参数更新的方向和大小是准确的，极大地提高了模型的**训练稳定性**。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124232804313.png" alt="image-20251124232804313" style="zoom:50%;" />



# 8.归一化Normalization和正则化

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124233107638.png" alt="image-20251124233107638" style="zoom:50%;" />





## 8.1为什么要归一化

特征尺度的巨大差异，比如**年龄 (Age)：** 范围 $$[0, 65]$$，**薪水 (Salary)：** 范围 $$[0, 100,000]$$。

模型学习到的**权重（Weights）**在更新时会受到这种尺度差异的强烈影响,显然年龄相关的权重则需要更大的调整才能跟上薪水权重的影响。

另外，当输入特征尺度不一致时，会导致以下问题：

（1）震荡

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124211118448.png" alt="image-20251124211118448" style="zoom:50%;" />

（2）优化器（如梯度下降）必须沿着最陡峭的方向前进，导致它在陡峭的维度上**剧烈震荡**。为了防止在陡峭方向上超调（overshoot）导致发散，我们被迫使用非常低的学习率。结果是，模型沿着平坦的方向进展缓慢，学习过程变慢，收敛效率低下。

## 8.2输入归一化的方法

**Z-score 标准化**或 **Standard Scaling**

显然，归一化之后，损失曲面被“解扭曲”，变得更加**圆形和对称**。优化器可以沿着更直接的路径向最小值前进，不再需要处理剧烈的震荡。我们可以使用**更高的学习率**而不会导致发散，从而实现**更快的收敛**和**更稳定的训练**。

既然这个解决方案如此巧妙，为什么我们不将网络中每一层的激活值都进行归一化呢？

## 8.3批归一化(激活值归一化)

在**每一层**的激活值（在激活函数之前或之后）都进行归一化，确保每一层的输入分布保持稳定，从而进一步加速训练和提高模型性能。

批量归一化（批归一化，Batch Normalization）的提出是为了解决 **内部协变量漂移** 问题。

【1】内部协变量漂移 (Internal Covariate Shift, ICS)

简单来说，在神经网络训练过程中，前一层参数的每一次更新，都会导致该层输出的**激活值分布发生改变**。紧随其后的下一层需要**不断适应**这种不断变化的输入分布，这使得模型难以收敛，导致**收敛速度变慢**和**训练不稳定**。

【2】BN：BN 的思想是像对输入层进行归一化一样，对网络**每一层**的激活值进行归一化，以控制其分布。

BN 在一个 **小批量 (Mini-Batch)** 上计算激活值的均值和方差，对激活值进行中心化（减去 $$\mu$$）和缩放（除以 $$\sigma$$），即完成标准化。

为了不让网络完全丢失归一化之前的表示能力（例如，如果 Sigmoid/Tanh 函数的输入总是被强制归一化到0附近，它们将无法利用非线性区域），BN 引入了两个**可学习的参数**：

- **缩放因子 ($$\gamma$$):** 用于**缩放 (Scaling)** 归一化后的分布。
- **平移因子 ($$\beta$$):** 用于**平移 (Shifting)** 归一化后的分布。

这两个参数会和网络权重一起，通过**反向传播**进行学习和优化。它们允许网络自主决定最佳的分布，例如，如果网络需要一个非零均值或非单位方差的分布来提高性能，$$\gamma$$ 和 $$\beta$$ 可以通过学习恢复这种分布。

$$\text{BN}_{\gamma, \beta}(x_i) = \gamma \cdot \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} + \beta$$

【3】BN 的主要优势

由于网络中每一层的输入分布保持固定，后续层不再需要适应不断变化的输入，从而允许我们使用**更高的学习率**，**加快收敛速度**。BN解决了 ICS，使训练过程更加稳定。

基于 Mini-Batch 的统计量引入了**随机的噪声**，类似于 Dropout 机制，有助于防止过拟合，因此 BN 可以作为一种正则化技术。

【4】 BN 的局限性

（1）对批量大小 (Batch Size) 的依赖，Batch Size太大导致参数更新慢，容易陷入尖锐最小值；BN 假设当前 Batch 的 $$\mu$$ 和 $$\sigma^2$$ 是对整个数据集 $$\mu$$ 和 $$\sigma^2$$ 的良好估计。当 **Batch Size 太小**时，这个估计非常不准确，引入的噪声过大，导致性能**急剧下降**。如下图，batch size过小，错误率越高：

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125112038931.png" alt="image-20251125112038931" style="zoom:50%;" />

（2）测试/推理时间 (Testing Time) 的不一致性

BN 的训练和测试过程需要不同的计算方式。在测试或推理时（例如自动驾驶汽车只输入**单帧图像**），无法再计算有效的 Batch 统计量。网络必须使用在训练期间预先计算和存储的**全局平均值和全局方差**（通过对训练过程中所有 Batch 的 $$\mu$$ 和 $$\sigma^2$$ 进行移动平均估计）。如果训练时的全局统计量与实际测试数据分布存在较大偏差，就会导致 **“在训练和测试中结果不一致”** 的问题。

这些局限性，特别是对 Batch Size 的强依赖，促使社区开发了诸如 **层归一化 (Layer Normalization)**、**实例归一化 (Instance Normalization)** 和 **组归一化 (Group Normalization)** 等替代方法，来避免对 Batch 的依赖。

推荐阅读：https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf

## 8.4正则化

$$\mathcal{L}_{\text{L2}}(\boldsymbol{W}) = \mathcal{L}(\boldsymbol{W}) + \frac{\lambda}{2} \sum_{i} W_i^2$$

$$\mathcal{L}_{\text{L1}}(\boldsymbol{W}) = \mathcal{L}(\boldsymbol{W}) + \lambda \sum_{i} |W_i|$$

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125124738477.png" alt="image-20251125124738477" style="zoom:50%;" />

神经网络比较灵活，不同的层可以使用不同的L1或L2正则。且上方公式的$$\lambda$$不同层可以选择不同的权重。

思考，对于正则化公式的$$\lambda$$，神经网络输入层和输出层哪边的$$\lambda$$更大比较好？

> 答案：对网络深层的权重施加更强的正则化。(答案不唯一，也有人认为浅层应设置更大的值)
>
> 浅层（靠近输入数据）的权重主要负责提取数据的**低级特征**，例如边缘、纹理、颜色等基础模式。这些低级特征往往是**通用且稳定的**，并且在数据集中是**普遍存在**的。对这些基础特征提取器施加**过强的正则化（过大的 $$\lambda$$）**，可能会**过度抑制**网络学习这些基础模式的能力，导致模型性能下降。因此，我们倾向于使用**较小**的 $$\lambda$$。
>
> 深层（靠近输出层）的权重主要负责将前一层提取的**高级语义特征**（例如，物体的特定组合、复杂的概念）映射到最终的**决策或预测**。这些深层权重往往更容易捕捉到训练集中的**偶然噪声和特定模式**，从而导致**过拟合**。它们负责网络最复杂的逻辑判断。为了避免这些高层决策逻辑**过度依赖**训练集中的微小波动或噪声，我们需要对深层权重施加**更强的约束（更大的 $$\lambda$$）**，鼓励它们保持较小的数值，从而提高模型的**泛化能力**。
>
> 在实际应用中，最佳的 $$\lambda$$ 仍然需要通过**交叉验证（Cross-Validation）**在特定的数据集和模型结构上进行实验确定

L1正则的剪枝效果：因为L1正则化倾向于使不重要的权重**精确地等于零**。可以剔除冗余特征，实现**特征选择**。在实际的深度学习训练中，我们通常选择 **L2 正则化**（即权重衰减），因为它能平滑地控制模型复杂度，更好地防止过拟合。只有当我们特别需要**稀疏解**或进行**特征选择**时，才会考虑使用 L1 正则化。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125130337611.png" alt="image-20251125130337611" style="zoom:50%;" />









