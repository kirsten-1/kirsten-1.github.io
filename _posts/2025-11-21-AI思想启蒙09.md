---
layout: post
title: "【AI思想启蒙09】逻辑回归5让学习更高效，数值优化和一只看不见的手 "
subtitle: "Z-score标准化解决梯度尺度不一致、加速收敛；从同方差高斯+贝叶斯生成模型推导出逻辑回归与LDA等价；再由最大似然估计自然得到BCE损失本质为负对数似然"
date: 2025-11-21
author: "Hilda"
header-img: "img/post-bg-2015.jpg"
tags:
- AI思想启蒙
---

<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>





# 1.特征缩放-Z-score标准化/归一化

回顾逻辑回归的损失函数，及其导数：

损失函数是BCE：$$\mathcal{L}_{\text{BCE}}(P || Q) = - \sum_{k \in \{0, 1\}} P(Y=k) \log Q(Y=k) = - [y \log \hat{y} + (1 - y) \log (1 - \hat{y})]=\\ \sum_{i=1}^{n} \left[ y_i \log \frac{y_i}{f_i} + (1 - y_i) \log \frac{1 - y_i}{1 - f_i} \right]$$

导数：损失函数 $$\mathcal{L}_{\text{BCE}}(P || Q)$$对权重向量 $$\mathbf{w}$$ 中任一分量 $$w_j$$ 的偏导数 $$\frac{\partial L_i}{\partial w_j}$$：

$$\frac{\partial L_i}{\partial w_j} = - (y_i - \hat{y}_i) x_{i,j} = (\hat{y}_i - y_i) x_{i,j}$$

总体损失梯度如下：

对于整个训练集（所有 $$N$$ 个样本），总损失 $$L(\mathbf{w}) = \frac{1}{N} \sum_{i=1}^{N} L_i(\mathbf{w})$$ 对 $$w_j$$ 的梯度为：

$$\frac{\partial L}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial L_i}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}$$

---

梯度下降：对单个权重 $$w_j$$ 的更新:

$$w_{j, \text{new}} = w_{j, \text{old}} - \alpha \cdot \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}$$

如果特征都是大于0的取值，根据梯度下降的式子，参数$$w$$只会越来越小。反之如果特征都是小于0的数值，参数只会越来越大。如果用4个象限的等高线描述梯度下降的过程，那么梯度只会往第一象限/第三象限走。此时就会使得收敛过程发生巨大的震荡。

这些都属于**未归一化/未标准化特征**在梯度下降优化过程中的**效率和稳定性**问题。**特征尺度的不一致或不平衡，会导致损失函数的等高线变得极度扁平狭长，使得梯度下降路径低效且震荡。**

解决这个问题的最优方案是使用 **特征缩放（Feature Scaling）**，特别是 **Z-Score 归一化（标准化）**。

每个特征 $$x_j$$ 转换为标准正态分布，使其均值 $$\mu=0$$，标准差 $$\sigma=1$$。

$$x_{\text{new}} = \frac{x - \mu}{\sigma}$$

这是最推荐的解决方案，尤其适用于基于梯度下降的线性模型（如逻辑回归）和神经网络。

当然也可以用Min-Max 归一化 (Normalization)，但是Min-Max 归一化对异常值很敏感。

![image-20251121003720143](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251121003720143.png)

# 2.逻辑回归的总结

1.sigmoid伯努利分布   特殊的softmax（多项分布）

2.BCE  不用MSE

3.L1/L2正则

4.归一化

5.指标  ： 准确率/召回率/查准率/AUC-ROC







# 3.先验条件

一元高斯分布（Univariate Gaussian Distribution），也称为**一维正态分布**，其概率密度函数（Probability Density Function, PDF）为：

<img src="/Users/apple/Library/Application Support/typora-user-images/image-20251121004636434.png" alt="image-20251121004636434" style="zoom: 33%;" />

$$f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

其中：

- $$x$$ 是随机变量的取值。
- $$\mu$$ 是分布的**均值**（Mean），决定了分布的中心位置。
- $$\sigma^2$$ 是分布的**方差**（Variance），决定了分布的形状，即数据的分散程度。$$\sigma$$越大，越分散。
- $$\sigma$$ 是标准差（Standard Deviation）。
- $$\frac{1}{\sqrt{2\pi\sigma^2}}$$ 是归一化常数，确保概率密度函数在整个实数域上的积分为 $1$。

-----

相对论的成功在于它基于最少的公理，得到了最大的推论，构建了一个自洽的宇宙模型。

即：假设（公理）---->推论

机器学习中，这个假设（公理）就是：**对于某一类来说，分布符合正态分布**

> 其实严格来说，判别模型（逻辑回归/softmax回归等）应该符合的是伯努利分布/多项分布（统称为指数族分布）。

# 4.贝叶斯公式与高斯分布的结合

贝叶斯公式是所有生成模型的理论基础。

$$P(y|x) = \frac{P(x|y) \cdot P(y)}{P(x)} \quad \text{（后验概率）}$$

例如身高判断 $$x=170$$ 是男还是女

**$$P(y=1|x)$$：** 在已知身高 $$x$$ 的情况下，是**男**的概率。

**$$P(y=0|x)$$：** 在已知身高 $$x$$ 的情况下，是**女**的概率。

由于 $$P(x)$$ 对两个类别的比较是相同的，我们只需要比较**分子**：

选择$$\quad y = \arg\max_{y \in \{\text{男}, \text{女}\}} \left\{ P(x=170|y) \cdot P(y) \right\}$$

还有已知条件：

$$P(y=\text{男}) + P(y=\text{女}) = 1$$，这是先验概率 (Prior Probability)。

在观察到特定数据 $$x$$（例如，身高 $$x=170$$）之后，其对应的后验概率之和也必须为 1：$$P(y=\text{男}|x=170) + P(y=\text{女}|x=170) = 1$$

----

假设特征 $$x$$ 在每个类别下服从**正态分布**（高斯分布），另外假设**方差 $$\sigma^2$$ 相等**，即两个类别（$$y=1$$ 和 $$y=0$$）的方差相等，即 $$\sigma_1^2 = \sigma_0^2 = \sigma^2$$。

- 类别 $$y=1$$ 的分布：$$P(x|y=1) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu_1)^2}{2\sigma^2}}$$
- 类别 $$y=0$$ 的分布：$$P(x|y=0) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu_0)^2}{2\sigma^2}}$$

计算两个类别似然项的比值 $$\frac{P(x|y=1)}{P(x|y=0)}$$：

$$\frac{P(x|y=1)}{P(x|y=0)} = \frac{\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu_1)^2}{2\sigma^2}}}{\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu_0)^2}{2\sigma^2}}}$$

由于 $$\sigma^2$$ 相等，系数 $$\frac{1}{\sqrt{2\pi\sigma^2}}$$ 被抵消：

$$\frac{P(x|y=1)}{P(x|y=0)} = e^{-\frac{1}{2\sigma^2} \left[ (x - \mu_1)^2 - (x - \mu_0)^2 \right]}$$

化简指数上的项 $$(x - \mu_1)^2 - (x - \mu_0)^2$$ 的负值：

$$\begin{aligned} \text{指数项} &= -(x - \mu_1)^2 + (x - \mu_0)^2 \\ &= - (x^2 - 2x\mu_1 + \mu_1^2) + (x^2 - 2x\mu_0 + \mu_0^2) \\ &= -x^2 + 2x\mu_1 - \mu_1^2 + x^2 - 2x\mu_0 + \mu_0^2 \\ &=  (2x\mu_1 - 2x\mu_0) + (\mu_0^2 - \mu_1^2) \\ &= 2x(\mu_1 - \mu_0) + (\mu_0^2 - \mu_1^2) \end{aligned}$$

将化简后的结果代回似然比的指数：

$$\log \left( \frac{P(x|y=1)}{P(x|y=0)} \right) = \frac{1}{2\sigma^2} \left[ 2x(\mu_1 - \mu_0) + (\mu_0^2 - \mu_1^2) \right]$$

最终目标是后验概率 $$P(y=1|x)$$。在贝叶斯公式中：

$$\frac{P(y=1|x)}{P(y=0|x)} = \frac{P(x|y=1) P(y=1)}{P(x|y=0) P(y=0)} = \left( \frac{P(x|y=1)}{P(x|y=0)} \right) \cdot \left( \frac{P(y=1)}{P(y=0)} \right)$$

取对数，得到**对数几率 (Log-Odds)**：

$$\log \left( \frac{P(y=1|x)}{P(y=0|x)} \right) = \log \left( \frac{P(x|y=1)}{P(x|y=0)} \right) + \log \left( \frac{P(y=1)}{P(y=0)} \right)$$

将结果代入：

$$\log \left( \frac{P(y=1|x)}{P(y=0|x)} \right) = \underbrace{\left[ \frac{1}{\sigma^2} (\mu_1 - \mu_0) \right]}_{W} x + \underbrace{\left[ \frac{1}{2\sigma^2} (\mu_0^2 - \mu_1^2) + \log \left( \frac{P(y=1)}{P(y=0)} \right) \right]}_{W_0}$$

模型的对数几率被表示成了特征 $$x$$ 的**线性函数** $$W x + W_0$$。

$$\log \left( \frac{P(y=1|x)}{P(y=0|x)} \right) = W x + W_0 = \mathbf{z}$$

两边同时取 $$e$$ 的指数：

$$\frac{P(y=1|x)}{P(y=0|x)} = e^{\mathbf{z}}$$

在二分类问题中，事件 $$y=1$$ 和 $$y=0$$ 是互斥且穷尽的，因此它们的概率之和必须为 1：

$$P(y=1|x) + P(y=0|x) = 1$$

从上式可得：

$$P(y=0|x) = 1 - P(y=1|x)$$

代入：

$$\frac{P(y=1|x)}{1 - P(y=1|x)} = e^{\mathbf{z}}$$

现在开始代数求解 $$P(y=1|x)$$（设 $$\hat{y} = P(y=1|x)$$ 方便书写）：

$$\frac{\hat{y}}{1 - \hat{y}} = e^{\mathbf{z}}$$

$$\hat{y} = e^{\mathbf{z}} (1 - \hat{y})$$

$$\hat{y} = e^{\mathbf{z}} - \hat{y} e^{\mathbf{z}}$$

将包含 $$\hat{y}$$ 的项移到等式左侧：

$$\hat{y} + \hat{y} e^{\mathbf{z}} = e^{\mathbf{z}}$$

提取 $$\hat{y}$$：

$$\hat{y} (1 + e^{\mathbf{z}}) = e^{\mathbf{z}}$$

最终解出 $$\hat{y}$$：

$$\hat{y} = \frac{e^{\mathbf{z}}}{1 + e^{\mathbf{z}}}$$

为了得到更标准的 Sigmoid 形式，我们将分子和分母同时除以 $$e^{\mathbf{z}}$$：

$$\hat{y} = \frac{e^{\mathbf{z}} / e^{\mathbf{z}}}{(1 + e^{\mathbf{z}}) / e^{\mathbf{z}}} = \frac{1}{1/e^{\mathbf{z}} + e^{\mathbf{z}}/e^{\mathbf{z}}} = \frac{1}{e^{-\mathbf{z}} + 1}$$

最后，用 $$\mathbf{z} = W x + W_0$$ 替换 $$\mathbf{z}$$：

$$P(y=1|x) = \frac{1}{1 + e^{-(W x + W_0)}}$$

这就是**逻辑函数（Sigmoid 函数）** 的形式。

这证明了在特征服从高斯分布且**方差相等**的假设下，**线性判别分析 (LDA)** 的结果在数学上与 **逻辑回归** 是等价的。

或者说上面的内容论证了**生成模型**（基于贝叶斯公式和高斯分布假设）如何**推导出**这个判别模型的形式。

**将生成模型（假设特征服从高斯分布的贝叶斯分类器）推导至逻辑回归形式**所需的特定条件：

- 满足正态分布
- 方差相等

**当数据满足“高斯分布”和“同方差”这两个条件时，基于贝叶斯和高斯分布的生成模型（即LDA）的决策边界形式，会自然地推导出与判别模型“逻辑回归”完全一致的形式。** 换句话说，在这些假设下，两个模型本质上是相同的线性分类器。如果**类别条件概率**服从**同方差高斯分布**，那么从贝叶斯公式推导出的**后验概率** $$P(y|x)$$ 必然是**逻辑回归**的Sigmoid形式。

# 5.最大似然估计到BCE/KL散度

对于二分类问题，最大似然估计（Maximum Likelihood Estimation, MLE）的目标是找到一组参数 $$\mathbf{W}$$ 和 $$b$$ (或 $$\mathbf{W}$$ 和 $$W_0$$)，使得观测到的训练数据出现的**总概率最大化**。

对于逻辑回归（Logistic Regression）模型，最大化似然函数（Likelihood Function）的等价操作就是最小化**二元交叉熵损失（Binary Cross-Entropy Loss, BCE）**。

下面是详细的推导过程。

-----

【1】逻辑回归的二分类模型

在二分类问题中，我们通常使用逻辑回归（Sigmoid 函数）来建模**后验概率** $$P(y=1|x)$$。

设输入特征为 $$\mathbf{x}$$，模型参数为 $$\boldsymbol{\theta} = \{\mathbf{W}, b\}$$。

1. 线性组合 (Logit):

   $$z = \mathbf{W}^T \mathbf{x} + b$$

2. 类别 1 的概率 (Sigmoid 函数):

   $$\hat{y} = P(y=1|\mathbf{x}; \boldsymbol{\theta}) = \frac{1}{1 + e^{-z}}$$

3. 类别 0 的概率:

   $$P(y=0|\mathbf{x}; \boldsymbol{\theta}) = 1 - \hat{y} = 1 - \frac{1}{1 + e^{-z}} = \frac{e^{-z}}{1 + e^{-z}}$$

【2】似然函数

对于单个训练样本 $$(\mathbf{x}_i, y_i)$$，其中 $$y_i \in \{0, 1\}$$，我们可以将 $$P(y_i|\mathbf{x}_i; \boldsymbol{\theta})$$ 写成一个紧凑的形式：

$$P(y_i|\mathbf{x}_i; \boldsymbol{\theta}) = (\hat{y}_i)^{y_i} \cdot (1 - \hat{y}_i)^{1 - y_i}$$

- 如果 $$y_i = 1$$，则概率为 $$\hat{y}_i^1 (1 - \hat{y}_i)^0 = \hat{y}_i$$。
- 如果 $$y_i = 0$$，则概率为 $$\hat{y}_i^0 (1 - \hat{y}_i)^1 = 1 - \hat{y}_i$$。

假设我们有 $$N$$ 个独立同分布的训练样本 $$(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_N, y_N)$$。**似然函数 $$\mathcal{L}(\boldsymbol{\theta})$$** 是所有样本概率的乘积：

$$\mathcal{L}(\boldsymbol{\theta}) = P(Y|\mathbf{X}; \boldsymbol{\theta}) = \prod_{i=1}^{N} P(y_i|\mathbf{x}_i; \boldsymbol{\theta})$$

$$\mathcal{L}(\boldsymbol{\theta}) = \prod_{i=1}^{N} (\hat{y}_i)^{y_i} \cdot (1 - \hat{y}_i)^{1 - y_i}$$

最大似然估计（MLE）的目标是找到 $$\boldsymbol{\theta}$$ 使得 $$\mathcal{L}(\boldsymbol{\theta})$$ 最大化：

$$\boldsymbol{\theta}_{\text{MLE}} = \arg \max_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})$$

【3】对数似然函数（Log-Likelihood Function）

为了简化计算（将乘积转化为求和，且避免浮点数下溢），我们通常最大化**对数似然函数 $$\ell(\boldsymbol{\theta})$$**：

$$\ell(\boldsymbol{\theta}) = \log \mathcal{L}(\boldsymbol{\theta}) = \log \left( \prod_{i=1}^{N} (\hat{y}_i)^{y_i} \cdot (1 - \hat{y}_i)^{1 - y_i} \right)$$

根据对数运算的性质:

$$\ell(\boldsymbol{\theta}) = \sum_{i=1}^{N} \left[ \log \left( (\hat{y}_i)^{y_i} \right) + \log \left( (1 - \hat{y}_i)^{1 - y_i} \right) \right]$$

$$\ell(\boldsymbol{\theta}) = \sum_{i=1}^{N} \left[ y_i \log (\hat{y}_i) + (1 - y_i) \log (1 - \hat{y}_i) \right]$$

最大化对数似然函数的等价于最大化原始似然函数：

$$\boldsymbol{\theta}_{\text{MLE}} = \arg \max_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta})$$

【4】损失函数：最小化负对数似然（Negative Log-Likelihood）

在优化领域，习惯于将优化问题转化为**最小化损失函数**的形式。因此，我们定义**损失函数 $$J(\boldsymbol{\theta})$$** 为负的对数似然函数（Negative Log-Likelihood, NLL）：

$$J(\boldsymbol{\theta}) = - \ell(\boldsymbol{\theta})$$

$$\boldsymbol{\theta}_{\text{MLE}} = \arg \min_{\boldsymbol{\theta}} J(\boldsymbol{\theta})$$

将 NLL 展开：

$$J(\boldsymbol{\theta}) = - \sum_{i=1}^{N} \left[ y_i \log (\hat{y}_i) + (1 - y_i) \log (1 - \hat{y}_i) \right]$$

【5】得到二元交叉熵损失 (BCE Loss)

最终得到的这个损失函数 $$J(\boldsymbol{\theta})$$ **正是二元交叉熵损失（Binary Cross-Entropy Loss, BCE）**，通常也称为对数损失（Log Loss）。

通常，损失函数还会在前面加上 $$\frac{1}{N}$$ 进行平均：

$$L_{\text{BCE}}(\boldsymbol{\theta}) = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log (\hat{y}_i) + (1 - y_i) \log (1 - \hat{y}_i) \right]$$

----

总结来说：对于逻辑回归模型，**最大化似然函数** $$\mathcal{L}(\boldsymbol{\theta})$$ 在数学上等价于**最小化二元交叉熵损失函数 $$L_{\text{BCE}}(\boldsymbol{\theta})$$**。交叉熵损失来源于最大似然估计在伯努利分布假设下的自然推导。