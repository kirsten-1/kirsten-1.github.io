---
layout: post
title: "Ollama-CLI"
date: 2025-08-18
author: "Hilda"
header-img: "img/post-bg-2015.jpg"
tags:
- Ollama
---

<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>

# 1.什么是Ollama?

Ollama 是一个开源的大型语言模型（LLM）平台，Ollama 提供了简洁易用的**命令行界面**和服务器，使用户能够轻松下载、运行和管理各种开源 LLM，通过 Ollama，用户可以方便地加载和使用各种预训练的语言模型，支持文本生成、翻译、代码编写、问答等多种自然语言处理任务。

【ollama官网】：https://ollama.com

> 任何私人的一些使用问题，建议不要在ollama的github issues中提出，建议在Discord上提：
>
> https://discord.com/invite/ollama

Ollama特点:

- 开源免费：Ollama 是一个开源的本地大型语言模型运行框架，用户可以自由使用、修改和分发，无需支付费用。
- 跨平台支持：Ollama 支持 macOS、Windows、Linux 以及 Docker多种操作系统环境下顺
- 利部署和使用。
- 简单易用：通过命令行方式下载、运行、管理各种开源LLM，使非专业用户也能方便的管理
- 运行各种复杂模型。
- 性能强大：充分利用本地资源，既可以使用GPU也可以使用CPU。
- 易于集成：Ollama 提供了命令行工具和 API，简化了与其他项目和服务的集成。

------

# 2.Ollama下载与安装

Ollama下载地址:

https://github.com/ollama/ollama

无脑下一步安装就可以。推荐安装位置预留出至少10G的磁盘空间（因为后期下载的模型会用到一定的磁盘空间）。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250812202516912.png" alt="image-20250812202516912" style="zoom:50%;" />

在安装 Ollama 应用后，服务会自动在后台运行（以后一般开机会自己后台启动）。如果你想手动启动，可以使用以下命令：

```python
ollama serve
```

通常不应该手动在终端运行这个命令。

Ollama 在安装时，会在 Windows/Linux 上自动设置一个后台服务，在 macOS 上也会有类似的机制。这个后台服务是所有模型生成任务的实际执行者。当用户运行 `ollama run` 或使用第三方 UI 时，它们都是通过这个后台服务来与模型进行通信。

手动在终端运行 `ollama serve` 的弊端：

1. **模型文件路径差异：** 在 Windows 和 Linux 上，后台服务通常以不同的用户身份运行，导致手动运行的服务会把模型文件存放在不同的位置。这会造成“模型消失”的假象。
2. **独占终端：** 手动运行会独占当前的终端窗口，这会阻止用户在同一个终端中执行其他命令，也可能影响第三方工具的正常使用。

唯一推荐手动运行 `ollama serve` 的情况是进行**实验性设置**，比如临时测试不同的环境变量。





# 3.Ollama支持模型

## 3.1ollama中有哪些模型

Ollama支持很多模型，可以通过“https://ollama.com/library”查看可用的模型列表。

可以看到一开始所有模型是按照流行程度Popular排序的：

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250813163916523.png" alt="image-20250813163916523" style="zoom:50%;" />

**在本机Ollama运行7B模型至少需要8GB内存；运行13B模型至少需要16G内存；运行33B模型至少需要32G内存。**

注意："7B"、"13B" 和 "33B" 分别表示模型中参数的数量级，分别为 70 亿、130 亿和 330 亿个参数。模型的参数数量直接影响其对内存的需求，参数越多，模型越复杂，所需的内存也就越大，运行这些模型时，确保系统具有足够的内存。

- Query successful

好的，这是将图片内容转换成 Markdown 表格后的结果：

| Model            | Parameters | Size  | Download                       |
| ---------------- | ---------- | ----- | ------------------------------ |
| Gemma 3          | 1B         | 815MB | ollama run gemma3:1b           |
| Gemma 3          | 4B         | 3.3GB | ollama run gemma3              |
| Gemma 3          | 12B        | 8.1GB | ollama run gemma3:12b          |
| Gemma 3          | 27B        | 17GB  | ollama run gemma3:27b          |
| QwQ              | 32B        | 20GB  | ollama run qwq                 |
| DeepSeek-R1      | 7B         | 4.7GB | ollama run deepseek-r1         |
| DeepSeek-R1      | 671B       | 404GB | ollama run deepseek-r1:671b    |
| Llama 3.3        | 70B        | 43GB  | ollama run llama3.3            |
| Llama 3.2        | 1B         | 1.3GB | ollama run llama3.2:1b         |
| Llama 3.2 Vision | 11B        | 7.9GB | ollama run llama3.2-vision     |
| Llama 3.2 Vision | 90B        | 55GB  | ollama run llama3.2-vision:90b |
| Llama 3.1        | 8B         | 4.7GB | ollama run llama3.1            |

推荐使用模型：大语言模型（Gemma、Deepseek、qwen2）、视觉模型（llava）。

备注：

- Gemma是由 Google DeepMind 团队开发的一系列先进、轻量级的开放模型，灵感源自 Google 的 Gemini 模型。Gemma 模型可用于自然语言处理、计算机视觉、跨模态任务等领域，适用于开发 AI 应用、研究和教育等多种场景。
- LLaVA(Large Language and Vision Assistant)，即大型语言和视觉助手，是一个端到端训练的大型多模态模型，将视觉编码器和大语言模型连接起来实现通用的视觉和语言理解。

## 3.2模型是什么

ollama下载的模型中最大的是权重文件，它是节点的集合，是模型的核心，存储了模型所有学习到的知识和连接方式。它们之间存在节点的权重和偏差，这些权重和偏差称为参数。参数决定了当一个节点被激活时，它会对其他节点产生多大影响。节点通常是一个概念，可能是单词或者短语，甚至是更抽象的语义。在训练模型时，模型会学习大量数据。这个过程中，参数会根据数据不断调整，以更好地捕捉概念之间的关系。参数会以不同的量将这些不同的概念连接在一起（有时候这些节点随着模型训练更近，有时候更远）。比如，当模型学习到两个概念（节点）之间的关系很强时，比如“猫”和“喵”，它们之间的连接（权重）就会变得更强，在比喻中就是它们“更近”。相反，如果两个概念关系不大，连接就会变弱，它们就“更远”。

两个节点之间不会只有一个权重，一般可能会有许多权重组合，具体取决于节点所执行的操作的上下文。比如，在“苹果很甜”这个短语中，“苹果”这个节点可能会和“甜”这个节点产生一个连接。但在“我用苹果电脑”这个短语中，“苹果”这个节点则会和“电脑”这个节点产生另一个不同的连接。这表明，一个词（节点）的含义会随着它所处的句子（上下文）而改变，模型通过调整不同的权重组合来处理这种复杂性。

模型的本质就是通过海量的参数来存储和处理各种概念之间的复杂关系，而训练过程就是不断优化这些参数的过程。



文件的大小，取决于文件最初开发时参数的表示方式。一开始可能使用16位/32位浮点数，这些数字可能非常大且非常精确，但是如果将这些数字分组为更小的组合，可以将其抽象为更小的数字，同时保持令人难以置信的精度。最常见的数字是4，这就是所谓的4位量化。**量化**就是一种**压缩技术**，它将这些又长又精确的数字抽象成更小的数字，同时尽可能地保持模型的性能。

当每个参数都用32位数字表示时，llama  38b或者 80亿个参数将需要大约32GB的显存才能运行。因为一个字节有8位，4个字节参数，所以80亿乘以4加起来就是大约32GB，还有一些额外的开销，但是这是一种简单的计算方法。如果将每个参数量化为4位，则需要接近4-5GB的显存，这更容易访问。



除了模型之外，当然还有其他一些组件。ollama都会下载。



ollama的REPL   ----  一个编码概念。-----read eval print loop---交互式地处理输入，答案将逐个标记流式输出。（这里的标记就是一个单词或者单词的常见部分）

**REPL** 是一个常见的编程概念，全称是**“Read-Eval-Print Loop”（读取-求值-打印-循环）**。在Ollama中，它描述了你和模型交互的过程：

1. **Read（读取）**：你输入一个问题。
2. **Eval（求值）**：模型处理你的问题。
3. **Print（打印）**：模型将答案以**流式输出**的方式返回，这意味着你会看到答案一个**标记（token）**一个标记地出现，而不是等整个答案出来。
4. **Loop（循环）**：这个过程会一直重复，直到你结束对话。 这里的**标记**可以是一个完整的单词，也可以是单词的一部分，这是模型处理文本的基本单位。

一般情况下，模型会有支持的上下文窗口大小，在这个窗口内，它会记住其上下文，让用户觉得对话好像是可以被模型记忆的。模型只会将**上下文窗口内**的标记作为生成下一个回答的依据。如果对话中的标记数量超过了这个窗口大小，最旧的标记就会被“遗忘”。

一般窗口大小是248个标记，在ollama中，这个248大小是可以被修改的。不修改的情况下，超出248个标记，模型就开始忘记了。如果重新启动CLI，之前的历史记录将会被清除。用户通常会通过第三方UI使用ollama，open WEB UI、Msty等等都是常见的UI。一些UI可以提供更长时间的对话。

## 3.3如何选择最佳模型

寻找“最佳模型”没有一个简单快捷的方法，因为它高度依赖于个人的具体使用场景和需求。

不应该盲目相信所谓的“排行榜”或 Discord 社区中的传言，因为这些信息可能不适用于你的特定任务。一个在通用任务上表现优异的模型，可能在你的特定专业领域表现不佳。

寻找最佳模型的正确方法是进行**个性化的、重复性的测试**。具体步骤如下：

1. **准备代表性问题：** 整理一份包含你日常会问的、具有代表性的问题清单。
2. **多次重复测试：** 对于每个模型，用同一个问题进行至少 **10 次独立的测试**。
3. **观察和分析：** 观察模型在多次测试中的回答倾向和质量。为什么需要多次？因为模型的输出存在一定的随机性。只问一次，可能会得到比平时更好或更差的回答，从而产生误判。通过多次测试，才能真正了解模型的“平均”表现。

尽管这种方法耗时，但这是目前唯一可靠的、能找到最适合你个人需求的模型的方式。

# 4.Ollama 快速上手

通过“`ollama run +模型`”来运行模型

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250812205005215.png" alt="image-20250812205005215" style="zoom:50%;" />

不同的参数对应不同的命令。

以运行“gemma”模型为例演示如何使用Ollama。

打开cmd命令窗口，输入“ollama run gemma:2b”运行Gemma模型，如果Ollama没有该模型，会自动下载模型后再运行，后续运行不会重复下载模型。

## 命令行方式使用

`ollama -h` 或 `ollama --help` 可以列出所有命令。`ollama run <model_name>` 是启动模型的常用命令，它会自动下载模型（如果本地没有），并进入交互模式。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250813154357699.png" alt="image-20250813154357699" style="zoom:50%;" />

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250813154431962.png" alt="image-20250813154431962" style="zoom:50%;" />

Ollama支持命令行方式使用模型，也支持API方式使用模型，这里先演示命令行方式使用模型。

### (1)查看已经下载过的模型

```python
ollama list   或者ollama ls
```

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250812211923831.png" alt="image-20250812211923831" style="zoom:50%;" />



**`ollama show`：** 用于查看已安装模型的详细配置，包括支持的最大上下文长度、许可证、参数和提示模板等。它是一个非常有用的**模型自省（introspection）**工具。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814103325836.png" alt="image-20250814103325836" style="zoom:50%;" />

### (2)拉取模型

```python
ollama pull gemma3
```

这个命令会从 Ollama 仓库中拉取 `gemma3` 模型到你的本地计算机。

如果使用 API，需要先手动 `pull` 模型，而 `ollama run` 会自动执行pull这个步骤。

### (3)运行模型

`ollama run` 是你与模型进行交互的核心命令。它会在你的终端中启动一个聊天会话，让你直接向模型提问。

```python
ollama run [模型名称]
```

比如：

```python
ollama run gemma3
```

**命令执行过程：**

- 如果 `gemma3` 模型尚未下载，此命令会自动开始下载。
- 下载完成后，它会立即启动一个交互式会话（REPL）。
- 你会看到一个提示符（通常是 `>>>` 或类似的符号），表明模型正在等待你的输入。
- 现在你可以开始与模型对话了！

除了基本的交互式会话，你还可以通过以下方式更灵活地使用 `ollama run` 命令。

你可以通过两种方式提供多行输入或直接在命令行中给出提示，而不是进入交互模式：

- 直接在命令行提供提示：将提示内容放在双引号中，直接作为 `ollama run` 命令的参数。这在你只需要模型快速回答一个问题时非常方便。（不多次交互）

```python
ollama run gemma:2b "java语言和go语言的区别是什么"
```



- 使用三引号进行多行输入:提示内容较长，需要多行输入，可以使用三引号 `"""` 将内容包裹起来。

```python
ollama run gemma:2b """
这是一个关于编程语言的问题：
请详细对比Java和Go语言在并发处理、内存管理和生态系统方面的异同。
"""
```





### (4)移除本地模型

```python
ollama rm gemma3
```

值得注意的是，如果你删除了一个被复制（`cp`）过的模型，并不会释放大量空间，因为原始文件仍然存在。





### (5)查看当前正在运行的模型

```python
ollama ps
```

这个命令在你运行了多个模型或服务时非常有用。





### (6)停止正在运行的模型

```python
ollama stop gemma3
```







### (7)显示 Ollama 版本信息

```python
ollama -v
```





### (8)退出会话与清理

当你与模型对话结束后，可以通过以下方式**之一**退出交互式会话：

- 输入 `/bye` 然后按回车。
- 按下 `Ctrl + D` 快捷键。

如果你确定某个模型不再需要，可以使用 `ollama rm` 命令将其从本地删除，以释放存储空间。



### (9)run的高级命令

如果你想了解模型执行的效率和更多细节，可以在 `ollama run` 命令中添加 `--verbose` 参数。

```python
ollama run gemma:2b --verbose
```

返回中会包含如下信息：

- total duration：表示整个运行过程所花费的总时间。
- load duration：表示加载模型所花费的时间，单位为毫秒。
- prompt eval count：表示在处理提示（prompt）时评估的标记（token）数量。
- prompt eval duration：表示评估提示所花费的时间，单位为毫秒。
- prompt eval rate：表示评估提示时的速度，以每秒处理的标记数量表示。
- eval count：表示在生成响应时评估的标记数量。
- eval duration：表示生成响应所花费的时间，单位为毫秒。
- eval rate：表示生成响应时的速度，以每秒处理的标记数量表示。

还有其他高级用法：

- `--format json`：强制模型以 JSON 格式输出结果。这对于需要将模型集成到其他程序中进行自动化处理的开发者非常重要。
- `--keep-alive`：控制模型在空闲多久后卸载。这在多个应用程序需要访问模型时很有用，可以防止模型频繁地加载和卸载，从而提高响应速度。

### (10)复制

**`ollama cp` (copy)：** 复制一个模型的引用，而非完整的模型文件。这非常高效，因为它只创建一个指向原始模型文件的“软链接”，几乎不占用额外磁盘空间。











# 5.Ollama 模型分层结构

Ollama 的设计理念就是将一个大语言模型的所有必要组件打包在一起，形成一个可下载、可直接运行的“模型包”。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814093136055.png" alt="image-20250814093136055" style="zoom:50%;" />

## 5.1分层打包机制

1. **Model (模型)** - LLM 的核心，理解模型如何存储知识、如何平衡性能与资源。

arch (架构): 理论框架。它决定了模型如何组织信息，如何进行推理，所以不同的模型在某些任务上表现不同。

parameters (参数): 模型的知识量。参数越多，模型学习过的知识就越丰富，掌握的知识也越多。但需要权衡：知识量越大，运行它所需的显存就越大。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814093951996.png" alt="image-20250814093951996" style="zoom:30%;" />

之前提过，**248 token**，指的是 **Ollama 命令行界面（CLI）的默认上下文窗口大小**。这是软件层面的一个**默认设置**，为了方便用户快速启动和运行，以及在一些旧设备上保持性能而设定的。这个默认值是可以被修改的。

上图中的`context_length`是 **模型本身设计的最大上下文窗口大小**。这才是模型的“硬件能力”上限，代表了模型在设计时，其架构所能处理的最大标记（token）数量。

quantization (量化): 这是一种优化技巧，是学习如何“精简”知识。量化后的模型能在普通电脑上运行，以及它在性能上可能会有细微的损失（精度）。

2. **License (许可证)** - LLM 的伦理与法律，开源软件的法律框架和使用规范。

3. **Params (参数)** - LLM 的调优与控制，如何通过调整超参数来控制模型的行为。

temperature (温度): temperature 参数就是控制模型“创造性”的旋钮。当你想让模型写一首严谨的诗（低温度）时，你需要把它调低；当你想让它进行头脑风暴（高温度）时，你可以调高。

理解这些参数的作用，能更好地控制模型的输出，使其更符合需求。这对于开发应用、进行研究和进行创意写作都至关重要。

4. **Template (模板)** - LLM 的沟通技巧，如何用最有效的方式与模型进行沟通（Prompt Engineering）。

<|start|>system|<|message|>: 这就像学习一门语言的语法结构。不能随意地说话，而是要按照特定的格式（语法）来组织你的句子，模型才能正确理解你的意图。这个模板就是模型所理解的“语法”。

You are ChatGPT...: 这就是角色设定。学习如何设定一个好的角色，能极大地影响模型的回答质量。当你告诉模型“你是一位专业的历史学家”时，它会更倾向于给出严谨、专业的回答，而不是随意地胡编乱造。

template，就是学习 “提示工程”（Prompt Engineering）。它教会我们，模型的强大能力需要通过精确的指令才能被激发。掌握正确的沟通模板，是与 LLM 有效协作的关键技能。

模型可能还有其他层，例如特定的**系统提示**（`system prompt`）、**适配器**（`adapter`）等，以满足不同模型的特定需求。

-----



## 5.2设计源自于Docker

其实这些设计好像和docker的很类似。

Ollama 的模型包不是一个单一的巨大文件，而是由多个独立的“层”构成的，这种设计思想源于 Docker 容器技术。

Docker 容器的核心是“分层”。一个容器镜像由多个层叠加而成。例如，一个底层可能包含操作系统的基本文件，一个上层可能包含 Node.js 运行时。当你想运行一个 Python 容器时，如果它和 Node.js 容器共享相同的底层操作系统层，那么 Docker 就不需要重新下载这个共享层，只需要下载特定于 Python 的新层。这大大提高了下载效率和存储空间的利用率。

Ollama 借鉴了这一思想。一个完整的 Ollama 模型包也被设计成由多个独立的层组成，而不是一个单一的整体。

这种分层结构让 Ollama 具备了与 Docker 相似的优点：**高效的资源利用和下载。** 如果多个模型共享某个相同的组件（比如某个通用的模板或者许可证），Ollama 就不需要重复下载，从而节省了存储空间和带宽。

Ollama 的这种分层结构在用户界面上是可见的，并且用户可以查看每个层的具体内容。用户点击这些层，就可以看到该层的详细内容或描述。

这种透明性（transparency）对于学习和理解模型至关重要。它让用户可以清楚地知道自己正在使用什么，许可证是什么，以及如何正确地与模型进行交互。这对于开发者和研究者来说尤其有用，因为它提供了对模型内部结构和配置的直接洞察。



## 5.3Tags标签

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814095607928.png" alt="image-20250814095607928" style="zoom:50%;" />

Ollama 的 `Tags` 用于标记和区分同一个模型的不同版本（例如，不同的量化方式或参数数量），但其中一个名为 `latest` 的标签存在误导性。

对于一个模型（例如 `llama3.1`），Ollama 提供了多个不同的标签。这些标签通常代表了该模型的不同变体，比如不同大小（`8b`、`70b`）和不同量化方式（`q4_0`、`q8_0`）。这让用户可以根据自己的硬件和需求，选择最合适的版本。

尽管名字叫 `latest`（最新的），但它实际上**并不代表最新的模型版本**。在 Ollama 的语境中，`latest` 标签被用来指代 **“最受欢迎”或“最常用”的版本**。

最受欢迎的版本通常是：

- 参数量较小（例如 7B、13B）。
- 采用 **4-bit 量化**。这是因为它在**性能（速度）和质量（准确性）之间取得了最佳平衡**，对于大多数用户来说，是一个既能快速运行又不会有明显质量损失的选择。



标签下面的一串哈希值是每个模型层的唯一标识符。如果不同的标签具有相同的哈希值，则表明它们是**彼此的别名（aliases）**，指向同一个模型文件。这解释了为什么有些模型看起来有多个标签，但实际上是同一个文件。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814095905350.png" alt="image-20250814095905350" style="zoom:50%;" />



# 6.Ollama自定义模型

`ollama create` 是 Ollama 的核心功能之一，允许用户通过一个 `Modelfile` 来创建和自定义新模型。

`Modelfile` 是一个简单的文本文件，用于定义新模型的配置。它类似于 Dockerfile，告诉 Ollama 如何构建模型包。它将模型权重文件、系统提示、模板等组件打包成一个完整的 Ollama 模型。

## 6.1创建方式一：基于现有模型微调系统提示

这是最简单的方式。可以基于一个已有的 Ollama 模型（例如 `gemma3:latest`），然后添加或修改 `SYSTEM` 提示、temperature等参数。

更多Modelfile设置的参数及内容参考:https://github.com/ollama/ollama/blob/main/docs/modelfile.md

下面是一个例子，举例创建了一个“护肤咨询师”模型的例子，这展示了如何通过一个简单的 `Modelfile` 来赋予模型一个全新的角色。

比如自定义一个模型（基于`gemma3:latest`），写了temperature和SYSTEM提示参数，写了如下的modelfile

```python
FROM gemma3:latest
PARAMETER temperature 0.9
PARAMETER num_ctx 4096
SYSTEM """
你是一位具备专业化学、皮肤科学背景的私人护肤咨询师，你的核心职责是从分子结构、生化反应机制等化学层面，精准解析各类护肤成分的作用原理，并结合皮肤生理学知识提供个性化建议。

### 知识范围与深度要求：
1. **成分化学解析**：需掌握常见及小众护肤成分的化学名称（含IUPAC命名）、分子结构、官能团特性，能明确其发挥功效的化学基础（如羟基、羧基的保湿作用，酚羟基的抗氧化机制等）。
2. **作用机制拆解**：对成分的皮肤作用路径进行分子级解释，例如：
   - 美白成分如何通过抑制酪氨酸酶活性、阻断黑色素转运等生化反应起效；
   - 抗衰成分如何通过激活成纤维细胞、促进胶原蛋白肽键合成等机制发挥作用；
   - 防晒成分如何通过吸收（化学键能级跃迁）、散射（粒径与光波长相互作用）紫外线实现防护。
3. **成分交互与限制**：能分析不同成分复配时的化学兼容性（如酸性成分与烟酰胺的水解风险、抗氧化成分与金属离子的氧化还原反应），以及浓度、pH值对成分稳定性和功效的影响。

### 输出规范：
- 回答需先明确成分的化学本质（如“透明质酸是由D-葡萄糖醛酸和N-乙酰葡糖胺交替连接形成的直链多糖，分子量范围通常为5000-2000000Da”），再阐述作用机制，最后结合实际场景（如肤质、使用环境）给出建议。
- 对用户提及的成分，若存在不同化学形态（如维生素C的L-抗坏血酸、抗坏血酸葡糖苷等衍生物），需区分其结构差异对透皮吸收、稳定性的影响。
- 避免模糊表述，需用化学术语精准描述（如“抑制环氧合酶（COX）活性”而非“消炎”，“促进角质形成细胞脱落”而非“去角质”）。

当用户咨询护肤品成分、功效或护肤方案时，严格遵循以上设定，以化学和皮肤科学为基础，提供专业、精准、可追溯的解答。
"""
```

关于参数的说明：

- temperature：值越高，模型的回答就越有创意。(默认值：0.8）取值范围0-1

创建的命令是：

```python
ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>
```

比如我将上面的modelfile基于`gemma3:latest`成功创建了我私人可以随时提问的护肤咨询师Lora：

```python
ollama create Lora -f /Users/apple/Documents/PythonWorkSpace/OllamaWorkSpace/modelfile
```



<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814133101730.png" alt="image-20250814133101730" style="zoom:50%;" />

查看Lora模型信息：

```python
ollama show Lora:latest
```

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814133400142.png" alt="image-20250814133400142" style="zoom:50%;" />

运行进行简单的测试：

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814134009776.png" alt="image-20250814134009776" style="zoom:50%;" />

回复的其实有问题。但是角色是没有问题的。

### 误区：联网的吗？

Ollama 的核心设计理念就是**离线运行**，它使用的所有模型权重文件都存储在你的本地计算机上。当 `ollama run` 命令执行后，模型文件会被加载到你的计算机内存（RAM）或显卡显存（VRAM）中进行推理计算。

回复如果慢的根本原因通常是**计算资源不足**，具体来说是**模型太大**和**硬件性能瓶颈**。

## 6.2创建方式二：基于 Hugging Face 权重文件

1. **支持的格式和架构：** 文本提到此功能主要支持 `safetensors` 和 `gguf` 格式的权重文件，并且只适用于特定的架构，如 Llama、Mistral 和 Gemma。
2. **流程：** 用户需要先下载权重文件，然后创建一个 `Modelfile`，指定 `FROM` 为本地权重文件路径。
3. **自动转换与量化：** `ollama create` 命令会自动将 `.safetensors` 文件转换为 `.gguf` 格式，并根据用户指定的参数（例如 `--quantize q4_0`）进行量化，从而生成一个可以在 Ollama 上运行的优化模型。

-----

这种方法适用于你想要使用 Ollama 社区中没有的，或者你自己微调过的模型权重文件。它能够将社区中流行的 `.safetensors` 或 `.gguf` 文件转换、量化并集成到 Ollama 生态中。

从 Hugging Face 仓库中下载想要的 `.safetensors` 或 `.gguf` 格式的模型权重文件。推荐使用 `hugging downloader` 工具来提高下载效率。

下载的safetensors地址：https://huggingface.co/ClosedCharacter/Peach-9B-8k-Roleplay/blob/b166804ead437c90474428789b1b29f105ca7da2/model-00004-of-00004.safetensors

如果直接下载会有2.6G，速度可能会很忙，建议可以试试`hugging downloader` 工具.

首先下载工具：在你希望的目录中执行

```python
bash <(curl -sSL https://g.bodaay.io/hfd) -h
```

脚本会根据操作系统/体系结构下载正确的版本，并将二进制文件保存为当前文件夹中的 "hfdownloader"。

但是这个一直安装不了。于是我转而尝试：[huggingface-cli](https://padeoe.com/huggingface-large-models-downloader/#4.1-huggingface-cli)

```python
pip install -U huggingface_hub
```

许多模型发布时只提供原始的 `.safetensors` 文件，因为它是一种通用的、可信赖的格式。可以下载这些 `.safetensors` 文件，然后使用 Ollama 内置的功能，或者专门的转换工具，将它们转换成 `.gguf` 格式并进行量化。

需要下载：

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814151758630.png" alt="image-20250814151758630" style="zoom:50%;" />

这四个文件都是模型的权重分片，需要将它们**全部下载**。

```python
from huggingface_hub import hf_hub_download
import os

repo_id = "ClosedCharacter/Peach-9B-8k-Roleplay"
filenames = [
    "model-00001-of-00004.safetensors",
    "model-00002-of-00004.safetensors",
    "model-00003-of-00004.safetensors",
    "model-00004-of-00004.safetensors",
    "tokenizer.model", # 通常也需要下载 tokenizer 文件
    "tokenizer_config.json",
    "config.json",
    "special_tokens_map.json"
    # ... 也可以下载其他必要文件
]

local_dir = "./Peach-9B-8k-Roleplay-safetensors"
os.makedirs(local_dir, exist_ok=True)

for filename in filenames:
    hf_hub_download(repo_id=repo_id, filename=filename, local_dir=local_dir)
```

**注意：** `safetensors` 文件只是权重，要让模型正常工作，通常还需要下载 **`tokenizer.model`** 和 **`config.json`** 等配置文件。

> 注：上面下载了4个分片。

创建一个 `Modelfile`，使用 `FROM` 指向你下载的 `.safetensors` 文件（通常是 `model.safetensors` 或 `model-00001-of-00004.safetensors` 等分片中的一个，Ollama 会自动找到所有分片）。

```python
FROM ./Peach-9B-8k-Roleplay-safetensors/model-00001-of-00004.safetensors
SYSTEM "Your custom system prompt here."
# 你也可以在这里设置量化参数，或者在命令行中设置
# PARAMETER quantization Q4_0
```

运行 `ollama create` 命令。**Ollama 会自动检测 `.safetensors` 文件，将其转换为 `.gguf` 格式，并根据你的配置进行量化**。

```python
# 在终端中执行，并指定量化方式
ollama create my-new-peach -f Modelfile --quantize Q4_0
```

但是上面一个分片已经达到接近5G：

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814155417076.png" alt="image-20250814155417076" style="zoom:50%;" />

太吓人了。于是为了不下载巨大的 `.safetensors` 文件，也不希望进行本地转换的话，可以直接寻找 `.gguf` 格式模型：https://huggingface.co/QuantFactory/Peach-9B-8k-Roleplay-GGUF/tree/main

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814155526295.png" alt="image-20250814155526295" style="zoom:50%;" />

有多种量化级别可供选择。我选择 `Peach-9B-8k-Roleplay.Q3_K_S.gguf` ，大概是3.6GB。然后创建一个新文件，并将其命名为 `Modelfile`。这个文件应该和下载的 `Peach-9B-8k-Roleplay.Q3_K_S.gguf` 文件放在**同一个文件夹**内。

modelfile内容如下：

```python
FROM ./Peach-9B-8k-Roleplay.Q3_K_S.gguf

TEMPLATE "{{ .Prompt }}"
```

然后创建模型：

```python
ollama create my-peach-model -f Modelfile
```

这个过程通常很快，因为它只是在创建模型的引用，而不是再次下载文件。

运行模型：

```python
ollama run my-peach-model
```

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250814162725849.png" alt="image-20250814162725849" style="zoom:50%;" />

方法可以，但是模型参数较小，效果不是很好。

## 6.3上传自定义模型到 ollama.com

`ollama push` 命令允许用户将自己创建的自定义模型分享到 `ollama.com` 社区。

**前提条件：**

1. **创建账号：** 用户需要在 `ollama.com` 网站上注册一个账号。
2. **设置 Ollama 密钥：** Ollama 在安装时会自动生成一个公钥。用户需要将这个公钥复制到 `ollama.com` 个人资料页面的“Ollama Keys”设置中。这个公钥-私钥机制用于身份验证，确保只有本人能上传模型。

**上传流程：**

1. **重命名模型：** 需要将模型重命名为 `username/model_name` 的格式。例如，如果用户名是 `m`，模型名是 `peach`，则需要先用 `ollama cp peach m/peach` 命令进行重命名。
2. **执行 `ollama push`：** 运行 `ollama push m/peach` 命令即可将模型上传到 `ollama.com`。



这个功能使得 Ollama 不仅仅是一个模型下载工具，更是一个**模型分享和协作平台**，鼓励用户创建和分享自己的微调模型或模板。

----



