---
layout: post
title: "2-7 线性回归的梯度下降"
subtitle: "线性回归问题运用梯度下降法（求偏导）"
date: 2025-03-03
author: "Hilda"
header-img: "img/post-bg-2015.jpg"
tags:
- 机器学习-吴恩达
---


<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


吴恩达课程系列

[1-1 欢迎参加《机器学习》课程【机器学习简介/例子】](https://kirsten-1.github.io/2025/02/28/%E5%90%B4%E6%81%A9%E8%BE%BEML1-1/)

[1-2 什么是机器学习【机器学习的2个定义和分类】](https://kirsten-1.github.io/2025/03/01/%E5%90%B4%E6%81%A9%E8%BE%BEML1-2/)

[1-3监督学习【监督学习的定义，监督学习的分类（回归与分类）】](https://kirsten-1.github.io/2025/03/01/%E5%90%B4%E6%81%A9%E8%BE%BE1-3/)

[1-4无监督学习【无监督学习的定义，无监督学习问题的分类（聚类/信号分离/降维）】](https://kirsten-1.github.io/2025/03/02/%E5%90%B4%E6%81%A9%E8%BE%BEML1-4%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/)

[2-1模型描述【如何描述一个模型（用一些符号），单变量线性回归是什么？】](https://kirsten-1.github.io/2025/03/02/%E5%90%B4%E6%81%A9%E8%BE%BEML2-1/)

[2-2~2-4代价函数【代价函数的数学定义、代价函数的直观理解】](https://kirsten-1.github.io/2025/03/02/%E5%90%B4%E6%81%A9%E8%BE%BEML2-2to2-4/)

[2-5 梯度下降【梯度下降的数学原理】](https://kirsten-1.github.io/2025/03/02/%E5%90%B4%E6%81%A9%E8%BE%BEML2-5/)

[2-6梯度下降知识点总结【导数项、学习率的直观理解】](https://kirsten-1.github.io/2025/03/03/%E5%90%B4%E6%81%A9%E8%BE%BEML2-6/)

-----

# 2-7 线性回归的梯度下降

回顾梯度下降算法和线性回归模型。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250303152423781.png" alt="image-20250303152423781" style="zoom:50%;" />

上面是梯度下降算法（2个参数，对应上面线性回归模型的2个参数），下面是之前学习过的线性回归模型。

现在为了线性回归问题运用梯度下降法，关键在于求出代价函数的导数：

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250303152700343.png" alt="image-20250303152700343" style="zoom:50%;" />

然后带入得到：

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250303152749022.png" alt="image-20250303152749022" style="zoom:50%;" />

直到收敛为止即可。
