---
layout: post
title: "【AI思想启蒙08】逻辑回归4让模型看的更准更稳，正则优化 "
subtitle: "逻辑回归训练中的两大关键问题：1. 参数冗余导致权重无限膨胀、过拟合，需用L2（防溢出、平滑）或L1（特征选择）正则化约束；2. 特征尺度不一致导致梯度椭圆、收敛慢，需归一化（推荐Z-score）使特征同尺度、加速收敛并防止激活函数饱和。"
date: 2025-11-20
author: "Hilda"
header-img: "img/post-bg-2015.jpg"
tags:
- AI思想启蒙
---

<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>



# 1.参数冗余性

对于同一条决策边界（直线），可以有**无数个** $$W$$（或 $$W$$ 和 $$W_0$$）进行表达。例如，如果 $$W$$ 和 $$W_0$$ 变为 $$-W$$ 和 $$-W_0$$，决策边界不变，但预测概率 $$f(x)$$ 变为 $$1-f(x)$$。

另外，如果$$W$$变成$$10W$$，此时要考虑参数大小的影响（过拟合风险）

当权重 $$W$$ 很大时（例如 $$W=100$$），输入 $$x$$ 的微小变化会导致 $$\text{Sigmoid}$$ 函数（$$\frac{1}{1+e^{-(Wx+W_0)}}$$）的输出变化非常大。

这表明**大的 $$W$$ 值会使模型对输入敏感（高方差）**，容易导致**过拟合**。这解释了为什么在训练中需要使用**正则化**来约束 $$W$$ 的大小。

在没有正则化的情况下，优化器可能会在这些等效的 $$W$$ 组合中来回震荡，导致训练过程不稳定，并且最终得到的 $$W$$ 缺乏可解释性。

逻辑回归模型在没有正则化时，一个潜在的问题就是**数值稳定性和过度自信**。

# 2.正则化的概念和作用

正则化用于防止模型在训练数据上**过拟合**。

正则化通过惩罚大的权重 $$W$$，降低模型的复杂度，使模型在 **测试集** 上的误差（泛化误差）尽可能小。

L2 正则化（也称为 **Ridge 正则化**或**权重衰减**）通过在标准损失函数 $$J(\theta)$$ 中添加一个与权重向量 $$W$$ 的平方范数成比例的惩罚项来实现。

**L2 正则化后的损失函数 $$J_{\text{reg}}(W)$$：**

$$J_{\text{reg}}(W) = \underbrace{J(W)}_{\text{原始损失（交叉熵）}} + \underbrace{\lambda \sum W_i^2}_{\text{L2 正则项}}$$

其中 $$\lambda$$ 是正则化系数，用于控制惩罚的强度。

【正则化的作用】：

1.从机器角度考虑：抑制 W 在分类正确情况下，按比例无限增大

> 在训练集中，一旦模型找到了一个能够正确分类所有样本的权重 $$W$$，那么将 $$W$$ 扩大任意倍数 $$c > 1$$（即 $$cW$$），模型损失（交叉熵）会更小。但是随着 $$W$$ 无限增大，模型的损失会**无限减小**。在数学上，梯度下降会推动 $W$ 不断增大，直到发生**数值溢出**
>
> 此时加入正则化就不同了。L2 正则项 $$\lambda \sum W_i^2$$ 成为一个**“约束”**。当 $$W$$ 增大时，惩罚项也会**二次方地快速增大**。这迫使优化器在减小原始损失 $$J(W)$$ 的同时，必须**付出越来越大的代价来增加 $$W$$**。最终，模型会找到一个 **$$W$$ 相对较小**的解，这个解既能正确分类，又能避免 $$W$$ 无限膨胀和潜在的数值溢出问题。

2.减少测试集和训练集的差异性（提高泛化能力）

> 过拟合的模型过度拟合了训练数据中的**噪声**，导致其学习到的权重 $$W$$ 过于复杂和极端。这种**复杂的 $$W$$ 使得决策边界在训练集上表现完美，但在测试集上表现糟糕**。
>
> L2 正则化倾向于使所有权重 $$W_i$$ 的值**趋于零**。
>
> 小的 $$W_i$$ 意味着模型对单个输入特征 $$x_i$$ 的依赖程度减小(对噪声的依赖也会减小)。这使得模型的决策边界更加**平滑**和**简单**，降低了模型的方差。
>
> L2 正则化强制模型关注那些对大多数样本都有效的**核心特征**，忽略训练集中的噪声和异常值，从而**提高模型在测试集上的泛化能力**，减小训练集和测试集性能的差异。

3.破坏训练集的效果（引入偏差，降低方差）

> 从统计学的偏差-方差权衡角度来看，正则化通过牺牲一点训练集的完美性，来换取测试集的稳定性。
>
> 正则化项 $$\lambda \sum W_i^2$$ **不是在帮助最小化训练误差**，它是在**增加一个额外的损失**。为了让 $$\sum W_i^2$$ 减小，模型必须将权重 $$W$$ 往零的方向拉，即使这会导致原始的交叉熵损失 $$J(W)$$ 略微增大。
>
> 因为模型被迫将 $$W$$ 约束得很小，它可能无法完美拟合训练集中的每一个点，导致训练集上的损失（偏差）略微增加。
>
> 正则化使得模型对数据的微小变动（即测试集与训练集的差异）不再那么敏感，极大地降低了模型的方差。
>
> 最终的 $$J_{\text{reg}}(W)$$ 追求的是 **“训练误差”** 和 **“模型复杂度”** 之间的最佳平衡点，这正是我们希望在**测试集上得到最小总误差**所需的特性。

# 3.L1 正则化（Lasso）和 L2 正则化（Ridge）

L2 正则项是权重W的平方和，乘以一个超参数 $$\lambda$$。损失函数变为 $$J_{\text{L2}}(W) = J(W) + \lambda \sum W_i^2$$。

L1 正则项是权重W**绝对值**的和，乘以一个超参数 $$\lambda$$。损失函数变为 $$J_{\text{L1}}(W) = J(W) + \lambda \sum |W_i|$$。

L2 惩罚项对**大的权重**施加的惩罚更大（二次方增长）。在梯度下降时，权重 $$W$$ 的每一次更新都会被拉向零点，但拉力与 $$W$$ 的值成正比（梯度为 $$2W$$）。

L1 惩罚项在 $$W$$ 不为零时，施加的是一个**恒定的惩罚**（梯度为 $$\pm 1$$）。

![img](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/1*nrWncnoJ4V_BkzEf1pd4MA.png)

L2约束区域是一个**圆形**。相切点往往落在非坐标轴上，这意味着所有权重都会被缩小，但很少会精确地变为零。

L1约束区域是一个**菱形**（或正方形）。由于菱形的尖角位于坐标轴上，损失函数的等值线更容易与这些尖角相切，使得相切点的某些维度 $$W_i$$ 恰好为零。所以L1 正则化的独特之处在于它能够产生**稀疏解**，从而实现**特征选择**。L1 倾向于将那些对模型贡献不大的特征所对应的权重**直接压缩为零**。当数据集中包含大量冗余或不重要的特征时，L1 可以帮助我们自动识别并保留最重要的特征。

- **如果目标是特征选择或降维**，L1 正则化通常是更好的选择。
- **如果目标是确保所有特征都保留下来，只是希望减小它们的权重以提高模型的鲁棒性和稳定性**，L2 正则化是更常见的选择。

在实践中，人们常结合两者，使用 **Elastic Net** 正则化来兼顾特征选择和模型平滑。

> 如果不指定正则，一般也会默认用L2正则，因为要防止参数溢出。
>
> 互联网行业，一般特征比较多（维度爆炸），一般采用L1正则，为了降维。
>
> 特征比较少的情况（比如生物），可以只采用L2正则（这也是默认的）

# 4.归一化

归一化（Normalization，或标准化）通常是指对输入特征 $$x$$ 进行缩放，使其落入特定范围（如 $$[0, 1]$$）或具有标准分布（如均值 $$0$$，方差 $$1$$）。

## 4.1归一化的重要性

模型的梯度（参数更新方向）依赖于输入特征 $$x$$。

回顾逻辑回归的梯度：https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9906/

对于整个训练集（所有 $$N$$ 个样本），总损失 $$L(\mathbf{w}) = \frac{1}{N} \sum_{i=1}^{N} L_i(\mathbf{w})$$ 对 $$w_j$$ 的梯度为：

$$\frac{\partial L}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial L_i}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}$$

显然，这个梯度依赖于x，并且梯度更新时，模型会沿着大范围特征（如 $$x_1$$）对应的维度进行大步更新，而在小范围特征（如 $$x_2$$）对应的维度进行小步更新。

这会导致**梯度下降的等高线变得非常扁平狭长**，优化过程需要很小的学习率才能避免振荡，导致收敛速度非常慢。

<img src="/Users/apple/Library/Application Support/typora-user-images/image-20251120233233215.png" alt="image-20251120233233215" style="zoom:50%;" />

归一化使得所有特征 $$x_i$$ 具有相似的尺度。

这保证了所有权重 $$W_i$$ 的梯度大致位于同一数量级，使得**等高线更接近圆形**，梯度下降可以沿着最陡峭的方向平稳快速地收敛。

-----

归一化间接帮助控制模型内部的线性得分（$$z = W^T x$$），这对于激活函数的稳定性非常重要。

Softmax 或 Sigmoid 函数的输出 $$f(x)$$ 依赖于线性得分 $$z = W^T x$$​。如果输入特征 $$x$$ 的值很大，即使 $$W$$ 很小， $$z$$ 也可能非常大或非常小。

当 $$z$$ 的绝对值过大时，Sigmoid函数会输出接近 0或 1的值(模型可能一开始就学不到新东西，梯度消失了，参数不更新，模型还没怎么训练或者训练其实不到位就认为已经收敛了)，此时函数的**导数（梯度）会非常小**（进入饱和区）。归一化确保 $$x$$ 的尺度适中，可以帮助控制 $$W^T x$$ 的范围。

配合**正则化**（它限制了 $$W$$ 的大小），可以共同确保线性得分 $$W^T x$$ 不会过度膨胀，从而**避免激活函数进入饱和区**，保证训练过程中梯度的有效性。

![image-20251120232445510](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251120232445510.png)

如何选择归一化的方法？是Min-Max 归一化还是Z-score？

【Min-Max 归一化】缺点：

- **对异常值（Outliers）非常敏感。** 如果数据中存在极大的或极小的异常值，它们会严重挤压其余数据的范围，导致大部分数据点集中在 $$[0, 1]$$ 范围的一小部分，失去区分度。

【Z-score归一化】缺点：

- 不将数据限制在特定的范围 $$[0, 1]$$ 内，缩放后的特征值可能超出此范围。

---

【Min-Max 归一化】优点：

- 将所有特征值限制在固定的 $$[0, 1]$$ 范围内，易于解释和比较。
- 在特征数量很少，且特征间差异不大的情况下表现良好。
- 不改变数据的原始分布形状。

【Z-score归一化】优点：

- 使得数据符合标准正态分布（但不会改变原始分布的形状，只是进行平移和缩放）。
- 鲁棒性好。算法使用均值和标准差进行缩放，受异常值的影响较小（尤其当数据集较大时）。
- 适用于**要求数据符合正态分布**或**不依赖于固定范围**的算法，例如梯度下降法（有助于收敛）和许多线性模型。
- 适用于不知道数据确切范围，但需要保证特征同尺度的场景。

----

**Z-Score 归一化 (Standardization)** 通常是更优的选择，因为它确保了所有特征的尺度一致，有助于梯度下降算法更快、更稳定地收敛。

**Min-Max 归一化** 也可使用，但在处理带有极端异常值的数据时，需要格外小心。