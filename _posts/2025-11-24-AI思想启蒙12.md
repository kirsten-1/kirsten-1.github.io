---
layout: post
title: "【AI思想启蒙12】深度学习第2篇-梯度下降法和矩阵求导术 "
subtitle: "Softmax偏导；交叉熵+Softmax梯度统一，即“预测概率-真实标签”误差乘输入；最后附矩阵/向量求导常用公式表。完整推导清晰，适合反向传播实现参考"
date: 2025-11-24
author: "Hilda"
header-img: "img/post-bg-2015.jpg"
tags:
- AI思想启蒙
---

<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>



# 1.Softmax函数求导

Softmax 函数将这个向量转化为概率向量 $$\mathbf{y} = [y_1, y_2, \dots, y_N]^T$$，其中每个元素 $$y_i$$ 表示输入属于第 $$i$$ 个类别的概率：

$$y_i = P(Y=i|\mathbf{d}) = \frac{e^{d_i}}{\sum_{j=1}^{N} e^{d_j}}$$

在神经网络的反向传播过程中，我们需要计算 Softmax 输出 $$y_i$$ 对其输入 $$d_k$$ 的偏导数 $$\frac{\partial y_i}{\partial d_k}$$。由于输出 $$y_i$$ 取决于所有的输入 $$d_1, d_2, \dots, d_N$$，求导需要分两种情况讨论：

符号定义：

令 $$S = \sum_{j=1}^{N} e^{d_j}$$，则 Softmax 函数可以简写为 $$y_i = \frac{e^{d_i}}{S}$$。

情况一：$$i=k$$（对角线元素）

我们需要计算 $$\frac{\partial y_i}{\partial d_i}$$。使用**除法法则** $$\left(\frac{u}{v}\right)' = \frac{u'v - uv'}{v^2}$$，其中 $$u = e^{d_i}$$， $$v = S = \sum_{j=1}^{N} e^{d_j}$$。

1. 计算 $$u$$ 对 $$d_i$$ 的导数：

   $$\frac{\partial u}{\partial d_i} = \frac{\partial (e^{d_i})}{\partial d_i} = e^{d_i}$$

2. 计算 $$v$$ 对 $$d_i$$ 的导数：

   $$ \frac{\partial v}{\partial d_i} = \frac{\partial (\sum_{j=1}^{N} e^{d_j})}{\partial d_i} = e^{d_i} \quad (\text{因为求和项中只有 } e^{d_i} \text{ 对 } d_i \text{求导不为零})$$

3. 应用除法法则：

   $$\frac{\partial y_i}{\partial d_i} = \frac{\frac{\partial u}{\partial d_i} \cdot v - u \cdot \frac{\partial v}{\partial d_i}}{v^2} = \frac{e^{d_i} \cdot S - e^{d_i} \cdot e^{d_i}}{S^2}$$

4. 化简：

   $$\frac{\partial y_i}{\partial d_i} = \frac{e^{d_i}}{S} \cdot \frac{S - e^{d_i}}{S} = \frac{e^{d_i}}{S} \cdot \left(1 - \frac{e^{d_i}}{S}\right) = y_i (1 - y_i)$$

情况二：$$i \ne k$$（非对角线元素）

我们需要计算 $$\frac{\partial y_i}{\partial d_k}$$。其中 $$u = e^{d_i}$$， $$v = S = \sum_{j=1}^{N} e^{d_j}$$。

1. 计算 $$u$$ 对 $$d_k$$ 的导数：

   $$\frac{\partial u}{\partial d_k} = \frac{\partial (e^{d_i})}{\partial d_k} = 0 \quad (\text{因为 } e^{d_i} \text{ 不包含 } d_k)$$

2. 计算 $$v$$ 对 $$d_k$$ 的导数：

   $$\frac{\partial v}{\partial d_k} = \frac{\partial (\sum_{j=1}^{N} e^{d_j})}{\partial d_k} = e^{d_k} \quad (\text{因为求和项中只有 } e^{d_k} \text{ 对 } d_k \text{求导不为零})$$

3. 应用除法法则：

   $$\frac{\partial y_i}{\partial d_k} = \frac{\frac{\partial u}{\partial d_k} \cdot v - u \cdot \frac{\partial v}{\partial d_k}}{v^2} = \frac{0 \cdot S - e^{d_i} \cdot e^{d_k}}{S^2}$$

4. 化简：

   $$\frac{\partial y_i}{\partial d_k} = -\frac{e^{d_i} e^{d_k}}{S^2} = -\frac{e^{d_i}}{S} \cdot \frac{e^{d_k}}{S} = -y_i y_k$$

总结softmax 雅可比矩阵

Softmax 函数的求导结果是紧凑且优美的，可以直接用于反向传播：

$$\frac{\partial y_i}{\partial d_k} = \begin{cases} y_i (1 - y_i) & \text{if } i = k \\ -y_i y_k & \text{if } i \ne k \end{cases}$$



# 2.交叉熵损失-梯度下降

现在来推导 Softmax 回归损失函数（多分类交叉熵）相对于模型参数 $$\theta_j$$ 的梯度。这个梯度是训练模型时使用梯度下降法的核心。

为了简洁，我们只推导**单个样本**的损失函数 $$J^{(i)}(\theta)$$ 相对于**某个特定类别 $$j$$ 的参数向量 $$\theta_j$$** 的梯度。

单个样本 $$(x, y)$$ 的损失函数（负对数似然）为：

$$J(\theta) = - \log\left( \phi_{y} \right)$$

其中，$$\phi_{y}$$ 是模型对真实类别 $$y$$ 预测的概率。

$$\phi_j$$ 是 Softmax 函数：

$$\phi_j = \frac{e^{\eta_j}}{\sum_{l=1}^{k} e^{\eta_l}}$$

并且 $$\eta_j = \theta_j^T x$$。

我们的目标是计算损失函数 $$J(\theta)$$ 对第 $$j$$ 个类别的参数向量 $$\theta_j$$ 的偏导数：

$$\nabla_{\theta_j} J(\theta) = \frac{\partial J(\theta)}{\partial \theta_j}$$

-----

$$J(\theta)$$ 是关于 $$\phi_y$$ 的函数，$$\phi_y$$ 是关于 $$\eta_l$$ 的函数，而 $$\eta_l$$ 是关于 $$\theta_j$$ 的函数，我们需要使用链式法则：

$$\frac{\partial J(\theta)}{\partial \theta_j} = \sum_{l=1}^{k} \left( \frac{\partial J(\theta)}{\partial \phi_l} \cdot \frac{\partial \phi_l}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j} \right)$$

$$\frac{\partial J(\theta)}{\partial \phi_l}$$ 只有在 $$l=y$$ 时非零，所以简化为：

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial J(\theta)}{\partial \phi_y} \cdot \frac{\partial \phi_y}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j}$$

然后计算各部分偏导数：

A. $$\frac{\partial J(\theta)}{\partial \phi_y}$$ (损失函数对概率的导数)

$$J(\theta) = - \log(\phi_y)$$

$$\frac{\partial J(\theta)}{\partial \phi_y} = \frac{\partial (-\log(\phi_y))}{\partial \phi_y} = - \frac{1}{\phi_y}$$

B. $$\frac{\partial \eta_j}{\partial \theta_j}$$ (自然参数对参数的导数)

$$\eta_j = \theta_j^T x$$

$$\frac{\partial \eta_j}{\partial \theta_j} = x$$

(注意 $$\frac{\partial}{\partial \theta_j}$$ 意味着对向量 $$\theta_j$$ 求梯度，结果是向量 $$x$$。)

C. $$\frac{\partial \phi_y}{\partial \eta_j}$$ (Softmax 对原始分数的导数)

这是最复杂的一步，需要根据 $$j$$ 是否等于真实类别 $$y$$ 进行分情况讨论：

**Case 1: $$j = y$$ (计算 $$\phi_y$$ 对 $$\eta_y$$ 的导数)**

$$\phi_y = \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}}$$

使用商法则 $$\left(\frac{u}{v}\right)' = \frac{u'v - uv'}{v^2}$$，其中 $$u = e^{\eta_y}$$，$$v = \sum_{l=1}^{k} e^{\eta_l}$$。

$$\frac{\partial \phi_y}{\partial \eta_y} = \frac{(e^{\eta_y}) \sum_{l=1}^{k} e^{\eta_l} - e^{\eta_y} (e^{\eta_y})}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2}$$

$$= \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}} - \frac{e^{\eta_y} e^{\eta_y}}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2} = \phi_y - \phi_y \cdot \phi_y$$

$$\frac{\partial \phi_y}{\partial \eta_y} = \phi_y (1 - \phi_y)$$

**Case 2: $$j \ne y$$ (计算 $$\phi_y$$ 对 $$\eta_j$$ 的导数)**

$$\frac{\partial \phi_y}{\partial \eta_j} = \frac{(0) \sum_{l=1}^{k} e^{\eta_l} - e^{\eta_y} (e^{\eta_j})}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2}$$

$$= - \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}} \cdot \frac{e^{\eta_j}}{\sum_{l=1}^{k} e^{\eta_l}} = - \phi_y \cdot \phi_j$$

$$\frac{\partial \phi_y}{\partial \eta_j} = - \phi_y \phi_j$$

-----

现在，将 A, B, C 代回链式法则，同样分 $$j=y$$ 和 $$j \ne y$$ 两种情况。

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial J(\theta)}{\partial \phi_y} \cdot \frac{\partial \phi_y}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j}$$

**Case 1: $$j = y$$ (真实类别的参数梯度)**

$$\nabla_{\theta_y} J(\theta) = \underbrace{\left(- \frac{1}{\phi_y}\right)}_{\text{A}} \cdot \underbrace{\left(\phi_y (1 - \phi_y)\right)}_{\text{C1}} \cdot \underbrace{(x)}_{\text{B}}$$

$$= - (1 - \phi_y) x = (\phi_y - 1) x$$

**Case 2: $$j \ne y$$ (非真实类别的参数梯度)**

$$\nabla_{\theta_j} J(\theta) = \underbrace{\left(- \frac{1}{\phi_y}\right)}_{\text{A}} \cdot \underbrace{\left(- \phi_y \phi_j\right)}_{\text{C2}} \cdot \underbrace{(x)}_{\text{B}}$$

$$= \phi_j x$$

其中$$\phi_j$$ 是 Softmax 函数：

$$\phi_j = \frac{e^{\eta_j}}{\sum_{l=1}^{k} e^{\eta_l}}$$

并且 $$\eta_j = \theta_j^T x$$。

> 可以用一个统一的公式来表达梯度 $$\nabla_{\theta_j} J(\theta)$$：
>
> $$\nabla_{\theta_j} J(\theta) = (\phi_j - \mathbb{I}\{y=j\}) x$$
>
> 其中，$$\mathbb{I}\{y=j\}$$ 是指示函数：如果 $$j$$ 是真实类别 $$y$$，则为1；否则为0。
>
> 【注意】括号内的项 $$(\phi_j - \mathbb{I}\{y=j\})$$ 是**预测概率**和**真实标签**之间的**误差**。
>
> **如果 $$j$$ 是真实类别 $$y$$：** 误差是 $$(\phi_y - 1)$$。由于 $$\phi_y < 1$$，误差是负的。梯度下降会向**增加** $$\phi_y$$ 的方向调整 $$\theta_y$$。
>
> **如果 $$j$$ 不是真实类别 $$y$$：** 误差是 $$(\phi_j - 0) = \phi_j$$。梯度是正的。梯度下降会向**减少** $$\phi_j$$ 的方向调整 $$\theta_j$$。

模型的参数 $$\theta_j$$ 的更新量正比于：

$$\text{误差} \times \text{输入特征}$$

**这个公式与线性回归和逻辑回归的梯度公式形式惊人地相似，展现了 GLM 框架的统一性。**

有了这个梯度，我们就可以使用梯度下降（或其变体）来迭代更新所有 $$\theta_j$$ 向量，直到损失函数收敛到最小值。



# 2.矩阵求导术

![image-20251122224506773](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122224506773.png)

## 2.1向量求导

<img src="/Users/apple/Library/Application Support/typora-user-images/image-20251122224527702.png" alt="image-20251122224527702" style="zoom:50%;" />

## 2.2矩阵求导

![image-20251122232509775](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122232509775.png)

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122224551401.png" alt="image-20251122224551401" style="zoom:50%;" />

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122224620015.png" alt="image-20251122224620015" style="zoom: 67%;" />



## 2.3矩阵求导链式法则

![image-20251122231850423](https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122231850423.png)



