---
layout: post
title: "2-5 梯度下降  Gradient Descent"
subtitle: "梯度下降的数学原理"
date: 2025-03-02
author: "Hilda"
header-img: "img/post-bg-2015.jpg"
tags:
- 机器学习-吴恩达
---


<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


吴恩达课程系列

[1-1 欢迎参加《机器学习》课程【机器学习简介/例子】](https://kirsten-1.github.io/2025/02/28/%E5%90%B4%E6%81%A9%E8%BE%BEML1-1/)

[1-2 什么是机器学习【机器学习的2个定义和分类】](https://kirsten-1.github.io/2025/03/01/%E5%90%B4%E6%81%A9%E8%BE%BEML1-2/)

[1-3监督学习【监督学习的定义，监督学习的分类（回归与分类）】](https://kirsten-1.github.io/2025/03/01/%E5%90%B4%E6%81%A9%E8%BE%BE1-3/)

[1-4无监督学习【无监督学习的定义，无监督学习问题的分类（聚类/信号分离/降维）】](https://kirsten-1.github.io/2025/03/02/%E5%90%B4%E6%81%A9%E8%BE%BEML1-4%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/)

[2-1模型描述【如何描述一个模型（用一些符号），单变量线性回归是什么？】](https://kirsten-1.github.io/2025/03/02/%E5%90%B4%E6%81%A9%E8%BE%BEML2-1/)

[2-2~2-4代价函数【代价函数的数学定义、代价函数的直观理解】](https://kirsten-1.github.io/2025/03/02/%E5%90%B4%E6%81%A9%E8%BE%BEML2-2to2-4/)

# 2-5 用梯度下降(Gradient Descent)算法最小化任意代价函数J

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250302141137310.png" alt="image-20250302141137310" style="zoom:50%;" />



梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$$J(\theta_0, \theta_1)$$的最小值。

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250302141432078.png" alt="image-20250302141432078" style="zoom:50%;" />

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250302141453264.png" alt="image-20250302141453264" style="zoom:50%;" />

不同的起点得到了不同的局部最小解（或者说“局部最优解”）。-----这也是梯度下降法的一个特点。

## 梯度下降算法的数学原理

具体是如何更新参数 $$\theta_0, \theta_1$$呢？

<img src="https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20250302153654327.png" alt="image-20250302153654327" style="zoom:50%;" />

$$\alpha$$是学习率，控制每次更新的步长(可以理解为当想要下山快一点，迈的步子大一点的时候，就可以让$$\alpha$$大一点)

注意： $$\theta_0, \theta_1$$是同步更新的，上图右下侧的方法不是真正意义上的梯度下降（虽然也说不准会有局部最优解，但不是介绍的真正的梯度下降）

梯度下降的步骤不断重复，直到损失函数收敛，找到最优的参数。

> **损失函数（Loss Function）收敛**指的是随着迭代次数的增加，损失函数的值逐渐减少，并趋近于一个稳定的最小值，不再有明显的下降。这意味着梯度下降算法已经找到（或接近）了一个最优解，进一步的更新不会显著改变参数或提高模型的性能。
>
> **损失函数收敛**意味着在多次迭代后，损失函数的变化越来越小，最终趋于稳定（就是那个差的平方求和几乎不变了，或者说求偏导的结果几乎等于0）





























