<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hilda</title>
    <description>这里是 Hilda 的个人博客，与你一起发现更大的世界 | 要做一个有 swag 的程序员</description>
    <link>https://kirsten-1.github.io/</link>
    <atom:link href="https://kirsten-1.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 25 Nov 2025 05:13:52 +0000</pubDate>
    <lastBuildDate>Tue, 25 Nov 2025 05:13:52 +0000</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>【AI思想启蒙14】深度学习第4篇-一文吃透神经网络训练的 8 大核心难题 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;1梯度下降基础问题&quot;&gt;1.梯度下降基础问题&lt;/h1&gt;

&lt;p&gt;回顾梯度下降：在训练神经网络时，我们的目标是最小化整个训练集上的&lt;strong&gt;总损失函数&lt;/strong&gt; \(\mathcal{L}(W)\)：&lt;/p&gt;

\[\mathcal{L}(W) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{(i)}(W)\]

&lt;p&gt;其中\(W\) 是所有模型的权重，\(N\) 是训练集中的总样本数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;目标是计算总梯度的平均值：&lt;/strong&gt; \(\frac{\partial \mathcal{L}}{\partial W} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial \mathcal{L}_{(i)}}{\partial W}\)。&lt;/p&gt;

&lt;p&gt;由于N往往非常大（百万甚至上亿），每次迭代都计算全部N个样本的梯度是&lt;strong&gt;不可行&lt;/strong&gt;且&lt;strong&gt;耗时&lt;/strong&gt;的。因此，我们使用 SGD 的小批量变体。&lt;/p&gt;

&lt;h2 id=&quot;11小批量随机梯度下降-mini-batch-sgd-的梯度&quot;&gt;1.1小批量随机梯度下降 (Mini-Batch SGD) 的梯度&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;小批量 SGD 的梯度&lt;/strong&gt;&lt;/p&gt;

\[\frac{\partial \text{Loss}_{SGD}}{\partial W} = \frac{1}{\text{batch\_size}} \sum_{i \in \text{batch}} \frac{\partial \text{Loss}_{(i)}}{\partial W}\]

&lt;blockquote&gt;
  &lt;p&gt;为了最大程度的应用GPU等硬件资源，batch size通常为32，64，128，256等等数值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在每次迭代中，我们从训练集中随机抽取一个大小为 \(B = \text{batch\_size}\) 的子集（一个批次）。**我们用这个小批次样本的平均梯度，来近似整个训练集的总平均梯度。另外，为了充分利用 GPU 等硬件的并行计算能力，通常需要增加 Batch Size。&lt;/p&gt;

&lt;p&gt;小批量 SGD 的成功依赖于&lt;strong&gt;无偏估计原理&lt;/strong&gt;：通过随机抽样，我们只需要计算一小部分样本的平均梯度，就能以极高的计算效率，沿着&lt;strong&gt;统计学上正确&lt;/strong&gt;的方向进行优化。这使得深度神经网络的大规模训练成为可能。&lt;/p&gt;

&lt;h2 id=&quot;12统计上相等的原理无偏估计&quot;&gt;1.2统计上相等的原理（无偏估计）&lt;/h2&gt;

\[E\left(\frac{1}{\text{batch\_size}} \sum_{i \in \text{batch}} \frac{\partial \text{Loss}_{(i)}}{\partial W}\right) = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial \text{Loss}_{(i)}}{\partial W}\]

&lt;p&gt;左式中\(E(\cdot)\) 代表&lt;strong&gt;期望值（Expected Value）&lt;/strong&gt;。左侧是小批量梯度的&lt;strong&gt;期望值&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;右侧是整个训练集的&lt;strong&gt;真实平均梯度&lt;/strong&gt;（即批量梯度下降 BGD 的梯度）。&lt;/p&gt;

&lt;p&gt;这个等式表明，虽然任何一个&lt;strong&gt;单独的小批量梯度&lt;/strong&gt;都可能与真实的总梯度有所偏差（这是随机性造成的），但如果我们对所有可能的小批量梯度取平均（即求期望），那么这个平均值&lt;strong&gt;精确等于&lt;/strong&gt;真实的总梯度。&lt;/p&gt;

&lt;p&gt;在统计学中，一个估计量（这里是小批量梯度）的期望值如果等于它所估计的真实参数（这里是真实总梯度），那么这个估计量就是&lt;strong&gt;无偏的（Unbiased）&lt;/strong&gt;。每次迭代我们都在向&lt;strong&gt;正确的方向&lt;/strong&gt;前进，即使每一步都有噪声。由于期望值是正确的，只要我们进行足够多的迭代，这些噪声就会相互抵消，最终会收敛到真实的总梯度方向附近。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;批量梯度下降 (BGD):&lt;strong&gt;梯度最精确&lt;/strong&gt;，但计算成本极高，无法用于大模型。适用于小规模/理论分析&lt;/li&gt;
  &lt;li&gt;纯随机梯度下降 (SGD):只选取一个进行梯度更新。&lt;strong&gt;梯度噪声最大&lt;/strong&gt;，但计算成本最低，易于跳出局部极小值。&lt;/li&gt;
  &lt;li&gt;小批量 SGD (Mini-Batch):&lt;strong&gt;平衡&lt;/strong&gt;了计算效率和梯度精度，是工业界和研究中最常用的方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124182229547.png&quot; alt=&quot;image-20251124182229547&quot; /&gt;&lt;/p&gt;

&lt;p&gt;batch size越大，梯度的方差越小，因为随机性越弱。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2batch-size-的作用影响以及常见的训练问题&quot;&gt;2.&lt;strong&gt;Batch Size&lt;/strong&gt; 的作用、影响以及常见的训练问题&lt;/h1&gt;

&lt;p&gt;当总样本数 N=10000 个时，如果使用批量大小 \(B=\text{batch\_size}\) 进行训练，那么遍历完所有样本所需执行的梯度更新次数（即一次 Epoch 完成的更新次数）是：&lt;/p&gt;

\[\text{更新次数} = \frac{N}{B} = \frac{10000}{\text{batch\_size}}\]

&lt;p&gt;batchsize越小，更新越频繁,网络调整方向的频率越高,这种频繁更新使得模型在单位时间内看到更多不同方向的梯度信息，理论上可以更快地响应损失函数的变化。&lt;/p&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Batch Size 过小（梯度过于随机），每次更新的梯度只根据极少数样本计算得出。根据我们之前讨论的&lt;strong&gt;无偏估计&lt;/strong&gt;原理，虽然它的&lt;strong&gt;期望值&lt;/strong&gt;是正确的，但&lt;strong&gt;方差（噪声）&lt;/strong&gt;极大。梯度方向具有&lt;strong&gt;高度随机性&lt;/strong&gt;，与真实的整体梯度方向差异过大。这使得优化路径非常曲折和不稳定，就像在一个摇摇晃晃的船上行走，难以稳定收敛。&lt;/li&gt;
  &lt;li&gt;Batch Size 过大，虽然梯度方差小，更新方向更精确，但更新频率低。此外，大 Batch Size 可能会导致模型更容易收敛到&lt;strong&gt;尖锐的局部极小值&lt;/strong&gt;（泛化能力差），并且需要更多的内存资源。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;在机器学习训练中，如果 Loss 曲线出现剧烈震荡而不是平滑下降，通常是以下原因造成的：&lt;/p&gt;

&lt;p&gt;1.如果 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batchsize=1&lt;/code&gt;（或非常小），每次更新的梯度噪声太大。每次参数更新都是对&lt;strong&gt;单个样本（或极少数样本）的妥协&lt;/strong&gt;，而不是对全局损失的优化。这导致参数在不同训练样本的需求之间来回摇摆不定，Loss 曲线表现为剧烈的&lt;strong&gt;锯齿状震荡&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;2.训练样本没有 Shuffle（打乱），例如，前 5000 个样本都是猫的图片，后 5000 个样本都是狗的图片），而没有进行打乱（Shuffle）在训练的前半段 Epoch 中，Batch 中可能全都是猫。模型会将所有参数朝向识别“猫”的方向更新。在训练的后半段 Epoch 中，Batch 中可能全都是狗。模型会突然发现之前的参数方向是错的，于是将参数剧烈地调整朝向识别“狗”的方向。这种剧烈的、周期性的方向调整，会在 Loss 曲线中表现为明显的周期性、大幅度的震荡或周期性的剧烈上升，严重影响训练的稳定性和收敛性。&lt;/p&gt;

&lt;h1 id=&quot;3逃离尖锐极小值&quot;&gt;3.逃离尖锐极小值&lt;/h1&gt;

&lt;p&gt;深度神经网络的损失函数通常是&lt;strong&gt;非凸的&lt;/strong&gt;。这意味着损失曲面非常复杂，存在大量的局部极小值（Local Minima）和鞍点（Saddle Points），使得找到全局最优解变得极其困难。&lt;/p&gt;

&lt;p&gt;【1】尖锐最小值的危害：参数的微小变化就会导致损失急剧增加，导致模型在未见过的数据（测试集）上表现不佳，即&lt;strong&gt;损失了泛化能力&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124184218811.png&quot; alt=&quot;image-20251124184218811&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;【2】到达局部尖锐最小值的原因：大 Batch Size 减小了梯度中的噪声，这使得模型更容易收敛到&lt;strong&gt;尖锐的局部极小值&lt;/strong&gt; 。LB 方法的梯度&lt;strong&gt;噪声（方差）极小&lt;/strong&gt;，路径平滑精确。这使其在优化过程中缺乏&lt;strong&gt;探索性&lt;/strong&gt;，一旦进入尖锐最小值的“吸引盆地”（Basin of Attraction），就&lt;strong&gt;无法逃脱&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;实践发现，使用大批量（LB，large batch）方法训练出的模型，虽然在&lt;strong&gt;训练集上的损失值（Training Loss）&lt;/strong&gt;与小批量（Small-Batch, SB）方法相似，但在&lt;strong&gt;测试集上的性能（泛化能力）&lt;/strong&gt;明显更差。这就是所谓的&lt;strong&gt;泛化差距&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;LB 方法缺乏 SB 方法的&lt;strong&gt;探索性&lt;/strong&gt;，倾向于收敛到靠近&lt;strong&gt;初始点&lt;/strong&gt;的最小值。SB 和 LB 方法收敛到&lt;strong&gt;本质上不同&lt;/strong&gt;的最小值，这些最小值具有不同的泛化特性。LB 方法缺乏泛化能力的原因在于它们倾向于收敛到&lt;strong&gt;尖锐最小值（Sharp Minimizers）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;SB 方法的梯度具有&lt;strong&gt;大量噪声&lt;/strong&gt;。这种噪声就像一个持续的&lt;strong&gt;扰动&lt;/strong&gt;，能够帮助优化过程跳出尖锐、狭窄的谷底，从而探索到更宽广、更平坦的最小值区域。&lt;/p&gt;

&lt;p&gt;LB 方法的梯度&lt;strong&gt;过于精确和稳定&lt;/strong&gt;，缺乏 SB 方法中噪声所提供的&lt;strong&gt;跳出和探索&lt;/strong&gt;能力。这导致 LB 方法最终停留在一个对参数微小变化不鲁棒的解上。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;看一些实践的数据：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194657465.png&quot; alt=&quot;image-20251124194657465&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194708249.png&quot; alt=&quot;image-20251124194708249&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124194818816.png&quot; alt=&quot;image-20251124194818816&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;早期停止（Early Stopping）&lt;/strong&gt;是一种常用的正则化技术，它通过在验证集损失开始上升时停止训练，来防止模型过拟合。&lt;/p&gt;

&lt;p&gt;对于大批量方法产生的&lt;strong&gt;泛化差距（Generalization Gap）&lt;/strong&gt;，简单地使用早期停止并不能有效地弥补。这意味着问题不是出在训练时间太长，而是出在&lt;strong&gt;收敛到的那个最小值本身的性质&lt;/strong&gt;有问题。存在&lt;strong&gt;比训练时间&lt;/strong&gt;更深层次的原因（即最小值的&lt;strong&gt;尖锐度&lt;/strong&gt;）导致泛化能力差。&lt;/p&gt;

&lt;h2 id=&quot;31最小值的尖锐度测量&quot;&gt;3.1最小值的尖锐度测量&lt;/h2&gt;

&lt;p&gt;最小值的尖锐度在理论上是通过损失函数在最小值点 \(x\) 处的&lt;strong&gt;海森矩阵 (\(\nabla^2 f(x)\))&lt;/strong&gt; 的&lt;strong&gt;特征值&lt;/strong&gt;来表征的。特征值越大，曲率越大，最小值越尖锐。但是在深度学习应用中，模型的参数 \(n\) 数量巨大，计算和分解海森矩阵的成本&lt;strong&gt;高到无法承受（prohibitive cost）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;新的尖锐性度量：基于&lt;strong&gt;敏感性（Sensitivity）&lt;/strong&gt;的度量方法。这种方法的核心是：通过&lt;strong&gt;探索解 \(x\) 周围的一个小邻域&lt;/strong&gt;，并计算函数 \(f\)（即损失函数）在该邻域内能达到的&lt;strong&gt;最大值&lt;/strong&gt;。这个最大值越高，说明损失函数在该点越敏感，即越尖锐。比如计算在 \(x\) 点附近&lt;strong&gt;损失函数值的相对增幅&lt;/strong&gt;。增幅越大，说明在该点稍有偏离损失就会大幅增加，即&lt;strong&gt;越尖锐&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;当然衡量尖锐度的方法肯定有很多，可以查阅更多论文了解。例如：https://arxiv.org/pdf/1609.04836&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;适度的噪声被认为是&lt;strong&gt;有益的&lt;/strong&gt;。这种噪声帮助优化算法在损失曲面上&lt;strong&gt;跳出&lt;/strong&gt;尖锐的局部极小值和鞍点，引导模型收敛到&lt;strong&gt;平坦的最小值（Flat Minima）&lt;/strong&gt;区域，从而提高模型的&lt;strong&gt;泛化能力&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&quot;4鞍点问题-优化过程中的隐形障碍&quot;&gt;4.鞍点问题-优化过程中的隐形障碍&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124201234427.png&quot; alt=&quot;image-20251124201234427&quot; /&gt;&lt;/p&gt;

&lt;p&gt;深度神经网络的损失曲面（Loss Surface）实际上是一个&lt;strong&gt;混沌的高维地形&lt;/strong&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124203051499.png&quot; alt=&quot;image-20251124203051499&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这个高维地形中，最常见的障碍物不是&lt;strong&gt;局部极小值（Local Minima）&lt;/strong&gt;，而是&lt;strong&gt;鞍点（Saddle Points）&lt;/strong&gt;。理解鞍点是理解深度学习优化器设计原理的关键。&lt;/p&gt;

&lt;h2 id=&quot;41鞍点的数学定义&quot;&gt;4.1鞍点的数学定义&lt;/h2&gt;

&lt;p&gt;第一：在鞍点处，损失函数 \(f\) 的&lt;strong&gt;梯度&lt;/strong&gt; \(\nabla f\) 等于零 (\(\nabla f = 0\))。—-&amp;gt;这会导致优化过程停滞。&lt;/p&gt;

&lt;p&gt;第二：函数的&lt;strong&gt;曲率（Curvature）&lt;/strong&gt;是&lt;strong&gt;混合的&lt;/strong&gt;。在某些方向上，曲面是&lt;strong&gt;向上弯曲&lt;/strong&gt;的（像最小值）；在另一些方向上，曲面是&lt;strong&gt;向下弯曲&lt;/strong&gt;的（像最大值）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;示例函数 \(f(x, y) = x^2 - y^2\)：&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;梯度：&lt;/strong&gt; \(\nabla f = (2x, -2y)\)。在原点 \((0, 0)\) 处，梯度为0。&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;曲率（通过海森矩阵的特征值）：&lt;/strong&gt; 特征值为 \(\{2, -2\}\)。由于特征值&lt;strong&gt;符号混合&lt;/strong&gt;（一个正，一个负），原点 \((0, 0)\) 是一个鞍点。&lt;/li&gt;
    &lt;li&gt;在多变量微积分中，&lt;strong&gt;海森矩阵（Hessian Matrix）就是二阶导数的矩阵表示&lt;/strong&gt;。海森矩阵是多变量函数&lt;strong&gt;二阶导数的完整集合&lt;/strong&gt;，它以矩阵的形式体现了函数在各个方向上的弯曲程度。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;总结：鞍点是损失曲面上梯度为零的点，但它在某些方向上是最小值，在另一些方向上是最大值。在深度网络中，鞍点比局部极小值更普遍。&lt;/p&gt;

&lt;h2 id=&quot;42为什么鞍点是一个问题&quot;&gt;4.2为什么鞍点是一个问题&lt;/h2&gt;

&lt;p&gt;鞍点是训练深度神经网络时导致训练效率低下的主要原因。&lt;/p&gt;

&lt;p&gt;在鞍点附近，&lt;strong&gt;梯度（Gradient）几乎为零&lt;/strong&gt; (\(\nabla f \approx 0\))。结果就是&lt;strong&gt;收敛停滞（convergence stalls）&lt;/strong&gt;，模型在很长时间内都不会有显著进展。当优化器陷入鞍点时，损失值停止下降，但并没有达到一个理想的低值。从业者可能会误认为训练已经&lt;strong&gt;“完成”&lt;/strong&gt;，因为损失已经不再变化，但实际上优化器只是&lt;strong&gt;被困在一个平坦的平台（flat plateau）&lt;/strong&gt;上。&lt;/p&gt;

&lt;p&gt;在低维参数空间中，极小值（Minima）和极大值（Maxima）是主要的驻点。但在&lt;strong&gt;高维空间&lt;/strong&gt;（例如拥有数百万个参数的 DNN）中，&lt;strong&gt;鞍点的数量会远超局部极小值&lt;/strong&gt;（Dauphin et al., 2014）。这意味着在训练过程中，优化器遇到鞍点的概率远高于遇到局部极小值。&lt;/p&gt;

&lt;h2 id=&quot;43如何推断陷入鞍点&quot;&gt;4.3如何推断陷入鞍点&lt;/h2&gt;

&lt;p&gt;由于直接计算鞍点过于复杂，实践中我们通过观察训练动态来&lt;strong&gt;推断&lt;/strong&gt;模型是否被鞍点困住:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;损失值长时间不下降，但并未收敛到接近零的低值。&lt;/li&gt;
  &lt;li&gt;在多个训练步骤中，梯度的&lt;strong&gt;范数（Norm）&lt;/strong&gt;（即梯度的整体大小）接近于零。&lt;/li&gt;
  &lt;li&gt;模型参数在鞍点周围轻微“徘徊”（hover），但整体上没有朝着损失更低的方向移动。&lt;/li&gt;
  &lt;li&gt;理论上，可以通过计算&lt;strong&gt;海森矩阵（Hessian）的特征值&lt;/strong&gt;。如果特征值&lt;strong&gt;正负混合&lt;/strong&gt;，则可以确定该点是鞍点。然而，正如前面所解释的，海森矩阵对于大型网络&lt;strong&gt;计算上不可行（infeasible）&lt;/strong&gt;。深度学习框架（如 TensorFlow, PyTorch）&lt;strong&gt;不会检查海森矩阵&lt;/strong&gt;，而是依靠优化器的动态特性来自动逃离陷阱。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;44逃离鞍点的策略&quot;&gt;4.4逃离鞍点的策略&lt;/h2&gt;

&lt;p&gt;现代优化算法的核心设计目标就是提供逃离鞍点所需的“推力”或“方向感”。&lt;/p&gt;

&lt;p&gt;(1)小批量 SGD 带来的随机梯度噪声（方差）是一种天然的&lt;strong&gt;扰动&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;(2) &lt;strong&gt;现代优化器（如 Adam, RMSprop）&lt;/strong&gt;通过引入动量（Momentum）和自适应学习率机制，能够更有力地推开优化过程，有效&lt;strong&gt;逃离鞍点&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;动量为参数更新添加了&lt;strong&gt;速度（velocity）&lt;/strong&gt;，这是基于历史梯度的积累。即使在鞍点梯度消失时，动量也能凭借惯性&lt;strong&gt;带着参数向前冲&lt;/strong&gt;，从而越过平坦区域。&lt;/p&gt;

&lt;p&gt;这类优化器（Adam、RMSProp）会为&lt;strong&gt;每个参数&lt;/strong&gt;动态地缩放（调整）学习率。在鞍点处，优化器能识别出&lt;strong&gt;平坦方向&lt;/strong&gt;（梯度变化慢），并给予这些方向&lt;strong&gt;相对较大的步长&lt;/strong&gt;，而对&lt;strong&gt;尖锐方向&lt;/strong&gt;（梯度变化快）则给予较小步长，从而&lt;strong&gt;加速逃离&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;通过精心调整和衰减学习率，可以防止优化器一开始就以极快的速度冲过低损失区域，避免陷入鞍点后因学习率过低而无法逃脱。&lt;/p&gt;

&lt;p&gt;(3)正则化有助于减少网络的冗余和退化，从而&lt;strong&gt;重塑（reshapes）&lt;/strong&gt;损失曲面。这可以减少鞍点的数量或使鞍点更容易被逃离。&lt;/p&gt;

&lt;h2 id=&quot;45pytorch如何解决鞍点问题&quot;&gt;4.5PyTorch如何解决鞍点问题&lt;/h2&gt;

&lt;p&gt;现代优化器逃离鞍点的策略，都是为了在梯度接近于零的平坦区域提供额外的&lt;strong&gt;推力&lt;/strong&gt;和&lt;strong&gt;方向感&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;（1）动量 SGD (SGD with Momentum)：本身SGD有噪声，一定程度上可以缓解鞍点的问题。但是还可以引入一个&lt;strong&gt;惯性项（Momentum Term）&lt;/strong&gt;来解决传统 SGD 在鞍点附近和狭长曲面上的震荡和停滞问题。&lt;/p&gt;

&lt;p&gt;优化器维护一个&lt;strong&gt;速度缓冲 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buf&lt;/code&gt;&lt;/strong&gt;（或称为动量项），它存储了过去梯度的指数加权平均值。当优化器遇到鞍点时 ，虽然当前的梯度很小，但由于 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buf&lt;/code&gt; 中累积了之前的梯度信息，&lt;strong&gt;动量会带着参数继续向前推进&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buf = momentum * buf + grad&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;momentum&lt;/code&gt; 是动量系数，通常接近 1 (如 0.9或 0.99)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;新速度&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buf&lt;/code&gt;) 是&lt;strong&gt;旧速度&lt;/strong&gt;按动量衰减后的值，加上&lt;strong&gt;当前梯度&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad&lt;/code&gt;)。&lt;/li&gt;
  &lt;li&gt;在鞍点处，连续的微小梯度会被累加，形成一个较大的速度向量，从而提供逃逸所需的推力。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参数更新直接使用累积的速度 (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buf&lt;/code&gt;)，而不是原始梯度。&lt;/p&gt;

&lt;p&gt;（2）Adam 优化器 (Adaptive Moment Estimation)：目前工业界最流行的优化器之一，它结合了 &lt;strong&gt;动量&lt;/strong&gt; 和 &lt;strong&gt;自适应学习率&lt;/strong&gt; 的双重优势，使其在逃离鞍点方面表现出色。&lt;/p&gt;

&lt;h1 id=&quot;5学习的震荡问题&quot;&gt;5.学习的震荡问题&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124211118448.png&quot; alt=&quot;image-20251124211118448&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;损失函数曲面是狭长的，损失函数对不同参数的敏感度差异很大，参数被迫在山谷的&lt;strong&gt;两侧来回剧烈跳跃（Oscillation）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;导致的问题：大部分的学习步长被浪费在无意义的、垂直于目标方向的来回跳动上。只有微小的进步是沿着谷底向真正的最小值方向进行的。如果我们尝试使用较大的学习率来加速沿着谷底的进展，那么在陡峭的垂直方向上，参数就会&lt;strong&gt;发散（Diverge）&lt;/strong&gt;，无法收敛。整个训练过程的学习率不得不受到最陡峭方向的限制，因此平坦方向的学习速度就会非常慢。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;解决：比如&lt;strong&gt;动量（Momentum）：&lt;/strong&gt; 通过累积历史梯度，震荡方向的梯度（它们相互抵消）被抑制，而沿着谷底方向的梯度（它们持续累加）被放大，从而加速沿着谷底的进展。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213049497.png&quot; alt=&quot;image-20251124213049497&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

\[\begin{aligned}&amp;amp;\boldsymbol{g}=\frac{\partial\mathrm{Loss}}{\partial\boldsymbol{W}}\\&amp;amp;\boldsymbol{v}_{t}=\alpha\boldsymbol{v}_{t-1}+\varepsilon\boldsymbol{g}\\&amp;amp;\boldsymbol{W}_{t}=\boldsymbol{W}_{t-1}-\boldsymbol{v}_{t}\end{aligned}\]

&lt;blockquote&gt;
  &lt;p&gt;这个方法说白了就是考虑当前梯度和上一次的梯度，做对冲&lt;/p&gt;

  &lt;p&gt;如果方向不一致，那么就会对冲，更新步幅会比较稳定&lt;/p&gt;

  &lt;p&gt;如果方向一致，那么步子会越迈越大，更加加快学习速度—-&amp;gt;巧妙！&lt;/p&gt;

  &lt;p&gt;动量法—-&amp;gt;既解决学习震荡的问题，又解决学习慢的问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先SGD 的第一步，计算当前小批量样本（Mini-Batch）在当前权重 \(\boldsymbol{W}_{t-1}\) 处的平均梯度 \(\boldsymbol{g}\)。&lt;/p&gt;

&lt;p&gt;新速度 \(\boldsymbol{v}_t\) 是由两部分组成的:一般推荐\(\alpha=0.9\)，\(\varepsilon=0.1\)&lt;/p&gt;

&lt;p&gt;1.&lt;strong&gt;历史速度的延续 (\(\alpha\boldsymbol{v}_{t-1}\)):&lt;/strong&gt; 参数带着上一步的惯性继续前进。\(\alpha\) 越大，惯性越强。&lt;/p&gt;

&lt;p&gt;2.&lt;strong&gt;当前梯度的推动 (\(\varepsilon\boldsymbol{g}\)):&lt;/strong&gt; 当前的梯度信息指导参数朝新的方向移动， \(\varepsilon\) 控制这个推动力的大小。&lt;/p&gt;

&lt;p&gt;最后，更新权重，权重 \(\boldsymbol{W}\) 是沿着新的&lt;strong&gt;速度向量 \(\boldsymbol{v}_t\)&lt;/strong&gt; 的反方向进行更新。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213112882.png&quot; alt=&quot;image-20251124213112882&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124213143374.png&quot; alt=&quot;image-20251124213143374&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sebastian Ruder 在他的论文中简明扼要地描述了动量的影响：”动量项在梯度指向相同的维度上会增加，而在梯度改变方向的维度上会减少更新。因此，我们可以加快收敛速度，减少振荡”&lt;/p&gt;

&lt;h1 id=&quot;6adagrad学习因子设定问题学习率&quot;&gt;6.AdaGrad学习因子设定问题（学习率）&lt;/h1&gt;

&lt;p&gt;AdaGrad（Adaptive Gradient Algorithm，自适应学习算法）是一种革命性的优化器，它首次实现了对&lt;strong&gt;每个参数&lt;/strong&gt;的&lt;strong&gt;独立学习率调整&lt;/strong&gt;。它的核心动机是解决不同参数对梯度的敏感度差异问题。&lt;/p&gt;

&lt;p&gt;在训练过程中，权重向量的不同分量（即不同的参数）可能会有极端的梯度差异。某些参数（例如对应频繁出现特征的权重）的梯度值相对稳定且适中。这是因为它们在每次更新时都使用了大量的样本信息。另一些参数（例如对应不常出现特征的权重，即稀疏数据）的梯度值可能极其巨大且不稳定。这是因为这些参数只在少数几次迭代中被更新，导致偶尔出现的信号被过分放大，造成梯度爆炸。&lt;/p&gt;

&lt;p&gt;另外，传统的 SGD 或带动量的 SGD 对所有参数使用单一的全局学习率 \(\alpha\)。如果 \(\alpha\) 设置得太小，那些梯度小的稀疏参数就更新得太慢；如果 \(\alpha\) 设置得太大，那些梯度大的参数就会剧烈震荡甚至发散。&lt;/p&gt;

&lt;p&gt;AdaGrad 通过为权重向量的&lt;strong&gt;每个组件（每个参数）独立地调整学习率&lt;/strong&gt;来解决上述问题。&lt;/p&gt;

&lt;p&gt;如果某个参数的历史梯度&lt;strong&gt;积累值很大&lt;/strong&gt; $\rightarrow$ 它的学习率应该&lt;strong&gt;变小&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果某个参数的历史梯度&lt;strong&gt;积累值很小&lt;/strong&gt; $\rightarrow$ 它的学习率应该&lt;strong&gt;变大&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;这样，那些梯度大的稀疏参数的学习率会被迅速抑制，稳定了训练；而那些梯度小的参数能获得更大的推动力，加速了学习，从而有助于解决&lt;strong&gt;梯度消失和梯度爆炸&lt;/strong&gt;问题。&lt;/p&gt;

&lt;p&gt;AdaGrad 通过累积&lt;strong&gt;过去所有迭代中梯度的平方&lt;/strong&gt;来实现自适应调整。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;累积量 \(v_t\)： 在每一步 \(t\)，AdaGrad 会计算一个累积变量 \(v_t\)（在公式中通常记为 \(G_t\) 或 \(V_t\)），它是从训练开始到当前步为止，所有历史梯度平方的元素级求和。&lt;/p&gt;

\[\boldsymbol{v}_t = \boldsymbol{v}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t\]

    &lt;p&gt;（其中 \(\odot\) 表示元素级乘法，\(\boldsymbol{g}_t\) 是当前梯度）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;更新方式： AdaGrad 将原始的学习率 \(\alpha\) 划分为 \(\sqrt{v_t}\)，作为每个参数的新的、自适应的学习率&lt;/p&gt;

\[\boldsymbol{W}_{t} = \boldsymbol{W}_{t-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_t }+ \epsilon} \odot \boldsymbol{g}_t\]

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;分母 \(\sqrt{\boldsymbol{v}_t}\)：&lt;/strong&gt; 作为一个&lt;strong&gt;动态的正则项&lt;/strong&gt;，其值越大，该参数的有效学习率就越小。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;\(\epsilon\)：&lt;/strong&gt; 一个很小的正数，防止分母为零。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最大的优势在于&lt;strong&gt;无需手动调参&lt;/strong&gt;（尤其是学习率 \(\alpha\)），因为算法在训练过程中会根据历史数据自动进行调整。非常适合处理&lt;strong&gt;稀疏数据&lt;/strong&gt;（如自然语言处理中的词嵌入），能让罕见特征获得更大的更新步长。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;或者说，学习的越多，W更新的越多，学习因子就大。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;最大的问题是 &lt;strong&gt;\(v_t\) 是一个持续累积正数&lt;/strong&gt;的变量，它只会单调递增。这意味着分母 \(\sqrt{\boldsymbol{v}_t}\) 会越来越大，导致&lt;strong&gt;有效学习率会随着训练的进行而不断、单调地衰减&lt;/strong&gt;。在训练的后期，有效学习率变得&lt;strong&gt;极低&lt;/strong&gt;，使得模型难以进行有效的参数更新，导致算法&lt;strong&gt;收敛速度非常慢&lt;/strong&gt;，甚至提前停止学习。这个局限性直接促成了后续改进算法如 &lt;strong&gt;RMSprop&lt;/strong&gt; 和 &lt;strong&gt;Adam&lt;/strong&gt; 的诞生。&lt;/p&gt;

&lt;h1 id=&quot;7rmsprop和adam&quot;&gt;7.RMSprop和Adam&lt;/h1&gt;

&lt;h2 id=&quot;71rmsprop优化器&quot;&gt;7.1RMSprop优化器&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;RMSProp (Root Mean Square Propagation)&lt;/strong&gt; 优化器，它作为对 AdaGrad 的改进，克服了 AdaGrad 的主要缺陷，由 Geoffrey Hinton 在其 Coursera 课程中提出的，它的主要目标是解决 AdaGrad 学习率单调递减导致后期收敛缓慢的问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124222159540.png&quot; alt=&quot;image-20251124222159540&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在AdaGrad中，二阶矩（即\(v_t\)）是简单的累积相乘，\(v_t\) 单调递增，学习率无限衰减。学到最后，就学的越来越少了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124222520938.png&quot; alt=&quot;image-20251124222520938&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而RMS prop中，二阶矩（即\(v_t\)）是&lt;strong&gt;指数移动平均 (EMA)&lt;/strong&gt;：\(v_t = \beta \cdot v_{t-1} + (1-\beta) \cdot g_t^2\)。\(\beta\) 控制历史信息衰减，学习率可以&lt;strong&gt;保持稳定&lt;/strong&gt;（通常接近 1，如 0.9）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;公式只涉及 \(t\) 和 \(t-1\) 时刻的量，为什么说它和“久远”的历史信息有关？指数移动平均（EMA）的“记忆”机制&lt;/p&gt;

  &lt;p&gt;因为\(\boldsymbol{v}_{t-1}\) 本身就包含了久远的历史信息。&lt;/p&gt;

\[\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha \boldsymbol{v}_{t-1}\]

  &lt;p&gt;而\(\boldsymbol{v}_{t-1} = (1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha \boldsymbol{v}_{t-2}\)&lt;/p&gt;

  &lt;p&gt;代入得到：\(\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha \left[ (1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha \boldsymbol{v}_{t-2} \right]\)&lt;/p&gt;

\[\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha(1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha^2 \boldsymbol{v}_{t-2}\]

  &lt;p&gt;如果将这个过程一直追溯到训练开始（\(t=0\)），可以看到 \(\boldsymbol{v}_t\) 实际上是&lt;strong&gt;所有历史梯度平方 \(\boldsymbol{g}_i^2\) 的加权和&lt;/strong&gt;：&lt;/p&gt;

\[\boldsymbol{v}_{t} = (1-\alpha)\boldsymbol{g}_{t}^2 + \alpha(1-\alpha)\boldsymbol{g}_{t-1}^2 + \alpha^2(1-\alpha)\boldsymbol{g}_{t-2}^2 + \alpha^3(1-\alpha)\boldsymbol{g}_{t-3}^2 + \dots\]

  &lt;p&gt;因此，说 \(\boldsymbol{v}_t\) 具有&lt;strong&gt;“记忆”&lt;/strong&gt;功能，并由 \(\alpha\) 控制这个记忆的&lt;strong&gt;“遗忘速度”&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;AdaGrad 与 RMSProp 维持历史信息的方式对比：&lt;/p&gt;

&lt;p&gt;AdaGrad是一个&lt;strong&gt;无限期、不衰减的累加过程&lt;/strong&gt;。每一个历史梯度平方 \(\boldsymbol{g}_i^2\) 对当前的 \(v_t\) 都拥有&lt;strong&gt;相同的、永久的权重&lt;/strong&gt;。\(v_t\) 的值是一个&lt;strong&gt;非递减序列&lt;/strong&gt;，它永远不会忘记过去的梯度。随着训练的不断进行，分母 \(\sqrt{\boldsymbol{v}_t}\) 会&lt;strong&gt;无限增大&lt;/strong&gt;，导致有效学习率 \(\frac{\alpha}{\sqrt{\boldsymbol{v}_t}}\) 不可避免地、&lt;strong&gt;无限地趋向于零&lt;/strong&gt;。在训练后期，即使遇到一个非常重要的、需要大步长更新的梯度，模型也因为学习率太小而无法做出有效响应。&lt;/p&gt;

&lt;p&gt;RMSProp是一个&lt;strong&gt;有选择性、指数衰减的平均过程&lt;/strong&gt;。通过 \(\beta\) 系数，&lt;strong&gt;久远的梯度信息权重会指数级地衰减&lt;/strong&gt;。模型只“记住”最近一段时间内的梯度平均大小。&lt;strong&gt;\(v_t\) 的值会收敛到近期梯度平方的平均值附近&lt;/strong&gt;，&lt;strong&gt;不会无限增大&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;72时间累积自适应梯度&quot;&gt;7.2时间累积+自适应梯度&lt;/h2&gt;

&lt;p&gt;可以把动量 SGD 和 RMSprop 结合起来，公式如下。&lt;/p&gt;

\[v_{t}=\rho_{1} v_{t-1}+(1-\rho_{1}) g\]

\[r_{t}=\rho_{2} r_{t-1}+(1-\rho_{2})\langle g, g\rangle\]

\[w_{t}=w_{t-1}-\alpha \frac{v_{t}}{\delta+\sqrt{r_{t}}}\]

&lt;p&gt;\(r_t\)是自适应的学习因子。&lt;/p&gt;

&lt;p&gt;不足：t早期冷启动的时候，v和r都会整体偏小&lt;/p&gt;

&lt;p&gt;解释：在训练开始时，优化器通常将 \(\boldsymbol{v}_0\) 和 \(\boldsymbol{r}_0\) 初始化为&lt;strong&gt;零向量&lt;/strong&gt;。由于 \(\rho_1\) 和 \(\rho_2\) 通常被设置成接近 1的值（例如 \(\rho_1=0.9\) 或 \(\rho_2=0.999\)），这意味着历史信息的权重很大，而当前信息的权重 \((1-\rho)\) 很小。所以在训练的最初几个时间步中，几乎从零开始，导致 \(\boldsymbol{v}_t\) 和 \(\boldsymbol{r}_t\) 的值在训练初期会&lt;strong&gt;系统性地偏小&lt;/strong&gt;，尤其是在 \(t\) 很小时。它们的真实期望值应该比计算出来的值要大。&lt;/p&gt;

&lt;p&gt;这种低估偏差导致模型&lt;strong&gt;初始的学习步长过小&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;解决方案：偏差修正 (Bias Correction),就是改进版的adam优化器&lt;/p&gt;

&lt;h2 id=&quot;73adam优化器&quot;&gt;7.3Adam优化器&lt;/h2&gt;

&lt;p&gt;为了解决这种冷启动偏差，Adam 引入了&lt;strong&gt;偏差修正项&lt;/strong&gt;。其思想是利用 \(\boldsymbol{v}_t\) 和 \(\boldsymbol{r}_t\) 的期望值，对它们进行放大，从而消除初始化偏差。&lt;/p&gt;

&lt;p&gt;修正后的一阶矩（动量）：&lt;/p&gt;

\[\hat{\boldsymbol{v}}_{t} = \frac{\boldsymbol{v}_{t}}{1-\rho_{1}^{t}}\]

&lt;p&gt;修正后的二阶矩（方差）：&lt;/p&gt;

\[\hat{\boldsymbol{r}}_{t} = \frac{\boldsymbol{r}_{t}}{1-\rho_{2}^{t}}\]

&lt;hr /&gt;

&lt;p&gt;当 \(t\) 较小（如 \(t=1, 2\)）时，\(1-\rho^t\) 的值接近于零。用这个接近零的值作除数，会&lt;strong&gt;极大地放大&lt;/strong&gt;原始的 \(v_t\) 和 \(r_t\)。这抵消了它们在初期被低估的偏差。&lt;/p&gt;

&lt;p&gt;随着 \(t\) 增大（例如 \(t \approx 50\)），\(\rho^t\) 会趋近于0，分母 \(1-\rho^t\) 趋近于1。此时，\(\hat{v}_t \approx v_t\)，偏差修正项&lt;strong&gt;自动消失&lt;/strong&gt;，优化器回归到标准的 EMA 更新。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;最终的 Adam 更新公式：&lt;/strong&gt;&lt;/p&gt;

\[\boldsymbol{w}_{t} = \boldsymbol{w}_{t-1}-\alpha \frac{\hat{\boldsymbol{v}}_{t}}{\delta+\sqrt{\hat{\boldsymbol{r}}_{t}}}\]

&lt;p&gt;通过引入这个偏差修正机制，Adam 确保了在训练初期（冷启动阶段）参数更新的方向和大小是准确的，极大地提高了模型的&lt;strong&gt;训练稳定性&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124232804313.png&quot; alt=&quot;image-20251124232804313&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;8归一化normalization和正则化&quot;&gt;8.归一化Normalization和正则化&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124233107638.png&quot; alt=&quot;image-20251124233107638&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;81为什么要归一化&quot;&gt;8.1为什么要归一化&lt;/h2&gt;

&lt;p&gt;特征尺度的巨大差异，比如&lt;strong&gt;年龄 (Age)：&lt;/strong&gt; 范围 \([0, 65]\)，&lt;strong&gt;薪水 (Salary)：&lt;/strong&gt; 范围 \([0, 100,000]\)。&lt;/p&gt;

&lt;p&gt;模型学习到的&lt;strong&gt;权重（Weights）&lt;/strong&gt;在更新时会受到这种尺度差异的强烈影响,显然年龄相关的权重则需要更大的调整才能跟上薪水权重的影响。&lt;/p&gt;

&lt;p&gt;另外，当输入特征尺度不一致时，会导致以下问题：&lt;/p&gt;

&lt;p&gt;（1）震荡&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124211118448.png&quot; alt=&quot;image-20251124211118448&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;（2）优化器（如梯度下降）必须沿着最陡峭的方向前进，导致它在陡峭的维度上&lt;strong&gt;剧烈震荡&lt;/strong&gt;。为了防止在陡峭方向上超调（overshoot）导致发散，我们被迫使用非常低的学习率。结果是，模型沿着平坦的方向进展缓慢，学习过程变慢，收敛效率低下。&lt;/p&gt;

&lt;h2 id=&quot;82输入归一化的方法&quot;&gt;8.2输入归一化的方法&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Z-score 标准化&lt;/strong&gt;或 &lt;strong&gt;Standard Scaling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;显然，归一化之后，损失曲面被“解扭曲”，变得更加&lt;strong&gt;圆形和对称&lt;/strong&gt;。优化器可以沿着更直接的路径向最小值前进，不再需要处理剧烈的震荡。我们可以使用&lt;strong&gt;更高的学习率&lt;/strong&gt;而不会导致发散，从而实现&lt;strong&gt;更快的收敛&lt;/strong&gt;和&lt;strong&gt;更稳定的训练&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;既然这个解决方案如此巧妙，为什么我们不将网络中每一层的激活值都进行归一化呢？&lt;/p&gt;

&lt;h2 id=&quot;83批归一化激活值归一化&quot;&gt;8.3批归一化(激活值归一化)&lt;/h2&gt;

&lt;p&gt;在&lt;strong&gt;每一层&lt;/strong&gt;的激活值（在激活函数之前或之后）都进行归一化，确保每一层的输入分布保持稳定，从而进一步加速训练和提高模型性能。&lt;/p&gt;

&lt;p&gt;批量归一化（批归一化，Batch Normalization）的提出是为了解决 &lt;strong&gt;内部协变量漂移&lt;/strong&gt; 问题。&lt;/p&gt;

&lt;p&gt;【1】内部协变量漂移 (Internal Covariate Shift, ICS)&lt;/p&gt;

&lt;p&gt;简单来说，在神经网络训练过程中，前一层参数的每一次更新，都会导致该层输出的&lt;strong&gt;激活值分布发生改变&lt;/strong&gt;。紧随其后的下一层需要&lt;strong&gt;不断适应&lt;/strong&gt;这种不断变化的输入分布，这使得模型难以收敛，导致&lt;strong&gt;收敛速度变慢&lt;/strong&gt;和&lt;strong&gt;训练不稳定&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;【2】BN：BN 的思想是像对输入层进行归一化一样，对网络&lt;strong&gt;每一层&lt;/strong&gt;的激活值进行归一化，以控制其分布。&lt;/p&gt;

&lt;p&gt;BN 在一个 &lt;strong&gt;小批量 (Mini-Batch)&lt;/strong&gt; 上计算激活值的均值和方差，对激活值进行中心化（减去 \(\mu\)）和缩放（除以 \(\sigma\)），即完成标准化。&lt;/p&gt;

&lt;p&gt;为了不让网络完全丢失归一化之前的表示能力（例如，如果 Sigmoid/Tanh 函数的输入总是被强制归一化到0附近，它们将无法利用非线性区域），BN 引入了两个&lt;strong&gt;可学习的参数&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;缩放因子 (\(\gamma\)):&lt;/strong&gt; 用于&lt;strong&gt;缩放 (Scaling)&lt;/strong&gt; 归一化后的分布。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;平移因子 (\(\beta\)):&lt;/strong&gt; 用于&lt;strong&gt;平移 (Shifting)&lt;/strong&gt; 归一化后的分布。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这两个参数会和网络权重一起，通过&lt;strong&gt;反向传播&lt;/strong&gt;进行学习和优化。它们允许网络自主决定最佳的分布，例如，如果网络需要一个非零均值或非单位方差的分布来提高性能，\(\gamma\) 和 \(\beta\) 可以通过学习恢复这种分布。&lt;/p&gt;

\[\text{BN}_{\gamma, \beta}(x_i) = \gamma \cdot \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} + \beta\]

&lt;p&gt;【3】BN 的主要优势&lt;/p&gt;

&lt;p&gt;由于网络中每一层的输入分布保持固定，后续层不再需要适应不断变化的输入，从而允许我们使用&lt;strong&gt;更高的学习率&lt;/strong&gt;，&lt;strong&gt;加快收敛速度&lt;/strong&gt;。BN解决了 ICS，使训练过程更加稳定。&lt;/p&gt;

&lt;p&gt;基于 Mini-Batch 的统计量引入了&lt;strong&gt;随机的噪声&lt;/strong&gt;，类似于 Dropout 机制，有助于防止过拟合，因此 BN 可以作为一种正则化技术。&lt;/p&gt;

&lt;p&gt;【4】 BN 的局限性&lt;/p&gt;

&lt;p&gt;（1）对批量大小 (Batch Size) 的依赖，Batch Size太大导致参数更新慢，容易陷入尖锐最小值；BN 假设当前 Batch 的 \(\mu\) 和 \(\sigma^2\) 是对整个数据集 \(\mu\) 和 \(\sigma^2\) 的良好估计。当 &lt;strong&gt;Batch Size 太小&lt;/strong&gt;时，这个估计非常不准确，引入的噪声过大，导致性能&lt;strong&gt;急剧下降&lt;/strong&gt;。如下图，batch size过小，错误率越高：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125112038931.png&quot; alt=&quot;image-20251125112038931&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;（2）测试/推理时间 (Testing Time) 的不一致性&lt;/p&gt;

&lt;p&gt;BN 的训练和测试过程需要不同的计算方式。在测试或推理时（例如自动驾驶汽车只输入&lt;strong&gt;单帧图像&lt;/strong&gt;），无法再计算有效的 Batch 统计量。网络必须使用在训练期间预先计算和存储的&lt;strong&gt;全局平均值和全局方差&lt;/strong&gt;（通过对训练过程中所有 Batch 的 \(\mu\) 和 \(\sigma^2\) 进行移动平均估计）。如果训练时的全局统计量与实际测试数据分布存在较大偏差，就会导致 &lt;strong&gt;“在训练和测试中结果不一致”&lt;/strong&gt; 的问题。&lt;/p&gt;

&lt;p&gt;这些局限性，特别是对 Batch Size 的强依赖，促使社区开发了诸如 &lt;strong&gt;层归一化 (Layer Normalization)&lt;/strong&gt;、&lt;strong&gt;实例归一化 (Instance Normalization)&lt;/strong&gt; 和 &lt;strong&gt;组归一化 (Group Normalization)&lt;/strong&gt; 等替代方法，来避免对 Batch 的依赖。&lt;/p&gt;

&lt;p&gt;推荐阅读：https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf&lt;/p&gt;

&lt;h2 id=&quot;84正则化&quot;&gt;8.4正则化&lt;/h2&gt;

\[\mathcal{L}_{\text{L2}}(\boldsymbol{W}) = \mathcal{L}(\boldsymbol{W}) + \frac{\lambda}{2} \sum_{i} W_i^2\]

\[\mathcal{L}_{\text{L1}}(\boldsymbol{W}) = \mathcal{L}(\boldsymbol{W}) + \lambda \sum_{i} |W_i|\]

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125124738477.png&quot; alt=&quot;image-20251125124738477&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;神经网络比较灵活，不同的层可以使用不同的L1或L2正则。且上方公式的\(\lambda\)不同层可以选择不同的权重。&lt;/p&gt;

&lt;p&gt;思考，对于正则化公式的\(\lambda\)，神经网络输入层和输出层哪边的\(\lambda\)更大比较好？&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;答案：对网络深层的权重施加更强的正则化。(答案不唯一，也有人认为浅层应设置更大的值)&lt;/p&gt;

  &lt;p&gt;浅层（靠近输入数据）的权重主要负责提取数据的&lt;strong&gt;低级特征&lt;/strong&gt;，例如边缘、纹理、颜色等基础模式。这些低级特征往往是&lt;strong&gt;通用且稳定的&lt;/strong&gt;，并且在数据集中是&lt;strong&gt;普遍存在&lt;/strong&gt;的。对这些基础特征提取器施加&lt;strong&gt;过强的正则化（过大的 \(\lambda\)）&lt;/strong&gt;，可能会&lt;strong&gt;过度抑制&lt;/strong&gt;网络学习这些基础模式的能力，导致模型性能下降。因此，我们倾向于使用&lt;strong&gt;较小&lt;/strong&gt;的 \(\lambda\)。&lt;/p&gt;

  &lt;p&gt;深层（靠近输出层）的权重主要负责将前一层提取的&lt;strong&gt;高级语义特征&lt;/strong&gt;（例如，物体的特定组合、复杂的概念）映射到最终的&lt;strong&gt;决策或预测&lt;/strong&gt;。这些深层权重往往更容易捕捉到训练集中的&lt;strong&gt;偶然噪声和特定模式&lt;/strong&gt;，从而导致&lt;strong&gt;过拟合&lt;/strong&gt;。它们负责网络最复杂的逻辑判断。为了避免这些高层决策逻辑&lt;strong&gt;过度依赖&lt;/strong&gt;训练集中的微小波动或噪声，我们需要对深层权重施加&lt;strong&gt;更强的约束（更大的 \(\lambda\)）&lt;/strong&gt;，鼓励它们保持较小的数值，从而提高模型的&lt;strong&gt;泛化能力&lt;/strong&gt;。&lt;/p&gt;

  &lt;p&gt;在实际应用中，最佳的 \(\lambda\) 仍然需要通过&lt;strong&gt;交叉验证（Cross-Validation）&lt;/strong&gt;在特定的数据集和模型结构上进行实验确定&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;L1正则的剪枝效果：因为L1正则化倾向于使不重要的权重&lt;strong&gt;精确地等于零&lt;/strong&gt;。可以剔除冗余特征，实现&lt;strong&gt;特征选择&lt;/strong&gt;。在实际的深度学习训练中，我们通常选择 &lt;strong&gt;L2 正则化&lt;/strong&gt;（即权重衰减），因为它能平滑地控制模型复杂度，更好地防止过拟合。只有当我们特别需要&lt;strong&gt;稀疏解&lt;/strong&gt;或进行&lt;strong&gt;特征选择&lt;/strong&gt;时，才会考虑使用 L1 正则化。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251125130337611.png&quot; alt=&quot;image-20251125130337611&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/25/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9914/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/25/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9914/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙13】深度学习第3篇-度量学习 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;度量学习&quot;&gt;度量学习&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124140910980.png&quot; alt=&quot;image-20251124140910980&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在许多人工智能应用（如人脸识别、推荐系统和搜索引擎）中，关键的挑战是测量数据点之间的相似性。因此诞生了度量学习和嵌入向量。&lt;/p&gt;

&lt;p&gt;度量学习的核心在于&lt;strong&gt;学习一种映射&lt;/strong&gt;，将原始、复杂的输入数据转换到一个新的、低维的特征空间，在这个空间中，距离可以直接反映语义上的相似性。&lt;/p&gt;

&lt;p&gt;通过神经网络或其他表示方法，将输入数据 \(x\) 映射到一个&lt;strong&gt;特征空间（Feature Space）&lt;/strong&gt;，即&lt;strong&gt;嵌入（Embedding）&lt;/strong&gt;。这个神经网络（孪生网络或三元组网络）充当了一个&lt;strong&gt;编码器&lt;/strong&gt;，将高维数据（如图像的数百万像素或文本的词汇表）压缩成一个短而稠密的向量。&lt;/p&gt;

&lt;p&gt;定义一个&lt;strong&gt;损失函数（Loss Function）&lt;/strong&gt;，鼓励&lt;strong&gt;相似的点靠得近&lt;/strong&gt;，&lt;strong&gt;不相似的点离得远&lt;/strong&gt;。这是度量学习的驱动力。与分类任务不同，损失函数不惩罚错误分类，而是惩罚&lt;strong&gt;不正确的相对距离&lt;/strong&gt;。它确保了在学习到的嵌入空间中，几何距离（如欧氏距离）与数据的语义相似度一致。&lt;/p&gt;

&lt;p&gt;传统的距离（如原始像素上的欧氏距离）对噪声敏感。度量学习则让神经网络根据任务数据，&lt;strong&gt;自主地调整&lt;/strong&gt;特征提取，使最终的嵌入空间中的距离真正符合人类的语义判断。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;strong&gt;相似/不相似&lt;/strong&gt;（即我们用于训练的标签 \(Y\)）对于模型来说是&lt;strong&gt;已知的输入&lt;/strong&gt;。&lt;/p&gt;

  &lt;p&gt;在度量学习的训练阶段，这些信息被称为&lt;strong&gt;监督信息（Supervision）&lt;/strong&gt;或&lt;strong&gt;真实标签（Ground Truth）&lt;/strong&gt;，它是用来指导模型学习的。&lt;/p&gt;

  &lt;p&gt;在训练度量学习模型时，我们需要构建特殊的训练样本，通常是：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;相似对（Positive Pair）：&lt;/strong&gt; 一对来自同一类别或具有相同语义的样本 \((X_i, X_j)\)，其标签 \(Y=1\)。&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;不相似对（Negative Pair）：&lt;/strong&gt; 一对来自不同类别或不相似语义的样本 \((X_i, X_k)\)，其标签 \(Y=0\)。&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;在训练时，模型接收的输入是：&lt;/p&gt;

\[(X_i, X_j, Y)\]
&lt;/blockquote&gt;

&lt;p&gt;度量学习核心目标是&lt;strong&gt;学习一个有效的距离函数或相似性度量&lt;/strong&gt;。这个度量函数能够使得在原始输入空间中相似的样本在新的特征空间中距离接近，而不相似的样本距离较远。度量学习不是学习分类边界，而是学习一个能够将语义相似性转化为空间距离的&lt;strong&gt;特征映射函数&lt;/strong&gt;。模型的工作就是找到一个特征映射函数，使得这种已知关系在嵌入空间中得到最好的体现。&lt;/p&gt;

&lt;p&gt;如果两个输入需要通过拥有相同权重的模型进行处理，则模型共享参数。&lt;/p&gt;

&lt;p&gt;度量学习的本质是比较&lt;strong&gt;成对的（pair）或三元的（triplet）&lt;/strong&gt;输入样本之间的关系，它不再只关注单个样本的分类结果。&lt;/p&gt;

&lt;p&gt;最终目标是学习一个&lt;strong&gt;嵌入空间（Embedding Space）&lt;/strong&gt;，在这个空间中，距离能够直接反映语义上的相似度。例如，“可爱的宝宝”和“惹人喜欢的孩子”这两个语义相似的输入，在嵌入空间中应该非常接近。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124110902616.png&quot; alt=&quot;image-20251124110902616&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;度量学习通常使用&lt;strong&gt;孪生网络（Siamese Network）&lt;/strong&gt;或类似结构来实现，主要区别在于模型是否共享权重。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;31损失函数-对比损失&quot;&gt;3.1损失函数-对比损失&lt;/h2&gt;

&lt;p&gt;对比损失(Contrastive Loss)是孪生网络中最常用的损失函数之一。对比损失要求：1) 相似的样本对之间的距离最小化；2) 不相似的样本对之间的距离必须大于一个预设的&lt;strong&gt;边际值&lt;/strong&gt;（Margin）。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果输入的两个样本相似 (\(\text{label} = 1\))，则最小化它们的嵌入距离 \(D\)。&lt;/li&gt;
  &lt;li&gt;如果输入的两个样本不相似 (\(\text{label} = 0\))，则最大化它们的嵌入距离，但只到&lt;strong&gt;边际值&lt;/strong&gt; \(m\) (margin) 为止。&lt;/li&gt;
&lt;/ul&gt;

\[\mathcal{L}(Y, D) = \frac{1}{2} Y D^2 + \frac{1}{2} (1-Y) \max(0, m - D)^2\]

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(Y\): 标签，1 表示相似，0 表示不相似。&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;\(D\): 两个嵌入向量之间的欧氏距离（Euclidean Distance） $$D =&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;\text{vector}&lt;em&gt;{(i)} - \text{vector}&lt;/em&gt;{(j)}&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;_2$$。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;\(m\): 边际值 (Margin)，一个大于 0 的常量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;或者表述成：\(\text{Loss} = \underbrace{\sum_{y_{(i,j)}=1} D_{(i,j)}}_{\text{相似对的惩罚项}} + \underbrace{\sum_{y_{(i,k)}=0} \text{hingle}(m-D_{(i,k)})}_{\text{不相似对的惩罚项}}\)&lt;/p&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(y_{(i,j)}\)：样本 \(\text{vector}_{(i)}\) 和 \(\text{vector}_{(j)}\) 之间的标签。
    &lt;ul&gt;
      &lt;li&gt;\(y=1\) 表示 \(\text{vector}_{(i)}\) 和 \(\text{vector}_{(j)}\) 属于&lt;strong&gt;相似对（Positive Pair）&lt;/strong&gt;。&lt;/li&gt;
      &lt;li&gt;\(y=0\) 表示 \(\text{vector}_{(i)}\) 和 \(\text{vector}_{(k)}\) 属于&lt;strong&gt;不相似对（Negative Pair）&lt;/strong&gt;。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;\(D_{(i,j)}\)：两个嵌入向量之间的距离。当 \(\text{vector}_{(i)}\) 和 \(\text{vector}_{(j)}\) 相似时（\(y=1\)），损失函数直接惩罚它们的距离 \(D_{(i,j)}\)。模型优化的方向是不断减小 \(D_{(i,j)}\)，将相似样本的嵌入点在空间中拉近。&lt;/li&gt;
  &lt;li&gt;\(m\)：&lt;strong&gt;边际值（Margin）&lt;/strong&gt;，一个预设的超参数，决定了不相似样本应至少保持的距离。&lt;/li&gt;
  &lt;li&gt;\(\text{hingle}(\cdot)\)：通常指 &lt;strong&gt;\(\max(0, \cdot)\)&lt;/strong&gt; 函数，类似于支持向量机（SVM）中的 Hinge Loss。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;欧式距离：\(D_{A,B} = \|\text{vector}_A - \text{vector}_B\|_2 = \sqrt{\sum_{i=1}^{k} (\text{vector}_{A,i} - \text{vector}_{B,i})^2}\)，其中 \(k\) 是嵌入向量的维度。&lt;/p&gt;

&lt;p&gt;余弦距离 (Cosine Distance):如果训练时使用的是余弦相似度相关的损失，那么我们通常会计算余弦距离。它衡量的是两个向量方向的差异，与向量的长度无关：&lt;/p&gt;

\[\text{Similarity}_{A,B} = \frac{\text{vector}_A \cdot \text{vector}_B}{\|\text{vector}_A\|_2 \|\text{vector}_B\|_2}\]

\[\text{Distance}_{A,B} = 1 - \text{Similarity}_{A,B}\]

&lt;hr /&gt;

&lt;p&gt;注意点：&lt;/p&gt;

&lt;p&gt;【1】相似/不相似（即标签 \(y\)）是训练时的已知信息，但模型最终要求得的是这个关系在嵌入空间中的表达（距离）。在训练度量学习模型时，我们使用的训练数据必须是&lt;strong&gt;成对&lt;/strong&gt;或&lt;strong&gt;三元组&lt;/strong&gt;的，并且这些对或组的&lt;strong&gt;相似性标签是已知的&lt;/strong&gt;，相似/不相似是训练的输入（Ground Truth），而具有语义意义的距离 \(D\) 是训练的输出和最终推理的依据。&lt;/p&gt;

&lt;p&gt;【2】为什么在已知“相似/不相似”的情况下，还要追求&lt;strong&gt;距离的变化&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;传统的分类任务学习一个&lt;strong&gt;决策边界&lt;/strong&gt;，将不同类别的样本分隔开。但是度量学习任务学习一个&lt;strong&gt;嵌入空间（Embedding Space）&lt;/strong&gt;，使空间中的距离具有&lt;strong&gt;语义意义&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;传统分类只能回答“是不是猫”。度量学习要回答的是“这只猫和那只猫有多像？”。只有通过&lt;strong&gt;距离&lt;/strong&gt;，我们才能量化这种&lt;strong&gt;程度&lt;/strong&gt;。假设你训练了 100 种动物的分类器。来了一只新动物“袋鼠”，传统分类器无能为力。但在度量学习中，如果模型学会了将所有有袋动物都映射到空间中的一个区域，那么这只新袋鼠就会落在那个区域，通过计算它与现有样本的&lt;strong&gt;距离&lt;/strong&gt;，我们就能判断它的相似性和类别。&lt;/p&gt;

&lt;p&gt;在搜索引擎或推荐系统中，我们输入一张图片 \(A\)，希望找到最相似的 \(K\) 张图片。这需要计算 \(A\) 的嵌入向量与数据库中所有嵌入向量的&lt;strong&gt;距离&lt;/strong&gt;，然后按距离从小到大排序。如果模型只是简单地输出“相似”或“不相似”，就无法进行排序。&lt;/p&gt;

&lt;p&gt;追求距离的变化，是为了让这个嵌入空间具备&lt;strong&gt;泛化能力&lt;/strong&gt;和&lt;strong&gt;可度量性（Metricity）&lt;/strong&gt;，从而支持基于相似度的&lt;strong&gt;检索、聚类和验证&lt;/strong&gt;等高级任务。&lt;/p&gt;

&lt;p&gt;【3】一旦模型训练完成，在实际应用（推理）时，我们就&lt;strong&gt;不再需要&lt;/strong&gt;这个 \(Y\) 标签了。我们只输入两张图片 \(A\) 和 \(B\)，模型输出它们的距离 \(D_{A,B}\)。这个距离 \(D_{A,B}\) 就是模型对它们相似程度的&lt;strong&gt;预测&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;总结来说：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;训练时：&lt;/strong&gt; \(Y\) &lt;strong&gt;已知&lt;/strong&gt; (输入/标签)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;推理时：&lt;/strong&gt; \(Y\) &lt;strong&gt;未知&lt;/strong&gt; (通过计算距离 $D$ 来预测相似度)。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;除了对比损失，还有三元组损失和余弦相似度损失。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;三元组损失&lt;/strong&gt;：作用于&lt;strong&gt;三元组（锚点、正例、负例）&lt;/strong&gt;，以强制执行&lt;strong&gt;相对距离&lt;/strong&gt;。比对比损失更强大。它要求锚点（Anchor）到正例（Positive）的距离，&lt;strong&gt;必须&lt;/strong&gt;比锚点到负例（Negative）的距离&lt;strong&gt;至少小一个边际值&lt;/strong&gt;。这确保了相似样本被更好地聚类。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;余弦相似度损失&lt;/strong&gt;：在&lt;strong&gt;角度空间&lt;/strong&gt;中最大化正样本对之间的相似度。这类损失使用余弦相似度而非欧氏距离，更关注嵌入向量的&lt;strong&gt;方向&lt;/strong&gt;而不是长度。它将所有嵌入点都视为在单位超球体上，优化它们的相对角度。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;32损失函数的区别与对比&quot;&gt;3.2损失函数的区别与对比&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;任务类型&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;损失函数举例&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;目标&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;特点&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;分类 (Classification)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;交叉熵（Cross-Entropy）/KL 距离&lt;/td&gt;
      &lt;td&gt;最小化预测概率分布与真实标签分布之间的距离。&lt;/td&gt;
      &lt;td&gt;关注单个样本的&lt;strong&gt;类别标签&lt;/strong&gt;。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;回归 (Regression)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;均方误差（MSE）/欧氏距离&lt;/td&gt;
      &lt;td&gt;最小化预测值与真实值之间的数值差异。&lt;/td&gt;
      &lt;td&gt;关注单个样本的&lt;strong&gt;数值输出&lt;/strong&gt;。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;度量学习 (Metric Learning)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;对比损失（Contrastive Loss）&lt;/td&gt;
      &lt;td&gt;最小化相似样本对的距离，最大化不相似样本对的距离。&lt;/td&gt;
      &lt;td&gt;关注&lt;strong&gt;样本对&lt;/strong&gt;或&lt;strong&gt;三元组&lt;/strong&gt;之间的&lt;strong&gt;相对距离&lt;/strong&gt;。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;33嵌入向量&quot;&gt;3.3嵌入向量&lt;/h2&gt;

&lt;p&gt;【1】什么是嵌入？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;嵌入&lt;/strong&gt;是&lt;strong&gt;连续空间中对象的向量表示&lt;/strong&gt;。 想象一个坐标系，每个对象（词语、图像、用户）都被表示为一个坐标点，这个坐标就是嵌入向量。每个数据点都被映射到一个&lt;strong&gt;低维向量&lt;/strong&gt;，该向量&lt;strong&gt;捕获了它的语义意义&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果两个词的意义相似（如“国王”和“女王”），它们在嵌入空间中的向量就会非常接近。&lt;/p&gt;

&lt;p&gt;【2】嵌入向量：&lt;/p&gt;

&lt;p&gt;嵌入向量（Embedding Vector）就是&lt;strong&gt;模型在最后一层或倒数第二层的输出&lt;/strong&gt;，它代表了输入数据的&lt;strong&gt;低维、密集且富有语义信息的特征表示&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt; \(X\) \(\rightarrow\) &lt;strong&gt;通过模型的各个隐藏层&lt;/strong&gt;（卷积、池化、激活等操作） \(\rightarrow\) &lt;strong&gt;得到高级特征&lt;/strong&gt; \(\rightarrow\) &lt;strong&gt;通过最后的嵌入层（全连接层）&lt;/strong&gt; \(\rightarrow\) &lt;strong&gt;输出嵌入向量 \(\text{vector}\)&lt;/strong&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;度量学习中的 &lt;strong&gt;嵌入向量（Embedding Vector）&lt;/strong&gt; 和 &lt;strong&gt;OpenAI 的 Embedding 模型&lt;/strong&gt;（例如 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-embedding-3-small&lt;/code&gt; 或 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-embedding-ada-002&lt;/code&gt;）在概念和功能上是&lt;strong&gt;完全一致&lt;/strong&gt;的，它们都代表了将数据映射到语义空间的核心思想。&lt;/p&gt;

&lt;p&gt;度量学习中的嵌入向量是将输入样本（如图片 \(X\)）通过训练的模型 \(f\) 映射得到的低维向量 \(f(X)\)，确保&lt;strong&gt;语义相似&lt;/strong&gt;的样本对在嵌入空间中&lt;strong&gt;距离近&lt;/strong&gt;，本质是将高维、稀疏或复杂的原始数据转换为&lt;strong&gt;密集、连续的语义表示&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;而OpenAI 的 Embedding 模型将输入文本（如句子 \(T\)）通过预训练的模型 \(f\) 映射得到的低维向量 \(f(T)\)，确保&lt;strong&gt;语义相关&lt;/strong&gt;的文本段落在嵌入空间中&lt;strong&gt;距离近&lt;/strong&gt;。OpenAI 的 Embedding 模型是&lt;strong&gt;大规模、通用&lt;/strong&gt;的&lt;strong&gt;文本&lt;/strong&gt;度量学习模型的结果。它提供了一个&lt;strong&gt;预训练好的&lt;/strong&gt;特征提取器 \(f\)，让用户可以直接跳过复杂的训练步骤，将文本转化为具有语义意义的嵌入向量，并直接在应用中使用我们前面讨论的&lt;strong&gt;距离计算&lt;/strong&gt;进行检索和相似度判断。&lt;/p&gt;

&lt;h2 id=&quot;34代码&quot;&gt;3.4代码&lt;/h2&gt;

&lt;p&gt;我们将使用 scikit-learn 嵌入和简单的三元组方法来说明。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;euclidean_distances&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load Iris dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Standardize features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create embeddings (for simplicity, use the raw scaled features as embeddings)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_scaled&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Example: Find most similar item to the first sample
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;anchor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 选取第一个样本作为锚点（Anchor），即我们要查找相似项目的目标。
# 计算锚点与数据集中所有样本（包括它自己）之间的欧氏距离。结果 distances 是一个包含 150 个距离值的数组
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;euclidean_distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;anchor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# (排除锚点自身)查找最近邻,因为锚点到自身的距离总是 0，但这没有意义
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;closest_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Anchor label:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Closest label:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;closest_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Distance:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;closest_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;代码使用著名的 Iris（鸢尾花）数据集，它包含 150 个样本，每个样本有 4 个特征（花萼/花瓣的长度和宽度），以及 3 个类别标签（0, 1, 2）。&lt;/p&gt;

&lt;p&gt;对特征数据进行&lt;strong&gt;标准化&lt;/strong&gt;。标准化是必要的预处理步骤，它将每个特征的均值变为 0、标准差变为 1，防止量纲差异影响距离计算。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;直接将标准化后的原始特征&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X_scaled&lt;/code&gt; 视为&lt;strong&gt;嵌入向量（Embeddings）&lt;/strong&gt;。(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;embeddings = X_scaled&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;代码会输出锚点样本的类别、距离最近样本的类别以及它们之间的距离&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124141318756.png&quot; alt=&quot;image-20251124141318756&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Iris 样本的 4 个标准化后的特征 \([X_1, X_2, X_3, X_4]\) 构成了它的嵌入向量。这是度量学习的基础 。距离计算（欧氏距离）是度量相似性的核心。距离越小，相似度越高。&lt;/p&gt;

&lt;p&gt;真正的度量学习会训练一个复杂的&lt;strong&gt;神经网络&lt;/strong&gt;（一个非线性转换函数），这个网络负责&lt;strong&gt;学习&lt;/strong&gt;最好的特征表示，确保同一个类别的样本（相似点）距离比不同类别的样本（不相似点）更近。&lt;/p&gt;

&lt;p&gt;在人脸识别等任务中，深度学习模型通过 &lt;strong&gt;Triplet Loss&lt;/strong&gt; 或 &lt;strong&gt;Contrastive Loss&lt;/strong&gt; 不断调整其内部参数，从而生成具有高区分度的嵌入向量。例如，即使两个人脸图像的姿态和光照不同，只要是同一个人，它们的嵌入距离也会非常小。&lt;/p&gt;

&lt;h2 id=&quot;35度量学习的局限&quot;&gt;3.5度量学习的局限&lt;/h2&gt;

&lt;p&gt;只能用来判断相似，不能用来判断不相似的正确性&lt;/p&gt;

&lt;p&gt;度量学习模型在&lt;strong&gt;识别相似样本（Positive Case）&lt;/strong&gt;方面表现出色，但它在&lt;strong&gt;确认不相似样本（Negative Case）的“真实性”或“正确性”&lt;/strong&gt;方面是受限的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;模型是在一个由训练数据定义的语义空间内进行度量。对于落在语义空间边界之外的输入，模型只能将其判定为“不相似”或“距离很远”，但无法判断这个“不相似”的输入是否具有有效性、真实性或正确性。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果攻击者使用一个模型从未见过的高级合成人脸（Deepfake），模型可能只会判断“不相似”，但无法确定这是“欺诈”还是“新用户”，需要额外的&lt;strong&gt;活体检测&lt;/strong&gt;或&lt;strong&gt;异常检测&lt;/strong&gt;系统来辅助判断输入 \(C\) 本身的正确性。&lt;/p&gt;

&lt;p&gt;搜索结果中出现一个距离很远的项，可能是因为它不相似，也可能是因为它是系统无法理解的垃圾数据。&lt;/p&gt;

&lt;p&gt;度量学习提供了一个强大的&lt;strong&gt;比较工具&lt;/strong&gt;，但它本身不是一个&lt;strong&gt;验证工具&lt;/strong&gt;。它擅长回答“它们是否一致？”，但不擅长回答“这个输入本身是否有效且正确？”。为了解决这个局限，通常需要将度量学习与传统的分类或异常检测技术结合使用。&lt;/p&gt;

&lt;h2 id=&quot;36使用场景&quot;&gt;3.6使用场景&lt;/h2&gt;

&lt;p&gt;推荐系统是一个典型的&lt;strong&gt;大数据&lt;/strong&gt;问题。假设商品数量 \(N=10000\) 个，目标是从中找到 \(K=10\) 个用户最喜欢的。直接计算个商10000品与用户之间的精确关系是非常耗时的。因此，需要分两步走。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;召回（recall）：&lt;strong&gt;快速&lt;/strong&gt;、&lt;strong&gt;高效&lt;/strong&gt;地从海量数据中筛选出用户&lt;strong&gt;可能&lt;/strong&gt;喜欢的少数候选集&lt;/li&gt;
  &lt;li&gt;排序（rank）：在召回的 100 个候选商品中，使用更复杂的模型进行&lt;strong&gt;精细化打分&lt;/strong&gt;，找出用户&lt;strong&gt;最&lt;/strong&gt;喜欢的 10 个（优中选优）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前2个方案：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;度量学习/孪生网络：高效率，低精度要求&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124143258719.png&quot; alt=&quot;image-20251124143258719&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DNN：充分的特征交叉，模型能够学习用户和商品特征之间&lt;strong&gt;复杂、非线性的交互关系&lt;/strong&gt;，低效率可接受，高精度要求，效果是更好的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251124143312547.png&quot; alt=&quot;image-20251124143312547&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;度量学习（方案 1）擅长处理&lt;strong&gt;海量数据&lt;/strong&gt;下的&lt;strong&gt;相似性检索&lt;/strong&gt;，能以高效率完成&lt;strong&gt;快速筛选（召回）&lt;/strong&gt;任务。&lt;/p&gt;

&lt;p&gt;方案 2 的计算复杂度高，但在小集合（100个）上能提供极高的&lt;strong&gt;准确性&lt;/strong&gt;和&lt;strong&gt;排序精度&lt;/strong&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;这种&lt;strong&gt;“粗排（召回）+ 精排（排序）”&lt;/strong&gt;的两阶段策略，是所有大规模推荐系统在&lt;strong&gt;保证用户体验（高精度）&lt;/strong&gt;和&lt;strong&gt;系统性能（高效率）&lt;/strong&gt;之间寻求平衡的最佳实践。&lt;/p&gt;

&lt;h2 id=&quot;37推荐的项目框架&quot;&gt;3.7推荐的项目/框架&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;YouTube DNN&lt;/strong&gt;（特指其在 2016 年论文《Deep Neural Networks for YouTube Recommendations》中提出的两阶段深度学习架构）在推荐系统领域，&lt;strong&gt;确实是度量学习的一个里程碑式的集大成者，但它不再是目前最尖端的单一技术。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;原始的 DNN 模型相对简单，后来的推荐系统采用了更复杂的模型来替代或增强它，包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Wide &amp;amp; Deep Learning:&lt;/strong&gt; 结合了浅层记忆能力和深度泛化能力。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transformer 模型：&lt;/strong&gt; 利用自注意力机制（Self-Attention）来捕捉用户行为序列中的复杂依赖关系，在序列建模上表现远超 DNN。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph Neural Networks (GNNs):&lt;/strong&gt; 利用用户-物品交互图中的连接信息来生成更精确的嵌入。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以说，&lt;strong&gt;YouTube DNN 奠定了现代推荐系统的基石&lt;/strong&gt;，&lt;strong&gt;它将度量学习从理论推向了工业实践的巅峰&lt;/strong&gt;。但现在，它的具体架构已经被更新、更复杂的 &lt;strong&gt;Transformer、GNN、多任务学习&lt;/strong&gt;等技术所取代或吸收。&lt;/p&gt;

</description>
        <pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/24/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9913/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/24/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9913/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙12】深度学习第2篇-梯度下降法和矩阵求导术 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;1softmax函数求导&quot;&gt;1.Softmax函数求导&lt;/h1&gt;

&lt;p&gt;Softmax 函数将这个向量转化为概率向量 \(\mathbf{y} = [y_1, y_2, \dots, y_N]^T\)，其中每个元素 \(y_i\) 表示输入属于第 \(i\) 个类别的概率：&lt;/p&gt;

\[y_i = P(Y=i|\mathbf{d}) = \frac{e^{d_i}}{\sum_{j=1}^{N} e^{d_j}}\]

&lt;p&gt;在神经网络的反向传播过程中，我们需要计算 Softmax 输出 \(y_i\) 对其输入 \(d_k\) 的偏导数 \(\frac{\partial y_i}{\partial d_k}\)。由于输出 \(y_i\) 取决于所有的输入 \(d_1, d_2, \dots, d_N\)，求导需要分两种情况讨论：&lt;/p&gt;

&lt;p&gt;符号定义：&lt;/p&gt;

&lt;p&gt;令 \(S = \sum_{j=1}^{N} e^{d_j}\)，则 Softmax 函数可以简写为 \(y_i = \frac{e^{d_i}}{S}\)。&lt;/p&gt;

&lt;p&gt;情况一：\(i=k\)（对角线元素）&lt;/p&gt;

&lt;p&gt;我们需要计算 \(\frac{\partial y_i}{\partial d_i}\)。使用&lt;strong&gt;除法法则&lt;/strong&gt; \(\left(\frac{u}{v}\right)&apos; = \frac{u&apos;v - uv&apos;}{v^2}\)，其中 \(u = e^{d_i}\)， \(v = S = \sum_{j=1}^{N} e^{d_j}\)。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;计算 \(u\) 对 \(d_i\) 的导数：&lt;/p&gt;

\[\frac{\partial u}{\partial d_i} = \frac{\partial (e^{d_i})}{\partial d_i} = e^{d_i}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;计算 \(v\) 对 \(d_i\) 的导数：&lt;/p&gt;

\[\frac{\partial v}{\partial d_i} = \frac{\partial (\sum_{j=1}^{N} e^{d_j})}{\partial d_i} = e^{d_i} \quad (\text{因为求和项中只有 } e^{d_i} \text{ 对 } d_i \text{求导不为零})\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;应用除法法则：&lt;/p&gt;

\[\frac{\partial y_i}{\partial d_i} = \frac{\frac{\partial u}{\partial d_i} \cdot v - u \cdot \frac{\partial v}{\partial d_i}}{v^2} = \frac{e^{d_i} \cdot S - e^{d_i} \cdot e^{d_i}}{S^2}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;化简：&lt;/p&gt;

\[\frac{\partial y_i}{\partial d_i} = \frac{e^{d_i}}{S} \cdot \frac{S - e^{d_i}}{S} = \frac{e^{d_i}}{S} \cdot \left(1 - \frac{e^{d_i}}{S}\right) = y_i (1 - y_i)\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;情况二：\(i \ne k\)（非对角线元素）&lt;/p&gt;

&lt;p&gt;我们需要计算 \(\frac{\partial y_i}{\partial d_k}\)。其中 \(u = e^{d_i}\)， \(v = S = \sum_{j=1}^{N} e^{d_j}\)。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;计算 \(u\) 对 \(d_k\) 的导数：&lt;/p&gt;

\[\frac{\partial u}{\partial d_k} = \frac{\partial (e^{d_i})}{\partial d_k} = 0 \quad (\text{因为 } e^{d_i} \text{ 不包含 } d_k)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;计算 \(v\) 对 \(d_k\) 的导数：&lt;/p&gt;

\[\frac{\partial v}{\partial d_k} = \frac{\partial (\sum_{j=1}^{N} e^{d_j})}{\partial d_k} = e^{d_k} \quad (\text{因为求和项中只有 } e^{d_k} \text{ 对 } d_k \text{求导不为零})\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;应用除法法则：&lt;/p&gt;

\[\frac{\partial y_i}{\partial d_k} = \frac{\frac{\partial u}{\partial d_k} \cdot v - u \cdot \frac{\partial v}{\partial d_k}}{v^2} = \frac{0 \cdot S - e^{d_i} \cdot e^{d_k}}{S^2}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;化简：&lt;/p&gt;

\[\frac{\partial y_i}{\partial d_k} = -\frac{e^{d_i} e^{d_k}}{S^2} = -\frac{e^{d_i}}{S} \cdot \frac{e^{d_k}}{S} = -y_i y_k\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;总结softmax 雅可比矩阵&lt;/p&gt;

&lt;p&gt;Softmax 函数的求导结果是紧凑且优美的，可以直接用于反向传播：&lt;/p&gt;

\[\frac{\partial y_i}{\partial d_k} = \begin{cases} y_i (1 - y_i) &amp;amp; \text{if } i = k \\ -y_i y_k &amp;amp; \text{if } i \ne k \end{cases}\]

&lt;h1 id=&quot;2交叉熵损失-梯度下降&quot;&gt;2.交叉熵损失-梯度下降&lt;/h1&gt;

&lt;p&gt;现在来推导 Softmax 回归损失函数（多分类交叉熵）相对于模型参数 \(\theta_j\) 的梯度。这个梯度是训练模型时使用梯度下降法的核心。&lt;/p&gt;

&lt;p&gt;为了简洁，我们只推导&lt;strong&gt;单个样本&lt;/strong&gt;的损失函数 \(J^{(i)}(\theta)\) 相对于&lt;strong&gt;某个特定类别 \(j\) 的参数向量 \(\theta_j\)&lt;/strong&gt; 的梯度。&lt;/p&gt;

&lt;p&gt;单个样本 \((x, y)\) 的损失函数（负对数似然）为：&lt;/p&gt;

\[J(\theta) = - \log\left( \phi_{y} \right)\]

&lt;p&gt;其中，\(\phi_{y}\) 是模型对真实类别 \(y\) 预测的概率。&lt;/p&gt;

&lt;p&gt;\(\phi_j\) 是 Softmax 函数：&lt;/p&gt;

\[\phi_j = \frac{e^{\eta_j}}{\sum_{l=1}^{k} e^{\eta_l}}\]

&lt;p&gt;并且 \(\eta_j = \theta_j^T x\)。&lt;/p&gt;

&lt;p&gt;我们的目标是计算损失函数 \(J(\theta)\) 对第 \(j\) 个类别的参数向量 \(\theta_j\) 的偏导数：&lt;/p&gt;

\[\nabla_{\theta_j} J(\theta) = \frac{\partial J(\theta)}{\partial \theta_j}\]

&lt;hr /&gt;

&lt;p&gt;\(J(\theta)\) 是关于 \(\phi_y\) 的函数，\(\phi_y\) 是关于 \(\eta_l\) 的函数，而 \(\eta_l\) 是关于 \(\theta_j\) 的函数，我们需要使用链式法则：&lt;/p&gt;

\[\frac{\partial J(\theta)}{\partial \theta_j} = \sum_{l=1}^{k} \left( \frac{\partial J(\theta)}{\partial \phi_l} \cdot \frac{\partial \phi_l}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j} \right)\]

&lt;p&gt;\(\frac{\partial J(\theta)}{\partial \phi_l}\) 只有在 \(l=y\) 时非零，所以简化为：&lt;/p&gt;

\[\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial J(\theta)}{\partial \phi_y} \cdot \frac{\partial \phi_y}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j}\]

&lt;p&gt;然后计算各部分偏导数：&lt;/p&gt;

&lt;p&gt;A. \(\frac{\partial J(\theta)}{\partial \phi_y}\) (损失函数对概率的导数)&lt;/p&gt;

\[J(\theta) = - \log(\phi_y)\]

\[\frac{\partial J(\theta)}{\partial \phi_y} = \frac{\partial (-\log(\phi_y))}{\partial \phi_y} = - \frac{1}{\phi_y}\]

&lt;p&gt;B. \(\frac{\partial \eta_j}{\partial \theta_j}\) (自然参数对参数的导数)&lt;/p&gt;

\[\eta_j = \theta_j^T x\]

\[\frac{\partial \eta_j}{\partial \theta_j} = x\]

&lt;p&gt;(注意 \(\frac{\partial}{\partial \theta_j}\) 意味着对向量 \(\theta_j\) 求梯度，结果是向量 \(x\)。)&lt;/p&gt;

&lt;p&gt;C. \(\frac{\partial \phi_y}{\partial \eta_j}\) (Softmax 对原始分数的导数)&lt;/p&gt;

&lt;p&gt;这是最复杂的一步，需要根据 \(j\) 是否等于真实类别 \(y\) 进行分情况讨论：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Case 1: \(j = y\) (计算 \(\phi_y\) 对 \(\eta_y\) 的导数)&lt;/strong&gt;&lt;/p&gt;

\[\phi_y = \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}}\]

&lt;p&gt;使用商法则 \(\left(\frac{u}{v}\right)&apos; = \frac{u&apos;v - uv&apos;}{v^2}\)，其中 \(u = e^{\eta_y}\)，\(v = \sum_{l=1}^{k} e^{\eta_l}\)。&lt;/p&gt;

\[\frac{\partial \phi_y}{\partial \eta_y} = \frac{(e^{\eta_y}) \sum_{l=1}^{k} e^{\eta_l} - e^{\eta_y} (e^{\eta_y})}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2}\]

\[= \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}} - \frac{e^{\eta_y} e^{\eta_y}}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2} = \phi_y - \phi_y \cdot \phi_y\]

\[\frac{\partial \phi_y}{\partial \eta_y} = \phi_y (1 - \phi_y)\]

&lt;p&gt;&lt;strong&gt;Case 2: \(j \ne y\) (计算 \(\phi_y\) 对 \(\eta_j\) 的导数)&lt;/strong&gt;&lt;/p&gt;

\[\frac{\partial \phi_y}{\partial \eta_j} = \frac{(0) \sum_{l=1}^{k} e^{\eta_l} - e^{\eta_y} (e^{\eta_j})}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2}\]

\[= - \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}} \cdot \frac{e^{\eta_j}}{\sum_{l=1}^{k} e^{\eta_l}} = - \phi_y \cdot \phi_j\]

\[\frac{\partial \phi_y}{\partial \eta_j} = - \phi_y \phi_j\]

&lt;hr /&gt;

&lt;p&gt;现在，将 A, B, C 代回链式法则，同样分 \(j=y\) 和 \(j \ne y\) 两种情况。&lt;/p&gt;

\[\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial J(\theta)}{\partial \phi_y} \cdot \frac{\partial \phi_y}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j}\]

&lt;p&gt;&lt;strong&gt;Case 1: \(j = y\) (真实类别的参数梯度)&lt;/strong&gt;&lt;/p&gt;

\[\nabla_{\theta_y} J(\theta) = \underbrace{\left(- \frac{1}{\phi_y}\right)}_{\text{A}} \cdot \underbrace{\left(\phi_y (1 - \phi_y)\right)}_{\text{C1}} \cdot \underbrace{(x)}_{\text{B}}\]

\[= - (1 - \phi_y) x = (\phi_y - 1) x\]

&lt;p&gt;&lt;strong&gt;Case 2: \(j \ne y\) (非真实类别的参数梯度)&lt;/strong&gt;&lt;/p&gt;

\[\nabla_{\theta_j} J(\theta) = \underbrace{\left(- \frac{1}{\phi_y}\right)}_{\text{A}} \cdot \underbrace{\left(- \phi_y \phi_j\right)}_{\text{C2}} \cdot \underbrace{(x)}_{\text{B}}\]

\[= \phi_j x\]

&lt;p&gt;其中\(\phi_j\) 是 Softmax 函数：&lt;/p&gt;

\[\phi_j = \frac{e^{\eta_j}}{\sum_{l=1}^{k} e^{\eta_l}}\]

&lt;p&gt;并且 \(\eta_j = \theta_j^T x\)。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;可以用一个统一的公式来表达梯度 \(\nabla_{\theta_j} J(\theta)\)：&lt;/p&gt;

\[\nabla_{\theta_j} J(\theta) = (\phi_j - \mathbb{I}\{y=j\}) x\]

  &lt;p&gt;其中，\(\mathbb{I}\{y=j\}\) 是指示函数：如果 \(j\) 是真实类别 \(y\)，则为1；否则为0。&lt;/p&gt;

  &lt;p&gt;【注意】括号内的项 \((\phi_j - \mathbb{I}\{y=j\})\) 是&lt;strong&gt;预测概率&lt;/strong&gt;和&lt;strong&gt;真实标签&lt;/strong&gt;之间的&lt;strong&gt;误差&lt;/strong&gt;。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;如果 \(j\) 是真实类别 \(y\)：&lt;/strong&gt; 误差是 \((\phi_y - 1)\)。由于 \(\phi_y &amp;lt; 1\)，误差是负的。梯度下降会向&lt;strong&gt;增加&lt;/strong&gt; \(\phi_y\) 的方向调整 \(\theta_y\)。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;如果 \(j\) 不是真实类别 \(y\)：&lt;/strong&gt; 误差是 \((\phi_j - 0) = \phi_j\)。梯度是正的。梯度下降会向&lt;strong&gt;减少&lt;/strong&gt; \(\phi_j\) 的方向调整 \(\theta_j\)。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;模型的参数 \(\theta_j\) 的更新量正比于：&lt;/p&gt;

\[\text{误差} \times \text{输入特征}\]

&lt;p&gt;&lt;strong&gt;这个公式与线性回归和逻辑回归的梯度公式形式惊人地相似，展现了 GLM 框架的统一性。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;有了这个梯度，我们就可以使用梯度下降（或其变体）来迭代更新所有 \(\theta_j\) 向量，直到损失函数收敛到最小值。&lt;/p&gt;

&lt;h1 id=&quot;2矩阵求导术&quot;&gt;2.矩阵求导术&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122224506773.png&quot; alt=&quot;image-20251122224506773&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;21向量求导&quot;&gt;2.1向量求导&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251122224527702.png&quot; alt=&quot;image-20251122224527702&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;22矩阵求导&quot;&gt;2.2矩阵求导&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122232509775.png&quot; alt=&quot;image-20251122232509775&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122224551401.png&quot; alt=&quot;image-20251122224551401&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122224620015.png&quot; alt=&quot;image-20251122224620015&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;23矩阵求导链式法则&quot;&gt;2.3矩阵求导链式法则&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122231850423.png&quot; alt=&quot;image-20251122231850423&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/24/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9912/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/24/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9912/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙11】深度神经网络入门</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;1机器学习中最大的难点&quot;&gt;1.机器学习中最大的难点&lt;/h1&gt;

&lt;p&gt;特征！特征！特征！&lt;/p&gt;

&lt;p&gt;一个好的特征，即使最简单的逻辑回归，也能出色的完成任务&lt;/p&gt;

&lt;p&gt;好特征的标准：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;区分性强：特征值在不同类别之间应有显著差异，而在同一类别内部应保持相似。&lt;/li&gt;
  &lt;li&gt;特征多：特征的数量并非越多越好，而是要&lt;strong&gt;尽可能覆盖目标变量的所有影响因素&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;特征的各类组合：有的时候看基础特征进行分类，意义不大。但是特征组合就很厉害。比如双十一时期，二三十岁的女性。需要升维（特征组合等）/使用非线性的模型（比如FM，深度学习等）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在实际工作中，数据科学家通常将 &lt;strong&gt;70%&lt;/strong&gt; 的时间投入到特征工程中，因为这是决定项目成败的关键。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;FM模型自动学习两两特征之间的潜在交互强度，但是只能组合&lt;strong&gt;二阶&lt;/strong&gt;特征，更高阶的组合仍需手动定义或忽略。&lt;/p&gt;

&lt;p&gt;借鉴FM模型，我们可以提供特征组合的框架，具体组合方式，由模型自动学习&lt;/p&gt;

&lt;h1 id=&quot;2一层神经网络&quot;&gt;2.一层神经网络&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122171035499.png&quot; alt=&quot;image-20251122171035499&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每个d都由所有x加权求和组成，权重为自动学习，即输出层的每个神经元，都是输入层所有特征的加权求和。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;一层神经网络&lt;/strong&gt;（通常也称为&lt;strong&gt;单层感知机&lt;/strong&gt;或&lt;strong&gt;无激活函数的全连接层&lt;/strong&gt;）&lt;/p&gt;

&lt;p&gt;这个网络模型由&lt;strong&gt;输入层&lt;/strong&gt;和&lt;strong&gt;输出层（隐藏层）&lt;/strong&gt;组成。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;输入向量 \(x\)：&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;表示 \(m\) 个输入特征，\(x = [x_1, x_2, \dots, x_m]^T\)。&lt;/li&gt;
      &lt;li&gt;它们是模型接收的原始数据点。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;输出向量 \(d^{(1)}\)：&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;表示 \(M^{(1)}\) 个神经元的输出，\(d^{(1)} = [d_1^{(1)}, d_2^{(1)}, \dots, d_{M^{(1)}}^{(1)}]^T\)。&lt;/li&gt;
      &lt;li&gt;上标 \(^{(1)}\) 表示这是第一层（通常也是&lt;strong&gt;唯一一层&lt;/strong&gt;隐藏层）的输出。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于输出层的第 \(j\) 个神经元 \(d_j^{(1)}\) 而言：&lt;/p&gt;

\[d_{j}^{(1)} = w_{j,1}^{(1)} x_{1} + \dots + w_{j,m}^{(1)} x_{m}\]

&lt;ul&gt;
  &lt;li&gt;\(w_{j,i}^{(1)}\)：是连接输入特征 \(x_i\) 到输出神经元 \(d_j^{(1)}\) 的&lt;strong&gt;权重 (Weight)&lt;/strong&gt;。这个权重衡量了输入 \(x_i\) 对输出 \(d_j^{(1)}\) 的重要性或贡献度。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;标准的神经网络计算通常还会包括一个偏置项 \(b_j\)，即 \(d_j^{(1)} = (\sum_{i=1}^{m} w_{j,i}^{(1)} x_{i}) + b_{j}^{(1)}\)。图中为了简化表示，省略了偏置项。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;矩阵形式的表达（效率和抽象）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将所有 \(M^{(1)}\) 个神经元的计算过程抽象为一个&lt;strong&gt;矩阵运算&lt;/strong&gt;，更加简洁高效：&lt;/p&gt;

\[d^{(1)} = W^{(1)} x\]

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251122174108626.png&quot; alt=&quot;image-20251122174108626&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;权重矩阵 \(W^{(1)}\)：&lt;/strong&gt; 这是一个 \(M^{(1)} \times m\) 维度的矩阵，包含了所有连接权重 \(w_{j,i}^{(1)}\)。
    &lt;ul&gt;
      &lt;li&gt;矩阵的每一行对应一个输出神经元 \(d_j^{(1)}\)。&lt;/li&gt;
      &lt;li&gt;矩阵的每一列对应一个输入特征 \(x_i\)。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;矩阵乘法 \(W^{(1)} x\) 实现了所有神经元同时进行加权求和的计算。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个模型的目标就是通过学习，找到最优的权重矩阵 \(W^{(1)}\)，使得模型的预测输出最接近真实值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;学习过程：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;模型计算当前权重下的输出 \(d^{(1)}\)。&lt;/li&gt;
  &lt;li&gt;计算输出与真实标签之间的&lt;strong&gt;损失 (Loss)&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;通过&lt;strong&gt;梯度下降 (Gradient Descent)&lt;/strong&gt; 等优化算法和&lt;strong&gt;反向传播 (Backpropagation)&lt;/strong&gt; 机制，计算损失对每个权重 \(w_{j,i}^{(1)}\) 的梯度（即偏导数）。&lt;/li&gt;
  &lt;li&gt;根据梯度来更新和调整权重 \(W^{(1)}\)，使损失函数减小。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果这个“一层神经网络”的输出 \(d^{(1)}\) 直接作为最终输出，并且没有使用非线性激活函数，那么这个模型本质上执行的是一个&lt;strong&gt;线性变换&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;作为线性模型：&lt;/strong&gt; 如果 \(M^{(1)}=1\)（只有一个输出神经元），并且在其上加一个 Sigmoid 激活函数，它就退化成一个&lt;strong&gt;逻辑回归 (Logistic Regression) 分类器&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;作为特征提取器：&lt;/strong&gt; 在多层网络中，这一层被称为&lt;strong&gt;隐藏层&lt;/strong&gt;。它将原始的 \(m\) 维输入特征 \(x\) 映射到了一个新的 \(M^{(1)}\) 维&lt;strong&gt;特征表示&lt;/strong&gt; \(d^{(1)}\)。这个新的 \(d^{(1)}\) 包含了原始特征的&lt;strong&gt;线性组合&lt;/strong&gt;信息，为后续更深层的非线性处理奠定了基础。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;21维度转换&quot;&gt;2.1维度转换&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122175839464.png&quot; alt=&quot;image-20251122175839464&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(m &amp;gt; n\)，升维/维度扩展，增加数据点的&lt;strong&gt;线性可分性&lt;/strong&gt;，提高分辨能力&lt;/li&gt;
  &lt;li&gt;\(m &amp;lt; n\)，降维/维度压缩，特征中可能包含大量&lt;strong&gt;冗余、噪声或不相关&lt;/strong&gt;的特征，去除无用数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般的神经网络，一般都是先升维然后降维。&lt;/p&gt;

&lt;h2 id=&quot;22非线性变换&quot;&gt;2.2非线性变换&lt;/h2&gt;

&lt;p&gt;加权后，还需要进行非线性变换 (激活函数)&lt;/p&gt;

\[a^{(1)} = f(d^{(1)} + w0^{(1)}) = \begin{bmatrix} f(d_1^{(1)} + w0_1^{(1)}) \\ \vdots \\ f(d_{M^{(1)}}^{(1)} + w0_{M^{(1)}}^{(1)}) \end{bmatrix}\]

\[a^{(1)} = f(d^{(1)} + w0^{(1)})\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;\(d^{(1)}\)：&lt;/strong&gt; 这是前一步骤的&lt;strong&gt;线性输出&lt;/strong&gt;。 \(d^{(1)} = W^{(1)} x\)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(w0^{(1)}\)：&lt;/strong&gt; 这是该层的&lt;strong&gt;偏置项 (Bias)&lt;/strong&gt;，通常记为 \(b^{(1)}\)。在向量形式中，\(w0^{(1)}\) 就是 \(b^{(1)}\) 向量，维度与 \(d^{(1)}\) 相同。
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;\(d^{(1)} + w0^{(1)}\)：&lt;/strong&gt; 这构成了神经元完整的&lt;strong&gt;净输入 (Net Input)&lt;/strong&gt; 或&lt;strong&gt;加权和&lt;/strong&gt;。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(f(\cdot)\)：&lt;/strong&gt; 这就是&lt;strong&gt;激活函数 (Activation Function)&lt;/strong&gt;，它是一个&lt;strong&gt;非线性函数&lt;/strong&gt;。
    &lt;ul&gt;
      &lt;li&gt;\(f(\cdot)\) 被逐元素（element-wise）地应用到净输入向量的每个分量上。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(a^{(1)}\)：&lt;/strong&gt; 这是该层的&lt;strong&gt;激活输出 (Activation Output)&lt;/strong&gt;，也是下一层的输入特征。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为什么要“一定要进行非线性变换”？&lt;/p&gt;

&lt;p&gt;这是深度学习中最重要的概念之一。如果神经网络中&lt;strong&gt;不使用任何激活函数&lt;/strong&gt;（即 \(f(x)=x\)，使用线性函数），那么无论网络堆叠多少层，整个网络最终都等同于&lt;strong&gt;一个单一的线性模型&lt;/strong&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;假设我们有一个两层网络，都只使用线性变换：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;第一层输出 (线性)：&lt;/strong&gt; \(d^{(1)} = W^{(1)} x\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;第二层输出 (线性)：&lt;/strong&gt; \(d^{(2)} = W^{(2)} d^{(1)}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将 \(d^{(1)}\) 代入 \(d^{(2)}\) 中：&lt;/p&gt;

\[d^{(2)} = W^{(2)} (W^{(1)} x)\]

&lt;p&gt;根据矩阵乘法的结合律，我们可以定义一个新的矩阵 \(W_{\text{new}} = W^{(2)} W^{(1)}\)：&lt;/p&gt;

\[d^{(2)} = W_{\text{new}} x\]

&lt;p&gt;&lt;strong&gt;结论：&lt;/strong&gt; 无论堆叠多少层线性层，整个网络的输出始终是&lt;strong&gt;输入 \(x\) 的一个线性函数&lt;/strong&gt; \(W_{\text{new}} x\)。&lt;/p&gt;

&lt;p&gt;线性模型（如逻辑回归）只能找到一个&lt;strong&gt;直线或平面&lt;/strong&gt;作为决策边界，无法拟合现实世界中大量存在的&lt;strong&gt;非线性&lt;/strong&gt;关系（例如圆圈、S 形、复杂曲线等）。 如果没有非线性，堆叠再多的层也无法增强模型的表达能力。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;激活函数通过引入非线性，为神经网络赋予了强大的能力：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;允许网络拟合&lt;strong&gt;任意复杂的函数&lt;/strong&gt;。这是区分深度学习与传统线性模型的关键。&lt;/li&gt;
  &lt;li&gt;每一层激活函数后的输出 \(a^{(1)}\) 都是&lt;strong&gt;非线性组合后的新特征&lt;/strong&gt;，这些特征比原始输入具有更高的抽象层次和更强的区分性。&lt;/li&gt;
  &lt;li&gt;激活函数（尤其是 Sigmoid 和 Tanh）最初是为了模仿生物神经元&lt;strong&gt;“阈值触发”&lt;/strong&gt;的机制，即当净输入超过某个阈值时，神经元才被激活。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;23常见的非线性激活函数&quot;&gt;2.3常见的非线性激活函数&lt;/h2&gt;

&lt;p&gt;(1)Sigmoid:将输出压缩到 \((0, 1)\) 区间，常用于二分类输出层。但容易导致梯度消失。&lt;/p&gt;

\[f(z) = \frac{1}{1 + e^{-z}}\]

&lt;p&gt;(2)Tanh:将输出压缩到 \((-1, 1)\) 区间，输出均值更接近 0，收敛速度通常比 Sigmoid 快。&lt;/p&gt;

\[f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]

&lt;p&gt;(3)ReLU:最常用。计算简单（只有阈值判断），有效解决了梯度消失问题，加速了网络训练。&lt;/p&gt;

\[f(z) = \max(0, z)\]

&lt;h2 id=&quot;24两层神经网络&quot;&gt;2.4两层神经网络&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122183830418.png&quot; alt=&quot;image-20251122183830418&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输入层 \(x\)：&lt;/strong&gt; 包含 \(m\) 个特征 \(x_1\) 到 \(x_m\)。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一层（隐藏层/全连接层）：&lt;/strong&gt; 进行了两次操作。是整个网络的核心，它实现了之前讨论的&lt;strong&gt;特征工程自动化&lt;/strong&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;线性加权和 (Weighted Sum)：&lt;/strong&gt; 从 \(x\) 到 \(d^{(1)}\)。\(d^{(1)} = [d_1^{(1)}, d_2^{(1)}, \dots, d_{M^{(1)}}^{(1)}]^T\)，其中\(d_{j}^{(1)} = w_{j,1}^{(1)} x_{1} + \dots + w_{j,m}^{(1)} x_{m}\)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;非线性激活 (Activation)：&lt;/strong&gt; 从 \(d^{(1)}\) 到 \(a^{(1)}\)，通过激活函数 \(f\) 实现。\(a^{(1)} = f(d^{(1)} + w0^{(1)})\)&lt;/li&gt;
  &lt;li&gt;这是一个&lt;strong&gt;线性变换&lt;/strong&gt;，将 \(m\) 维的原始输入 \(x\) 映射到一个 \(M^{(1)}\) 维的新空间 \(d^{(1)}\)。这一步完成了所有特征的&lt;strong&gt;线性交叉和重组&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;“全连接层”是基于它的&lt;/strong&gt;结构（拓扑）&lt;strong&gt;命名的；而&lt;/strong&gt;“隐藏层”是基于它的&lt;strong&gt;位置（功能）&lt;/strong&gt;命名的。&lt;/p&gt;

  &lt;p&gt;在大多数情况下，一个隐藏层同时也是一个全连接层，因此这两个术语经常互换使用。&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;“隐藏”是因为这层神经元的输入和输出既不是直接的原始输入 \(x\)，也不是最终的预测输出 \(y&apos;\)，而是&lt;strong&gt;位于输入和输出之间的层&lt;/strong&gt;。只有&lt;strong&gt;位于输入层和输出层之间的层&lt;/strong&gt;才能被称为隐藏层。一个网络可以有零个、一个或多个隐藏层。&lt;/li&gt;
    &lt;li&gt;“全连接”是描述这一层神经元之间的&lt;strong&gt;连接拓扑结构&lt;/strong&gt;。在一个全连接层中，该层的&lt;strong&gt;每一个&lt;/strong&gt;神经元都与前一层的&lt;strong&gt;所有&lt;/strong&gt;神经元（或输入特征）相连接。这种结构在数学上体现为&lt;strong&gt;矩阵乘法&lt;/strong&gt; \(d = Wx\)。绝大多数神经网络（如 MLP）的隐藏层都是全连接层。网络的最终输出层（例如分类器的 Softmax 层之前）通常也是一个全连接层。像&lt;strong&gt;卷积神经网络（CNN）&lt;/strong&gt;中的&lt;strong&gt;卷积层 (Convolutional Layer)&lt;/strong&gt; 就&lt;strong&gt;不是&lt;/strong&gt;全连接层，因为卷积核只连接到前一层特征图的&lt;strong&gt;局部区域&lt;/strong&gt;。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;第二层（输出层）：&lt;/strong&gt; 同样进行了两次操作。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;线性加权和：&lt;/strong&gt; 从 \(a^{(1)}\) 到 \(d^{(2)}\)。引入&lt;strong&gt;非线性&lt;/strong&gt;，使得新的特征表示 \(a^{(1)}\) 能够捕捉输入特征之间&lt;strong&gt;非线性的、高阶的复杂关系&lt;/strong&gt;。向量 \(a^{(1)}\) 是由 \(M^{(1)}\) 个&lt;strong&gt;高度抽象的、非线性组合的新特征&lt;/strong&gt;构成，这些特征是模型自动从数据中学习到的。“自动”体现在&lt;strong&gt;模型无需人类预先设计任何特征组合规则&lt;/strong&gt;，完全依靠数据驱动来找到最佳的特征表示。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;非线性激活 (Sigmoid)：&lt;/strong&gt; 从 \(d^{(2)}\) 到最终的预测输出 \(y&apos;\)。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;3反向传播-backpropagation&quot;&gt;3.&lt;strong&gt;反向传播 (Backpropagation)&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;BP 是&lt;strong&gt;反向传播 (Backpropagation)&lt;/strong&gt; 的缩写，它是一种用于训练多层前馈神经网络的&lt;strong&gt;高效算法&lt;/strong&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;前向传播 (Forward Pass)：&lt;/strong&gt; 从 \(x\) 到 \(y&apos;\) 计算一次预测值。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;计算损失 (Loss)：&lt;/strong&gt; 根据 \(y&apos;\) 和真实标签 \(y\) 计算误差 \(L\)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;反向传播 (Backward Pass)：&lt;/strong&gt; 利用&lt;strong&gt;链式法则&lt;/strong&gt;（Chain Rule），将损失 \(L\) 的梯度（导数）从输出层 \(y&apos;\) 逐层向后传递，计算出每一层（\(W^{(2)}\) 和 \(W^{(1)}\)）的权重对总损失的贡献。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;权重更新：&lt;/strong&gt; 利用优化器（如 SGD 或 Adam），根据计算出的梯度来更新和调整所有权重矩阵 $W^{(1)}$ 和 $W^{(2)}$，使损失最小化。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;优化器（Optimizer）的主要任务是：&lt;strong&gt;根据反向传播计算出的梯度，智能地调整神经网络中的所有参数（权重 \(W\) 和偏置 \(b\)），使得损失函数 \(L\) 的值持续下降。&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;4为什么一定要激活函数&quot;&gt;4.为什么一定要激活函数&lt;/h1&gt;

&lt;p&gt;第一：防止深度丧失意义。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;没有激活函数的两层线性变换，真的没有一点意义吗？&lt;/p&gt;

  &lt;p&gt;有意义，减少了大量的参数&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251122192121082.png&quot; alt=&quot;image-20251122192121082&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;而且参数量的大幅减少，相当于引入了一种&lt;strong&gt;正则化 (Regularization)&lt;/strong&gt;。参数越少，模型的自由度越低，它越难在训练数据上“死记硬背”，从而有效降低了模型在小样本数据集上的&lt;strong&gt;过拟合 (Overfitting)&lt;/strong&gt; 风险。&lt;/p&gt;

  &lt;p&gt;参数越少，每次反向传播计算梯度和更新权重所需的时间就越短，从而&lt;strong&gt;加速了模型的训练过程&lt;/strong&gt;，提高了计算效率。&lt;/p&gt;

  &lt;p&gt;总结：虽然&lt;strong&gt;添加非线性激活函数&lt;/strong&gt;是赋予神经网络强大拟合能力的关键，但&lt;strong&gt;没有激活函数的两层线性变换&lt;/strong&gt;仍有其明确的价值，尤其是在工程实践中：它通过引入一个&lt;strong&gt;低维的中间层&lt;/strong&gt;，在不改变模型最终线性表达能力的前提下，极大地&lt;strong&gt;减少了参数量&lt;/strong&gt;，从而提高了模型的泛化能力和训练效率。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;第二，根据泰勒公式，线性变换(加权求和 \(z = Wx+b\))：是神经网络中&lt;strong&gt;没有激活函数&lt;/strong&gt;时的计算形式，二阶以后 全都为0，这意味着无论我们如何堆叠线性层，它始终只能表达 \(x\) 的一阶关系，无法逼近任何复杂的非线性函数。&lt;/p&gt;

&lt;p&gt;但是合适的非线性变换（常见的激活函数）  ：高阶导数都不为0，尽可能保留高阶项。&lt;strong&gt;非线性激活函数&lt;/strong&gt;（如 Sigmoid, Tanh, ReLU）的引入，使得函数可以逼近&lt;strong&gt;任意复杂的非线性函数&lt;/strong&gt;。这些高阶项赋予了神经网络拟合曲线、圆环、螺旋线等&lt;strong&gt;非线性决策边界&lt;/strong&gt;的能力（相当于做了高阶特征组合）。这正是&lt;strong&gt;万能逼近定理（Universal Approximation Theorem）&lt;/strong&gt;的数学基础：一个具有至少一个隐藏层和非线性激活函数的神经网络，理论上可以无限逼近任何连续函数。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;【补充】泰勒公式及其展开式：&lt;/p&gt;

\[f(x) = \frac{f(a)}{0!} + \frac{f&apos;(a)}{1!}(x-a) + \frac{f&apos;&apos;(a)}{2!}(x-a)^2 + \dots + \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x)\]

  &lt;p&gt;&lt;strong&gt;关键注释：&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;任何函数&lt;/strong&gt;都可以写成多项式的和。&lt;/li&gt;
    &lt;li&gt;\(a\) 是函数定义域上的任意一点，\(x\) 在 \(a\) 的附近。&lt;/li&gt;
    &lt;li&gt;\(R_n\) 是&lt;strong&gt;余项&lt;/strong&gt;；\(n\) 越大，\(R_n\) 越小；当 \(n \to \infty\) 时，\(R_n \to 0\)。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;为什么\(f(x)=e^x\)很少作为激活函数？&lt;/p&gt;

&lt;p&gt;虽然 \(e^x\) 是一个非线性函数，且在数学上性质优良，但在实际的神经网络训练中，它存在两个致命的缺陷，使其难以作为通用的激活函数。&lt;/p&gt;

&lt;p&gt;【问题1】梯度爆炸问题&lt;/p&gt;

&lt;p&gt;在神经网络的前向传播中，如果神经元的净输入 \(z\) 稍微偏大，例如 \(z = 10\)，那么 \(f(z) = e^{10} \approx 22000\)。&lt;/p&gt;

&lt;p&gt;在反向传播中，梯度会乘以激活函数的导数 \(f&apos;(z)\)。&lt;/p&gt;

&lt;p&gt;如果 \(z=10\)，导数 \(f&apos;(10) = e^{10} \approx 22000\)。&lt;/p&gt;

&lt;p&gt;由于指数函数增长得非常快，一旦网络中的某个权重或输入使净输入 \(z\) 稍大，反向传播的梯度就会被这个巨大的导数放大，导致梯度在层间传递时&lt;strong&gt;急剧增大&lt;/strong&gt;，即&lt;strong&gt;梯度爆炸&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;梯度爆炸会导致网络权重更新幅度过大，模型参数快速震荡，损失函数变为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NaN&lt;/code&gt;（Not a Number），使训练过程&lt;strong&gt;彻底崩溃&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;【问题2】输出非零均值&lt;/p&gt;

&lt;p&gt;\(f(x) = e^x\) 的值域是 \((0, +\infty)\)，函数值&lt;strong&gt;永远是正数&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;隐藏层的输出 \(a^{(l)}\)（即下一层的输入 \(x^{(l+1)}\)）总是正的。&lt;/p&gt;

&lt;p&gt;这意味着，下一层的净输入 \(z^{(l+1)} = W^{(l+1)} a^{(l)} + b^{(l+1)}\) 对梯度的影响是&lt;strong&gt;同向的&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;当所有输入 \(a^{(l)}\) 都是正数时，下一层 \(W^{(l+1)}\) 的梯度计算（\(\partial L / \partial W\)）要么&lt;strong&gt;全部是正的&lt;/strong&gt;，要么&lt;strong&gt;全部是负的&lt;/strong&gt;。这使得梯度更新方向被局限在特定的象限内，无法直接朝向最优解的方向。这会减慢&lt;strong&gt;梯度下降的效率&lt;/strong&gt;和&lt;strong&gt;收敛速度&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&quot;5softmax函数&quot;&gt;5.softmax函数&lt;/h1&gt;

&lt;p&gt;Softmax 函数的公式和计算原理&lt;/p&gt;

&lt;p&gt;将净输入 \(d^{(L)} + w0^{(L)}\) 向量转换为概率向量 \(y&apos;\)。&lt;/p&gt;

\[y&apos; = \text{Softmax}(d^{(L)} + w0^{(L)}) = \frac{1}{\sum_{m=1}^{C} e^{d_m^{(L)} + w0_m^{(L)}}} \begin{bmatrix} e^{d_1^{(L)} + w0_1^{(L)}} \\ \vdots \\ e^{d_C^{(L)} + w0_C^{(L)}} \end{bmatrix}\]

&lt;p&gt;Softmax 软分类（可求导），而max是一种硬分类（不可导）。&lt;/p&gt;

&lt;p&gt;在机器学习中，函数求导是一件很重要的事情—-&amp;gt;softmax比max更好&lt;/p&gt;

&lt;p&gt;可导性是进行&lt;strong&gt;反向传播&lt;/strong&gt;和&lt;strong&gt;梯度下降&lt;/strong&gt;的前提。Max 函数无法提供梯度来指导模型如何修正错误。&lt;/p&gt;

&lt;p&gt;Softmax 提供了“软”的决策边界，有利于平滑训练。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Softmax 在&lt;strong&gt;只有 2 个类别&lt;/strong&gt;时的简化版本，等价于 Sigmoid 函数。&lt;/p&gt;

&lt;p&gt;https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9906/&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;二分类 Softmax 严格等价于 Sigmoid 函数&lt;/strong&gt;，这证明了 Softmax 是 Sigmoid 在多分类场景下的&lt;strong&gt;泛化&lt;/strong&gt;。&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/22/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9911/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/22/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9911/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙10】朴素贝叶斯模型：简单背后蕴含的有效 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251121165449435.png&quot; alt=&quot;image-20251121165449435&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1先验概率和后验概率&quot;&gt;1.先验概率和后验概率&lt;/h1&gt;

&lt;p&gt;先验概率是根据经验得到的，比如看摊位买的西瓜，大致认为60%的概率西瓜是好的。先验概率不需要样本数据，不受任何条件的影响。不根据其他，就根据常识大致判断。&lt;/p&gt;

&lt;p&gt;而后验概率就类似于看瓜蒂脱落与否判断西瓜是否是好的。&lt;/p&gt;

&lt;p&gt;计算后验概率就是朴素贝叶斯最核心的一步。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;联合概率&lt;/strong&gt;是几个事件同时发生的概率。例如P(瓜熟， 瓜蒂脱落)就是一个联合概率&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P (瓜熟，瓜蒂脱落) =P (瓜熟|瓜蒂脱落) *P (瓜蒂脱落)&lt;/code&gt;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(瓜熟，瓜蒂脱落) =P(瓜蒂脱落|瓜熟) *P(瓜熟)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;但是要求的是$$P(\text{瓜熟&lt;/td&gt;
      &lt;td&gt;瓜蒂脱落})$$：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$P(\text{瓜熟&lt;/td&gt;
      &lt;td&gt;瓜蒂脱落}) \times P(\text{瓜蒂脱落}) = P(\text{瓜蒂脱落&lt;/td&gt;
      &lt;td&gt;瓜熟}) \times P(\text{瓜熟})$$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$P(\text{瓜熟&lt;/td&gt;
      &lt;td&gt;瓜蒂脱落}) = \frac{P(\text{瓜蒂脱落&lt;/td&gt;
      &lt;td&gt;瓜熟}) \times P(\text{瓜熟})}{P(\text{瓜蒂脱落})}$$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;因为$$P(\text{瓜蒂脱落&lt;/td&gt;
      &lt;td&gt;瓜熟})\(和\)P(\text{瓜熟})\(是已知的，所以只需要求出\)P(\text{瓜蒂脱落})$$即可。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;而&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(瓜蒂脱落) = P(瓜蒂脱落|瓜熟) * P(瓜熟) + P(瓜蒂脱落|瓜生) * P(瓜生)&lt;/code&gt;(全概率公式)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;若已知P(瓜熟)=0.6，P(瓜蒂脱落&lt;/td&gt;
      &lt;td&gt;瓜生)=0.，P(瓜生)=0.4，P(瓜蒂脱落&lt;/td&gt;
      &lt;td&gt;瓜熟)=0.8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;则&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(瓜熟/瓜蒂脱落) = (0.8*0.6) / (0.8*0.6+0.4*0.4) =0.75&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如果特征很多（各个特征之间是独立的），那么可以这么计算：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251121164242437.png&quot; alt=&quot;image-20251121164242437&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以上就是朴素贝叶斯算法的一个应用。&lt;/p&gt;

&lt;h1 id=&quot;2朴素贝叶斯算法&quot;&gt;2.朴素贝叶斯算法&lt;/h1&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;朴素贝叶斯（Naive Bayes）是一种基于&lt;strong&gt;贝叶斯定理&lt;/strong&gt;的&lt;strong&gt;生成模型&lt;/strong&gt;。它通过计算后验概率 $$P(\text{类别}&lt;/td&gt;
      &lt;td&gt;\text{特征})$$ 来进行分类。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;贝叶斯公式是其理论基础：&lt;/p&gt;

\[P(\text{类别}| \text{特征}) = \frac{P(\text{特征}| \text{类别}) \times P(\text{类别})}{P(\text{特征})}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;\(P(\text{类别})\) (先验概率):&lt;/strong&gt; 这是在观察任何特征之前，我们对某一类别概率的预估（如您所说，比如上面例子中\(P(\text{瓜熟})=0.6\)）。&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;**$$P(\text{特征}&lt;/td&gt;
          &lt;td&gt;\text{类别})\((似然项):** 在类别已知的情况下，观察到特征的概率（如\)P(\text{瓜蒂脱落}&lt;/td&gt;
          &lt;td&gt;\text{瓜熟})=0.8$$）。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(P(\text{特征})\) (证据项):&lt;/strong&gt; 观察到特征本身的概率，通过&lt;strong&gt;全概率公式&lt;/strong&gt;计算得出（如 \(P(\text{瓜蒂脱落})\)）。&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;**$$P(\text{类别}&lt;/td&gt;
          &lt;td&gt;\text{特征})\((后验概率):** 这是最终要求的，在特征已知的情况下，类别发生的概率（如\)P(\text{瓜熟}&lt;/td&gt;
          &lt;td&gt;\text{瓜蒂脱落})=0.75$$）。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;21朴素的含义特征独立性假设&quot;&gt;2.1“朴素”的含义：特征独立性假设&lt;/h2&gt;

&lt;p&gt;在实际的机器学习问题中，特征 \(x\) 往往不止一个，而是多个特征向量 \(x = (x_1, x_2, \dots, x_n)\)。&lt;/p&gt;

&lt;p&gt;朴素贝叶斯名字中的“朴素”指的是它做了一个&lt;strong&gt;强假设&lt;/strong&gt;：&lt;strong&gt;所有特征之间相互独立&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;在类别 \(y\) 已知的情况下，所有特征的联合概率等于它们各自概率的乘积。&lt;/p&gt;

\[P(x_1, x_2, \dots, x_n | y) = P(x_1|y) \times P(x_2|y) \times \dots \times P(x_n|y)\]

&lt;p&gt;这个“朴素”的假设大大简化了计算，使得模型可以在特征数量非常大时依然高效运行，但这也是它&lt;strong&gt;牺牲模型准确性&lt;/strong&gt;的地方（因为现实中特征很少是完全独立的）。&lt;/p&gt;

&lt;h2 id=&quot;22朴素贝叶斯与逻辑回归线性回归的对比&quot;&gt;2.2朴素贝叶斯与逻辑回归、线性回归的对比&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;特性&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;朴素贝叶斯 (Naive Bayes)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;逻辑回归 (Logistic Regression)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;线性回归 (Linear Regression)&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;模型类型&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;生成模型&lt;/strong&gt; (Generative Model)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;判别模型&lt;/strong&gt; (Discriminative Model)&lt;/td&gt;
      &lt;td&gt;判别模型 (Discriminative Model)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;解决问题&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;分类&lt;/strong&gt; (Classification)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;分类&lt;/strong&gt; (Classification)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;回归&lt;/strong&gt; (Regression)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;目标输出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;离散的类别标签 (如“好瓜”/“坏瓜”)&lt;/td&gt;
      &lt;td&gt;类别概率&lt;/td&gt;
      &lt;td&gt;连续值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;训练速度极快；对缺失数据不敏感；对数据量较小的场景有效。&lt;/td&gt;
      &lt;td&gt;输出是概率值，可解释性强；决策边界是线性的；理论基础坚实。&lt;/td&gt;
      &lt;td&gt;模型简单且可解释性强；计算高效。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;因为强大的&lt;strong&gt;特征独立性&lt;/strong&gt;假设，但是独立性假设往往不成立，可能牺牲准确率。&lt;/td&gt;
      &lt;td&gt;易受异常值影响；只能解决线性可分或近似线性的问题。&lt;/td&gt;
      &lt;td&gt;只能拟合线性关系；对噪声和多重共线性敏感。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;3缺失值的处理&quot;&gt;3.缺失值的处理&lt;/h1&gt;

&lt;p&gt;比如婚恋相亲网站，每个用户信息填写的完整程度不同（用户填写意愿、隐私考量或信息复杂性）。在推荐/匹配/预测场景（机器学习）中，如果碰到缺失值如何处理？&lt;/p&gt;

&lt;p&gt;处理缺失值的方法大致可以分为三类：&lt;strong&gt;删除（Deletion）&lt;/strong&gt;、&lt;strong&gt;插值/填充（Imputation）&lt;/strong&gt;、和&lt;strong&gt;建模（Modeling）&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;31删除&quot;&gt;3.1删除&lt;/h2&gt;

&lt;p&gt;当缺失量极少或缺失模式很随机时可以考虑。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;方法&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;描述&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;风险/缺点&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;1. 行删除&lt;/strong&gt; (Listwise Deletion)&lt;/td&gt;
      &lt;td&gt;直接删除任何包含缺失值的样本（用户记录）。&lt;/td&gt;
      &lt;td&gt;缺失值占总数据量比例&lt;strong&gt;极低&lt;/strong&gt;（如 &amp;lt; 1%）且缺失是&lt;strong&gt;完全随机&lt;/strong&gt;的。&lt;/td&gt;
      &lt;td&gt;如果缺失值较多，会导致数据量急剧减少，损失大量信息，引入偏差。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;2. 列删除&lt;/strong&gt; (Feature Deletion)&lt;/td&gt;
      &lt;td&gt;如果某个特征（如“年收入”）的缺失率&lt;strong&gt;极高&lt;/strong&gt;（如 &amp;gt; 90%），则直接删除该特征。&lt;/td&gt;
      &lt;td&gt;该特征对核心业务价值不大，且缺失率高到无法有效填充。&lt;/td&gt;
      &lt;td&gt;可能丢失对模型有潜在价值的信息。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;32填充&quot;&gt;3.2填充&lt;/h2&gt;

&lt;p&gt;用某种值替换缺失值，这是最常用的方法。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;方法&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;描述&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;风险/缺点&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;3. 均值/中位数/众数填充&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;数值特征（如身高、年龄）：用&lt;strong&gt;均值&lt;/strong&gt;或&lt;strong&gt;中位数&lt;/strong&gt;填充。类别特征（如学历、信仰）：用&lt;strong&gt;众数&lt;/strong&gt;填充。&lt;/td&gt;
      &lt;td&gt;缺失值比例适中，或缺失是随机的。&lt;/td&gt;
      &lt;td&gt;这种方法会减小特征的方差，并引入偏差（特别是均值填充），&lt;strong&gt;使数据分布失真&lt;/strong&gt;。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;4. 哨兵值填充&lt;/strong&gt; (Sentinel/Flag Value)&lt;/td&gt;
      &lt;td&gt;用一个特定的值（如 -999 或 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Unknown&lt;/code&gt;）来标记缺失，让模型知道这个值是缺失的。&lt;/td&gt;
      &lt;td&gt;特别适用于类别特征，或当缺失本身可能包含信息（e.g., &lt;strong&gt;“不愿填写”本身就是一种信息&lt;/strong&gt;）。&lt;/td&gt;
      &lt;td&gt;模型可能会错误地将这个哨兵值解释为一个“正常”的极大或极小值。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;5. 基于模型填充&lt;/strong&gt; (Model-based Imputation)&lt;/th&gt;
      &lt;th&gt;使用其他非缺失特征作为输入，训练一个模型（如线性回归、决策树、KNN 等）来预测缺失值。&lt;/th&gt;
      &lt;th&gt;缺失值数量较大，且特征之间有较强相关性。&lt;/th&gt;
      &lt;th&gt;填充值更接近真实数据分布，精度高。常用的方法有 &lt;strong&gt;MICE&lt;/strong&gt;（多重插值）和 &lt;strong&gt;KNN Imputation&lt;/strong&gt;。&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;6. 前后值填充&lt;/strong&gt; (Locally based Imputation)&lt;/td&gt;
      &lt;td&gt;对于时间序列数据（不适用于相亲网站的静态数据），用前一个或后一个观测值填充。&lt;/td&gt;
      &lt;td&gt;数据存在时间或空间上的连续性。&lt;/td&gt;
      &lt;td&gt;不适用于独立的个人信息记录。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;33建模&quot;&gt;3.3建模&lt;/h2&gt;

&lt;p&gt;不直接改变数据，而是修改模型或增加信息。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;方法&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;描述&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;7. 缺失指示变量&lt;/strong&gt; (Missing Indicator)&lt;/td&gt;
      &lt;td&gt;对于每个有缺失值的特征X，增加一个二元指示变量I。如果 X缺失，则I=1，否则 I=0。然后用均值/中位数等填充 X。&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;强烈推荐&lt;/strong&gt;。假设缺失机制是非随机的（MNAR），即缺失的原因本身带有信息。&lt;/td&gt;
      &lt;td&gt;捕捉了“不愿填写”或“信息不存在”本身所包含的信号，保留了原始数据结构。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;8. 特殊模型&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;使用能够&lt;strong&gt;自动处理缺失值&lt;/strong&gt;的模型，例如基于树的模型（如 XGBoost, LightGBM, CatBoost）。&lt;/td&gt;
      &lt;td&gt;当缺失模式复杂、缺失量较大时。&lt;/td&gt;
      &lt;td&gt;模型在节点划分时可以自动处理或学习缺失值，无需手动填充。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;34案例&quot;&gt;3.4案例&lt;/h2&gt;

&lt;p&gt;在婚恋相亲网站的场景中，&lt;strong&gt;缺失值往往不是随机的，而是带有明确的意图或隐私偏好&lt;/strong&gt;（例如，高收入者可能不填收入，低收入者也可能不填收入）。因此，推荐结合使用以下方法：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;指示变量 + 填充 (方法 7 + 4)：&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;数值特征&lt;/strong&gt;（如年收入、身高）：用中位数填充，并新增一个“年收入_缺失指示”的二元特征。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;类别特征&lt;/strong&gt;（如家庭状况）：用“&lt;strong&gt;Unknown&lt;/strong&gt;”或“&lt;strong&gt;未公开&lt;/strong&gt;”作为单独的类别进行填充。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;使用树模型 (方法 8)：&lt;/strong&gt; 优先使用 XGBoost 或 LightGBM 等集成树模型进行预测/匹配建模，因为它们对缺失值的鲁棒性更好，并且能自动学习缺失值模式。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;领域知识优先：&lt;/strong&gt; 某些关键特征（如“性别”、“年龄”、“城市”）如果缺失，可能直接导致匹配失败，应要求用户必须填写，或者直接删除该记录。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;4线性回归和逻辑回归极高的耦合度&quot;&gt;4.线性回归和逻辑回归极高的耦合度&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;线性回归（Linear Regression）和逻辑回归（Logistic Regression）在数学和结构上确实存在着极高的耦合度&lt;/strong&gt;。它们常常被一起教授，正是因为它们共享了“广义线性模型”的核心思想。&lt;/p&gt;

&lt;p&gt;无论是在线性回归还是逻辑回归中，模型的第一步都是将输入特征 \(\mathbf{x}\) 进行&lt;strong&gt;线性组合&lt;/strong&gt;（加权求和），得到一个预测值 \(z\)：&lt;/p&gt;

\[z = \mathbf{W}^T \mathbf{x} + b\]

&lt;p&gt;两者都假设输入特征对目标变量的影响是&lt;strong&gt;线性的、可叠加的&lt;/strong&gt;。—-&amp;gt;耦合度很高。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;朴素贝叶斯&lt;/strong&gt;明确且&lt;strong&gt;强制&lt;/strong&gt;地要求&lt;strong&gt;特征之间相互独立&lt;/strong&gt;。这是其“朴素”的来源。&lt;/p&gt;

&lt;p&gt;而&lt;strong&gt;逻辑回归（LR）&lt;/strong&gt;则不同：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;LR&lt;/strong&gt; 是一种&lt;strong&gt;判别模型&lt;/strong&gt;，它直接建模 $$P(y&lt;/td&gt;
          &lt;td&gt;\mathbf{x})\(，并不对输入特征\)\mathbf{x}\(的**联合概率分布**\)P(x_1, x_2, \dots, x_n)$$ 做任何假设。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;因此，&lt;strong&gt;LR 本身并没有假设特征独立性&lt;/strong&gt;。它只关心 \(\mathbf{W}^T \mathbf{x}\) 这个线性组合，模型参数 $\mathbf{W}$ 会自动学习特征之间的相关性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LR 的耦合在于它&lt;strong&gt;强制将特征的贡献以线性和叠加的方式聚合&lt;/strong&gt;，这限制了它无法捕捉特征之间的复杂&lt;strong&gt;非线性交互作用&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;另外，线性部分 \(\mathbf{W}^T \mathbf{x} + b\) 过于紧密地依赖于&lt;strong&gt;特征间的关系&lt;/strong&gt;，从而导致： 如果 \(x_i\) 和 \(x_j\) 强相关，模型很难唯一确定它们各自的权重 \(w_i\) 和 \(w_j\)，从而让模型结构变得脆弱。(解决办法：利用&lt;strong&gt;正则化&lt;/strong&gt;（如 L1 或 L2）来解耦 LR 模型中特征之间的相关性，从而处理多重共线性问题)&lt;/p&gt;

&lt;p&gt;显然，特征之间独立的可能性一般不大。所以朴素贝叶斯使用场景极其有限。&lt;/p&gt;

&lt;p&gt;但是朴素贝叶斯对于  数据缺失，又想要数据解耦  的场景又很友好。&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;计算&lt;strong&gt;后验概率&lt;/strong&gt; $$P(y_k=1&lt;/td&gt;
      &lt;td&gt;\mathbf{x})$$：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[P(y_k=1|\mathbf{x}) = \frac{P(y_k=1) \cdot P(\mathbf{x}|y_k=1)}{P(\mathbf{x})}\]

&lt;p&gt;从通用贝叶斯公式到&lt;strong&gt;朴素贝叶斯分类器&lt;/strong&gt;的关键一步。&lt;/p&gt;

\[P(y_k=1|\mathbf{x}) = \frac{P(y_k=1) \cdot P(x_1|y_k=1) \cdot P(x_2|y_k=1) \cdot P(x_3|y_k=1)}{P(x_1, x_2, x_3)}\]

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;这个推导的关键在于分子中对**似然项 $$P(\mathbf{x}&lt;/td&gt;
      &lt;td&gt;y_k=1)$$ 的分解**：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[P(\mathbf{x}|y_k=1) = P(x_1, x_2, x_3 | y_k=1)\]

&lt;p&gt;根据朴素贝叶斯的&lt;strong&gt;核心假设（朴素假设）&lt;/strong&gt;：&lt;strong&gt;在类别 \(y\) 确定的条件下，所有特征 \(x_i\) 之间相互独立。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;因此，联合条件概率可以分解为各个特征条件概率的乘积：&lt;/p&gt;

\[P(x_1, x_2, x_3 | y_k=1) \approx P(x_1|y_k=1) \cdot P(x_2|y_k=1) \cdot P(x_3|y_k=1)\]

&lt;hr /&gt;

&lt;p&gt;所以，朴素贝叶斯就是搞统计，且样本少的时候，误差就很大了，样本可能存在很强的偶然性，此时做统计就不准确了。&lt;/p&gt;

&lt;p&gt;线性回归抗冗余强，但是朴素贝叶斯抗冗余不强。&lt;/p&gt;

&lt;h1 id=&quot;5拉普拉斯平滑&quot;&gt;5.拉普拉斯平滑&lt;/h1&gt;

&lt;p&gt;在&lt;strong&gt;朴素贝叶斯（Naive Bayes）&lt;/strong&gt;算法中处理&lt;strong&gt;小样本（或零概率）问题&lt;/strong&gt;的一种经典且有效的解决方案：&lt;strong&gt;拉普拉斯平滑（Laplace Smoothing）&lt;/strong&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;特征 \(x_1=1\) 的条件概率：&lt;/p&gt;

\[P(x_1=1 | y_1=1) = \frac{\text{Count}(x_1=1, y_1=1) + \frac{n}{2}}{\text{Count}(y_1) + n}\]

&lt;p&gt;特征 \(x_1=0\) 的条件概率：&lt;/p&gt;

\[P(x_1=0 | y_1=1) = \frac{\text{Count}(x_1=0, y_1=1) + \frac{n}{2}}{\text{Count}(y_1) + n}\]

&lt;p&gt;&lt;strong&gt;这个n的取值&lt;/strong&gt;：根据人为干预决定。也是根据人的经验得出的。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;拉普拉斯平滑&lt;/strong&gt;通过向分子和分母添加一个&lt;strong&gt;虚拟计数&lt;/strong&gt;（Pseudo-count）\(\alpha\) 来调整概率估计。&lt;/p&gt;

&lt;p&gt;一般的拉普拉斯平滑公式如下：&lt;/p&gt;

\[P_{\text{smooth}}(x_i|y_k) = \frac{\text{Count}(x_i, y_k) + \alpha}{\text{Count}(y_k) + \alpha \cdot D}\]

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\alpha\)：平滑参数（通常取 1，即“加一平滑”）。&lt;/li&gt;
  &lt;li&gt;\(D\)：特征 \(x_i\) 可能的取值数量（即&lt;strong&gt;维度&lt;/strong&gt;）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;它本质上是&lt;strong&gt;将训练集数据与一个均匀分布进行了加权平均&lt;/strong&gt;。当样本量 \(\text{Count}(y_k)\) 很小时，添加的 \(\alpha \cdot D\) 影响较大，&lt;strong&gt;防止极端概率&lt;/strong&gt;；当样本量很大时，平滑项的影响微乎其微。&lt;/p&gt;

&lt;p&gt;通过这种方式，拉普拉斯平滑有效地解决了小样本数据中由于偶然性导致的零概率问题，使朴素贝叶斯模型在训练集不完备的情况下依然能够给出合理的概率估计。&lt;/p&gt;

&lt;h1 id=&quot;6信息如何量化&quot;&gt;6.信息如何量化&lt;/h1&gt;

&lt;p&gt;1.明天太阳从东边升起&lt;/p&gt;

&lt;p&gt;2.明天下雨&lt;/p&gt;

&lt;p&gt;显然2的信息量更大，因为1发生的概率是1，而2发生的概率显然小于1&lt;/p&gt;

&lt;h2 id=&quot;61信息量--自信息&quot;&gt;6.1信息量 / 自信息&lt;/h2&gt;

&lt;p&gt;信息论认为，一个事件所包含的&lt;strong&gt;信息量&lt;/strong&gt;与其发生的&lt;strong&gt;概率&lt;/strong&gt;成反比。事件发生的概率越低，它所携带的信息量就越大。&lt;/p&gt;

&lt;p&gt;信息论中使用&lt;strong&gt;对数&lt;/strong&gt;来量化信息量 \(I(x)\)，因为信息量具有可加性（独立事件的总信息量等于各自信息量之和），而概率具有可乘性（独立事件的总概率等于各自概率之积）。对数正好能将乘法转化为加法。&lt;/p&gt;

&lt;p&gt;对于一个事件 \(x\)，其&lt;strong&gt;自信息&lt;/strong&gt; \(I(x)\) 的定义为：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;下面公式，香农严格证明过。&lt;/p&gt;
&lt;/blockquote&gt;

\[I(x) = - \log_b P(x)\]

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(P(x)\) 是事件 \(x\) 发生的概率。&lt;/li&gt;
  &lt;li&gt;\(b\) 是对数的底数，它决定了信息量的单位：
    &lt;ul&gt;
      &lt;li&gt;当 \(b=2\) 时，信息量的单位是&lt;strong&gt;比特 (bits)&lt;/strong&gt;。&lt;/li&gt;
      &lt;li&gt;当 \(b=e\) 时，信息量的单位是&lt;strong&gt;奈特 (nats)&lt;/strong&gt;。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过这个公式，可以看到：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果 \(P(x) = 1\)，则 \(I(x) = -\log_b(1) = 0\)。&lt;/li&gt;
  &lt;li&gt;如果 \(P(x) \to 0\)，则 \(I(x) \to \infty\)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个概念是后续&lt;strong&gt;熵 (Entropy)&lt;/strong&gt;、&lt;strong&gt;交叉熵 (Cross-Entropy)&lt;/strong&gt; 和&lt;strong&gt;信息增益 (Information Gain)&lt;/strong&gt; 等机器学习中核心概念的基石。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;比如推荐系统，喜欢推荐“新奇特”的信息一样，其信息量很大，发生的概率很小。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;62信息熵&quot;&gt;6.2信息熵&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;说白了，信息熵是信息量的期望。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;信息熵是用来量化一个随机变量或一个系统所包含的平均不确定性或信息量。&lt;/strong&gt;衡量在观察到 \(X\) 的任何结果之前，我们需要多少信息才能确定其结果。&lt;/p&gt;

&lt;p&gt;在机器学习中，熵常用于决策树（信息增益）、模型评估（交叉熵损失）等方面。&lt;/p&gt;

&lt;p&gt;熵可以被视为&lt;strong&gt;随机变量 \(X\) 所有可能结果的自信息期望值&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;对于一个离散随机变量 \(X\)，它有 \(n\) 种可能的取值 \(\{x_1, x_2, \dots, x_n\}\)，每种取值的概率是 \(P(x_i)\)，其熵 \(H(X)\) 定义为：&lt;/p&gt;

\[H(X) = E[I(X)] = \sum_{i=1}^{n} P(x_i) \cdot I(x_i)\]

&lt;p&gt;将自信息 \(I(x_i) = -\log_2 P(x_i)\) 代入，得到最终的熵公式（通常以比特为单位，故取底数为 2）：&lt;/p&gt;

\[H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i)\]

&lt;p&gt;&lt;strong&gt;单位：&lt;/strong&gt; 比特（bits）。熵的数值代表了要对该随机变量进行编码所需的&lt;strong&gt;平均最小比特数&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;熵值越高，不确定性越大。&lt;/strong&gt; 样本的分布越均匀，熵越大。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;熵值越低，不确定性越小。&lt;/strong&gt; 样本的分布越集中，熵越小。&lt;/p&gt;

&lt;p&gt;熵给出了对随机变量编码所需的&lt;strong&gt;理论平均最短长度&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;例如，如果一个系统的信息熵是 3 比特，那么在平均意义上，你需要 3 个二元问题（是/否）才能确定该系统的状态。&lt;/p&gt;

&lt;p&gt;熵是计算&lt;strong&gt;信息增益&lt;/strong&gt;的基础。在每次分裂时，决策树选择能最大程度&lt;strong&gt;降低熵&lt;/strong&gt;（即最大程度减少不确定性）的特征，从而达到最佳分类效果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;交叉熵（Cross-Entropy）&lt;/strong&gt;和&lt;strong&gt;相对熵（KL 散度）&lt;/strong&gt;都是基于信息熵的概念发展而来，用于衡量模型预测的概率分布与真实概率分布之间的差异。&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/22/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9910/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/22/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9910/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙09】逻辑回归5让学习更高效，数值优化和一只看不见的手 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;1特征缩放-z-score标准化归一化&quot;&gt;1.特征缩放-Z-score标准化/归一化&lt;/h1&gt;

&lt;p&gt;回顾逻辑回归的损失函数，及其导数：&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;损失函数是BCE：$$\mathcal{L}_{\text{BCE}}(P&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Q) = - \sum_{k \in {0, 1}} P(Y=k) \log Q(Y=k) = - [y \log \hat{y} + (1 - y) \log (1 - \hat{y})]=\ \sum_{i=1}^{n} \left[ y_i \log \frac{y_i}{f_i} + (1 - y_i) \log \frac{1 - y_i}{1 - f_i} \right]$$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;导数：损失函数 $$\mathcal{L}_{\text{BCE}}(P&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Q)\(对权重向量\)\mathbf{w}\(中任一分量\)w_j\(的偏导数\)\frac{\partial L_i}{\partial w_j}$$：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[\frac{\partial L_i}{\partial w_j} = - (y_i - \hat{y}_i) x_{i,j} = (\hat{y}_i - y_i) x_{i,j}\]

&lt;p&gt;总体损失梯度如下：&lt;/p&gt;

&lt;p&gt;对于整个训练集（所有 \(N\) 个样本），总损失 \(L(\mathbf{w}) = \frac{1}{N} \sum_{i=1}^{N} L_i(\mathbf{w})\) 对 \(w_j\) 的梯度为：&lt;/p&gt;

\[\frac{\partial L}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial L_i}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}\]

&lt;hr /&gt;

&lt;p&gt;梯度下降：对单个权重 \(w_j\) 的更新:&lt;/p&gt;

\[w_{j, \text{new}} = w_{j, \text{old}} - \alpha \cdot \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}\]

&lt;p&gt;如果特征都是大于0的取值，根据梯度下降的式子，参数\(w\)只会越来越小。反之如果特征都是小于0的数值，参数只会越来越大。如果用4个象限的等高线描述梯度下降的过程，那么梯度只会往第一象限/第三象限走。此时就会使得收敛过程发生巨大的震荡。&lt;/p&gt;

&lt;p&gt;这些都属于&lt;strong&gt;未归一化/未标准化特征&lt;/strong&gt;在梯度下降优化过程中的&lt;strong&gt;效率和稳定性&lt;/strong&gt;问题。&lt;strong&gt;特征尺度的不一致或不平衡，会导致损失函数的等高线变得极度扁平狭长，使得梯度下降路径低效且震荡。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;解决这个问题的最优方案是使用 &lt;strong&gt;特征缩放（Feature Scaling）&lt;/strong&gt;，特别是 &lt;strong&gt;Z-Score 归一化（标准化）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;每个特征 \(x_j\) 转换为标准正态分布，使其均值 \(\mu=0\)，标准差 \(\sigma=1\)。&lt;/p&gt;

\[x_{\text{new}} = \frac{x - \mu}{\sigma}\]

&lt;p&gt;这是最推荐的解决方案，尤其适用于基于梯度下降的线性模型（如逻辑回归）和神经网络。&lt;/p&gt;

&lt;p&gt;当然也可以用Min-Max 归一化 (Normalization)，但是Min-Max 归一化对异常值很敏感。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251121003720143.png&quot; alt=&quot;image-20251121003720143&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2逻辑回归的总结&quot;&gt;2.逻辑回归的总结&lt;/h1&gt;

&lt;p&gt;1.sigmoid伯努利分布   特殊的softmax（多项分布）&lt;/p&gt;

&lt;p&gt;2.BCE  不用MSE&lt;/p&gt;

&lt;p&gt;3.L1/L2正则&lt;/p&gt;

&lt;p&gt;4.归一化&lt;/p&gt;

&lt;p&gt;5.指标  ： 准确率/召回率/查准率/AUC-ROC&lt;/p&gt;

&lt;h1 id=&quot;3先验条件&quot;&gt;3.先验条件&lt;/h1&gt;

&lt;p&gt;一元高斯分布（Univariate Gaussian Distribution），也称为&lt;strong&gt;一维正态分布&lt;/strong&gt;，其概率密度函数（Probability Density Function, PDF）为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251121004636434.png&quot; alt=&quot;image-20251121004636434&quot; style=&quot;zoom: 33%;&quot; /&gt;&lt;/p&gt;

\[f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}\]

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(x\) 是随机变量的取值。&lt;/li&gt;
  &lt;li&gt;\(\mu\) 是分布的&lt;strong&gt;均值&lt;/strong&gt;（Mean），决定了分布的中心位置。&lt;/li&gt;
  &lt;li&gt;\(\sigma^2\) 是分布的&lt;strong&gt;方差&lt;/strong&gt;（Variance），决定了分布的形状，即数据的分散程度。\(\sigma\)越大，越分散。&lt;/li&gt;
  &lt;li&gt;\(\sigma\) 是标准差（Standard Deviation）。&lt;/li&gt;
  &lt;li&gt;\(\frac{1}{\sqrt{2\pi\sigma^2}}\) 是归一化常数，确保概率密度函数在整个实数域上的积分为 $1$。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;相对论的成功在于它基于最少的公理，得到了最大的推论，构建了一个自洽的宇宙模型。&lt;/p&gt;

&lt;p&gt;即：假设（公理）—-&amp;gt;推论&lt;/p&gt;

&lt;p&gt;机器学习中，这个假设（公理）就是：&lt;strong&gt;对于某一类来说，分布符合正态分布&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;其实严格来说，判别模型（逻辑回归/softmax回归等）应该符合的是伯努利分布/多项分布（统称为指数族分布）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;4贝叶斯公式与高斯分布的结合&quot;&gt;4.贝叶斯公式与高斯分布的结合&lt;/h1&gt;

&lt;p&gt;贝叶斯公式是所有生成模型的理论基础。&lt;/p&gt;

\[P(y|x) = \frac{P(x|y) \cdot P(y)}{P(x)} \quad \text{（后验概率）}\]

&lt;p&gt;例如身高判断 \(x=170\) 是男还是女&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;**$$P(y=1&lt;/td&gt;
      &lt;td&gt;x)\(：** 在已知身高\)x$$ 的情况下，是&lt;strong&gt;男&lt;/strong&gt;的概率。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;**$$P(y=0&lt;/td&gt;
      &lt;td&gt;x)\(：** 在已知身高\)x$$ 的情况下，是&lt;strong&gt;女&lt;/strong&gt;的概率。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;由于 \(P(x)\) 对两个类别的比较是相同的，我们只需要比较&lt;strong&gt;分子&lt;/strong&gt;：&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;选择$$\quad y = \arg\max_{y \in {\text{男}, \text{女}}} \left{ P(x=170&lt;/td&gt;
      &lt;td&gt;y) \cdot P(y) \right}$$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;还有已知条件：&lt;/p&gt;

&lt;p&gt;\(P(y=\text{男}) + P(y=\text{女}) = 1\)，这是先验概率 (Prior Probability)。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;在观察到特定数据 \(x\)（例如，身高 \(x=170\)）之后，其对应的后验概率之和也必须为 1：$$P(y=\text{男}&lt;/td&gt;
      &lt;td&gt;x=170) + P(y=\text{女}&lt;/td&gt;
      &lt;td&gt;x=170) = 1$$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;p&gt;假设特征 \(x\) 在每个类别下服从&lt;strong&gt;正态分布&lt;/strong&gt;（高斯分布），另外假设&lt;strong&gt;方差 \(\sigma^2\) 相等&lt;/strong&gt;，即两个类别（\(y=1\) 和 \(y=0\)）的方差相等，即 \(\sigma_1^2 = \sigma_0^2 = \sigma^2\)。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;类别 \(y=1\) 的分布：$$P(x&lt;/td&gt;
          &lt;td&gt;y=1) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu_1)^2}{2\sigma^2}}$$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;类别 \(y=0\) 的分布：$$P(x&lt;/td&gt;
          &lt;td&gt;y=0) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu_0)^2}{2\sigma^2}}$$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;计算两个类别似然项的比值 $$\frac{P(x&lt;/td&gt;
      &lt;td&gt;y=1)}{P(x&lt;/td&gt;
      &lt;td&gt;y=0)}$$：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[\frac{P(x|y=1)}{P(x|y=0)} = \frac{\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu_1)^2}{2\sigma^2}}}{\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu_0)^2}{2\sigma^2}}}\]

&lt;p&gt;由于 \(\sigma^2\) 相等，系数 \(\frac{1}{\sqrt{2\pi\sigma^2}}\) 被抵消：&lt;/p&gt;

\[\frac{P(x|y=1)}{P(x|y=0)} = e^{-\frac{1}{2\sigma^2} \left[ (x - \mu_1)^2 - (x - \mu_0)^2 \right]}\]

&lt;p&gt;化简指数上的项 \((x - \mu_1)^2 - (x - \mu_0)^2\) 的负值：&lt;/p&gt;

\[\begin{aligned} \text{指数项} &amp;amp;= -(x - \mu_1)^2 + (x - \mu_0)^2 \\ &amp;amp;= - (x^2 - 2x\mu_1 + \mu_1^2) + (x^2 - 2x\mu_0 + \mu_0^2) \\ &amp;amp;= -x^2 + 2x\mu_1 - \mu_1^2 + x^2 - 2x\mu_0 + \mu_0^2 \\ &amp;amp;=  (2x\mu_1 - 2x\mu_0) + (\mu_0^2 - \mu_1^2) \\ &amp;amp;= 2x(\mu_1 - \mu_0) + (\mu_0^2 - \mu_1^2) \end{aligned}\]

&lt;p&gt;将化简后的结果代回似然比的指数：&lt;/p&gt;

\[\log \left( \frac{P(x|y=1)}{P(x|y=0)} \right) = \frac{1}{2\sigma^2} \left[ 2x(\mu_1 - \mu_0) + (\mu_0^2 - \mu_1^2) \right]\]

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;最终目标是后验概率 $$P(y=1&lt;/td&gt;
      &lt;td&gt;x)$$。在贝叶斯公式中：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[\frac{P(y=1|x)}{P(y=0|x)} = \frac{P(x|y=1) P(y=1)}{P(x|y=0) P(y=0)} = \left( \frac{P(x|y=1)}{P(x|y=0)} \right) \cdot \left( \frac{P(y=1)}{P(y=0)} \right)\]

&lt;p&gt;取对数，得到&lt;strong&gt;对数几率 (Log-Odds)&lt;/strong&gt;：&lt;/p&gt;

\[\log \left( \frac{P(y=1|x)}{P(y=0|x)} \right) = \log \left( \frac{P(x|y=1)}{P(x|y=0)} \right) + \log \left( \frac{P(y=1)}{P(y=0)} \right)\]

&lt;p&gt;将结果代入：&lt;/p&gt;

\[\log \left( \frac{P(y=1|x)}{P(y=0|x)} \right) = \underbrace{\left[ \frac{1}{\sigma^2} (\mu_1 - \mu_0) \right]}_{W} x + \underbrace{\left[ \frac{1}{2\sigma^2} (\mu_0^2 - \mu_1^2) + \log \left( \frac{P(y=1)}{P(y=0)} \right) \right]}_{W_0}\]

&lt;p&gt;模型的对数几率被表示成了特征 \(x\) 的&lt;strong&gt;线性函数&lt;/strong&gt; \(W x + W_0\)。&lt;/p&gt;

\[\log \left( \frac{P(y=1|x)}{P(y=0|x)} \right) = W x + W_0 = \mathbf{z}\]

&lt;p&gt;两边同时取 \(e\) 的指数：&lt;/p&gt;

\[\frac{P(y=1|x)}{P(y=0|x)} = e^{\mathbf{z}}\]

&lt;p&gt;在二分类问题中，事件 \(y=1\) 和 \(y=0\) 是互斥且穷尽的，因此它们的概率之和必须为 1：&lt;/p&gt;

\[P(y=1|x) + P(y=0|x) = 1\]

&lt;p&gt;从上式可得：&lt;/p&gt;

\[P(y=0|x) = 1 - P(y=1|x)\]

&lt;p&gt;代入：&lt;/p&gt;

\[\frac{P(y=1|x)}{1 - P(y=1|x)} = e^{\mathbf{z}}\]

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;现在开始代数求解 $$P(y=1&lt;/td&gt;
      &lt;td&gt;x)\(（设\)\hat{y} = P(y=1&lt;/td&gt;
      &lt;td&gt;x)$$ 方便书写）：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[\frac{\hat{y}}{1 - \hat{y}} = e^{\mathbf{z}}\]

\[\hat{y} = e^{\mathbf{z}} (1 - \hat{y})\]

\[\hat{y} = e^{\mathbf{z}} - \hat{y} e^{\mathbf{z}}\]

&lt;p&gt;将包含 \(\hat{y}\) 的项移到等式左侧：&lt;/p&gt;

\[\hat{y} + \hat{y} e^{\mathbf{z}} = e^{\mathbf{z}}\]

&lt;p&gt;提取 \(\hat{y}\)：&lt;/p&gt;

\[\hat{y} (1 + e^{\mathbf{z}}) = e^{\mathbf{z}}\]

&lt;p&gt;最终解出 \(\hat{y}\)：&lt;/p&gt;

\[\hat{y} = \frac{e^{\mathbf{z}}}{1 + e^{\mathbf{z}}}\]

&lt;p&gt;为了得到更标准的 Sigmoid 形式，我们将分子和分母同时除以 \(e^{\mathbf{z}}\)：&lt;/p&gt;

\[\hat{y} = \frac{e^{\mathbf{z}} / e^{\mathbf{z}}}{(1 + e^{\mathbf{z}}) / e^{\mathbf{z}}} = \frac{1}{1/e^{\mathbf{z}} + e^{\mathbf{z}}/e^{\mathbf{z}}} = \frac{1}{e^{-\mathbf{z}} + 1}\]

&lt;p&gt;最后，用 \(\mathbf{z} = W x + W_0\) 替换 \(\mathbf{z}\)：&lt;/p&gt;

\[P(y=1|x) = \frac{1}{1 + e^{-(W x + W_0)}}\]

&lt;p&gt;这就是&lt;strong&gt;逻辑函数（Sigmoid 函数）&lt;/strong&gt; 的形式。&lt;/p&gt;

&lt;p&gt;这证明了在特征服从高斯分布且&lt;strong&gt;方差相等&lt;/strong&gt;的假设下，&lt;strong&gt;线性判别分析 (LDA)&lt;/strong&gt; 的结果在数学上与 &lt;strong&gt;逻辑回归&lt;/strong&gt; 是等价的。&lt;/p&gt;

&lt;p&gt;或者说上面的内容论证了&lt;strong&gt;生成模型&lt;/strong&gt;（基于贝叶斯公式和高斯分布假设）如何&lt;strong&gt;推导出&lt;/strong&gt;这个判别模型的形式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;将生成模型（假设特征服从高斯分布的贝叶斯分类器）推导至逻辑回归形式&lt;/strong&gt;所需的特定条件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;满足正态分布&lt;/li&gt;
  &lt;li&gt;方差相等&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;当数据满足“高斯分布”和“同方差”这两个条件时，基于贝叶斯和高斯分布的生成模型（即LDA）的决策边界形式，会自然地推导出与判别模型“逻辑回归”完全一致的形式。&lt;/strong&gt; 换句话说，在这些假设下，两个模型本质上是相同的线性分类器。如果&lt;strong&gt;类别条件概率&lt;/strong&gt;服从&lt;strong&gt;同方差高斯分布&lt;/strong&gt;，那么从贝叶斯公式推导出的&lt;strong&gt;后验概率&lt;/strong&gt; $$P(y&lt;/td&gt;
      &lt;td&gt;x)$$ 必然是&lt;strong&gt;逻辑回归&lt;/strong&gt;的Sigmoid形式。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;5最大似然估计到bcekl散度&quot;&gt;5.最大似然估计到BCE/KL散度&lt;/h1&gt;

&lt;p&gt;对于二分类问题，最大似然估计（Maximum Likelihood Estimation, MLE）的目标是找到一组参数 \(\mathbf{W}\) 和 \(b\) (或 \(\mathbf{W}\) 和 \(W_0\))，使得观测到的训练数据出现的&lt;strong&gt;总概率最大化&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;对于逻辑回归（Logistic Regression）模型，最大化似然函数（Likelihood Function）的等价操作就是最小化&lt;strong&gt;二元交叉熵损失（Binary Cross-Entropy Loss, BCE）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;下面是详细的推导过程。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;【1】逻辑回归的二分类模型&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;在二分类问题中，我们通常使用逻辑回归（Sigmoid 函数）来建模&lt;strong&gt;后验概率&lt;/strong&gt; $$P(y=1&lt;/td&gt;
      &lt;td&gt;x)$$。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;设输入特征为 \(\mathbf{x}\)，模型参数为 \(\boldsymbol{\theta} = \{\mathbf{W}, b\}\)。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;线性组合 (Logit):&lt;/p&gt;

\[z = \mathbf{W}^T \mathbf{x} + b\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;类别 1 的概率 (Sigmoid 函数):&lt;/p&gt;

\[\hat{y} = P(y=1|\mathbf{x}; \boldsymbol{\theta}) = \frac{1}{1 + e^{-z}}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;类别 0 的概率:&lt;/p&gt;

\[P(y=0|\mathbf{x}; \boldsymbol{\theta}) = 1 - \hat{y} = 1 - \frac{1}{1 + e^{-z}} = \frac{e^{-z}}{1 + e^{-z}}\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;【2】似然函数&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;对于单个训练样本 \((\mathbf{x}_i, y_i)\)，其中 \(y_i \in \{0, 1\}\)，我们可以将 $$P(y_i&lt;/td&gt;
      &lt;td&gt;\mathbf{x}_i; \boldsymbol{\theta})$$ 写成一个紧凑的形式：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[P(y_i|\mathbf{x}_i; \boldsymbol{\theta}) = (\hat{y}_i)^{y_i} \cdot (1 - \hat{y}_i)^{1 - y_i}\]

&lt;ul&gt;
  &lt;li&gt;如果 \(y_i = 1\)，则概率为 \(\hat{y}_i^1 (1 - \hat{y}_i)^0 = \hat{y}_i\)。&lt;/li&gt;
  &lt;li&gt;如果 \(y_i = 0\)，则概率为 \(\hat{y}_i^0 (1 - \hat{y}_i)^1 = 1 - \hat{y}_i\)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;假设我们有 \(N\) 个独立同分布的训练样本 \((\mathbf{x}_1, y_1), \dots, (\mathbf{x}_N, y_N)\)。&lt;strong&gt;似然函数 \(\mathcal{L}(\boldsymbol{\theta})\)&lt;/strong&gt; 是所有样本概率的乘积：&lt;/p&gt;

\[\mathcal{L}(\boldsymbol{\theta}) = P(Y|\mathbf{X}; \boldsymbol{\theta}) = \prod_{i=1}^{N} P(y_i|\mathbf{x}_i; \boldsymbol{\theta})\]

\[\mathcal{L}(\boldsymbol{\theta}) = \prod_{i=1}^{N} (\hat{y}_i)^{y_i} \cdot (1 - \hat{y}_i)^{1 - y_i}\]

&lt;p&gt;最大似然估计（MLE）的目标是找到 \(\boldsymbol{\theta}\) 使得 \(\mathcal{L}(\boldsymbol{\theta})\) 最大化：&lt;/p&gt;

\[\boldsymbol{\theta}_{\text{MLE}} = \arg \max_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})\]

&lt;p&gt;【3】对数似然函数（Log-Likelihood Function）&lt;/p&gt;

&lt;p&gt;为了简化计算（将乘积转化为求和，且避免浮点数下溢），我们通常最大化&lt;strong&gt;对数似然函数 \(\ell(\boldsymbol{\theta})\)&lt;/strong&gt;：&lt;/p&gt;

\[\ell(\boldsymbol{\theta}) = \log \mathcal{L}(\boldsymbol{\theta}) = \log \left( \prod_{i=1}^{N} (\hat{y}_i)^{y_i} \cdot (1 - \hat{y}_i)^{1 - y_i} \right)\]

&lt;p&gt;根据对数运算的性质:&lt;/p&gt;

\[\ell(\boldsymbol{\theta}) = \sum_{i=1}^{N} \left[ \log \left( (\hat{y}_i)^{y_i} \right) + \log \left( (1 - \hat{y}_i)^{1 - y_i} \right) \right]\]

\[\ell(\boldsymbol{\theta}) = \sum_{i=1}^{N} \left[ y_i \log (\hat{y}_i) + (1 - y_i) \log (1 - \hat{y}_i) \right]\]

&lt;p&gt;最大化对数似然函数的等价于最大化原始似然函数：&lt;/p&gt;

\[\boldsymbol{\theta}_{\text{MLE}} = \arg \max_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta})\]

&lt;p&gt;【4】损失函数：最小化负对数似然（Negative Log-Likelihood）&lt;/p&gt;

&lt;p&gt;在优化领域，习惯于将优化问题转化为&lt;strong&gt;最小化损失函数&lt;/strong&gt;的形式。因此，我们定义&lt;strong&gt;损失函数 \(J(\boldsymbol{\theta})\)&lt;/strong&gt; 为负的对数似然函数（Negative Log-Likelihood, NLL）：&lt;/p&gt;

\[J(\boldsymbol{\theta}) = - \ell(\boldsymbol{\theta})\]

\[\boldsymbol{\theta}_{\text{MLE}} = \arg \min_{\boldsymbol{\theta}} J(\boldsymbol{\theta})\]

&lt;p&gt;将 NLL 展开：&lt;/p&gt;

\[J(\boldsymbol{\theta}) = - \sum_{i=1}^{N} \left[ y_i \log (\hat{y}_i) + (1 - y_i) \log (1 - \hat{y}_i) \right]\]

&lt;p&gt;【5】得到二元交叉熵损失 (BCE Loss)&lt;/p&gt;

&lt;p&gt;最终得到的这个损失函数 \(J(\boldsymbol{\theta})\) &lt;strong&gt;正是二元交叉熵损失（Binary Cross-Entropy Loss, BCE）&lt;/strong&gt;，通常也称为对数损失（Log Loss）。&lt;/p&gt;

&lt;p&gt;通常，损失函数还会在前面加上 \(\frac{1}{N}\) 进行平均：&lt;/p&gt;

\[L_{\text{BCE}}(\boldsymbol{\theta}) = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log (\hat{y}_i) + (1 - y_i) \log (1 - \hat{y}_i) \right]\]

&lt;hr /&gt;

&lt;p&gt;总结来说：对于逻辑回归模型，&lt;strong&gt;最大化似然函数&lt;/strong&gt; \(\mathcal{L}(\boldsymbol{\theta})\) 在数学上等价于&lt;strong&gt;最小化二元交叉熵损失函数 \(L_{\text{BCE}}(\boldsymbol{\theta})\)&lt;/strong&gt;。交叉熵损失来源于最大似然估计在伯努利分布假设下的自然推导。&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/21/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9909/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/21/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9909/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙08】逻辑回归4让模型看的更准更稳，正则优化 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;1参数冗余性&quot;&gt;1.参数冗余性&lt;/h1&gt;

&lt;p&gt;对于同一条决策边界（直线），可以有&lt;strong&gt;无数个&lt;/strong&gt; \(W\)（或 \(W\) 和 \(W_0\)）进行表达。例如，如果 \(W\) 和 \(W_0\) 变为 \(-W\) 和 \(-W_0\)，决策边界不变，但预测概率 \(f(x)\) 变为 \(1-f(x)\)。&lt;/p&gt;

&lt;p&gt;另外，如果\(W\)变成\(10W\)，此时要考虑参数大小的影响（过拟合风险）&lt;/p&gt;

&lt;p&gt;当权重 \(W\) 很大时（例如 \(W=100\)），输入 \(x\) 的微小变化会导致 \(\text{Sigmoid}\) 函数（\(\frac{1}{1+e^{-(Wx+W_0)}}\)）的输出变化非常大。&lt;/p&gt;

&lt;p&gt;这表明&lt;strong&gt;大的 \(W\) 值会使模型对输入敏感（高方差）&lt;/strong&gt;，容易导致&lt;strong&gt;过拟合&lt;/strong&gt;。这解释了为什么在训练中需要使用&lt;strong&gt;正则化&lt;/strong&gt;来约束 \(W\) 的大小。&lt;/p&gt;

&lt;p&gt;在没有正则化的情况下，优化器可能会在这些等效的 \(W\) 组合中来回震荡，导致训练过程不稳定，并且最终得到的 \(W\) 缺乏可解释性。&lt;/p&gt;

&lt;p&gt;逻辑回归模型在没有正则化时，一个潜在的问题就是&lt;strong&gt;数值稳定性和过度自信&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&quot;2正则化的概念和作用&quot;&gt;2.正则化的概念和作用&lt;/h1&gt;

&lt;p&gt;正则化用于防止模型在训练数据上&lt;strong&gt;过拟合&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;正则化通过惩罚大的权重 \(W\)，降低模型的复杂度，使模型在 &lt;strong&gt;测试集&lt;/strong&gt; 上的误差（泛化误差）尽可能小。&lt;/p&gt;

&lt;p&gt;L2 正则化（也称为 &lt;strong&gt;Ridge 正则化&lt;/strong&gt;或&lt;strong&gt;权重衰减&lt;/strong&gt;）通过在标准损失函数 \(J(\theta)\) 中添加一个与权重向量 \(W\) 的平方范数成比例的惩罚项来实现。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;L2 正则化后的损失函数 \(J_{\text{reg}}(W)\)：&lt;/strong&gt;&lt;/p&gt;

\[J_{\text{reg}}(W) = \underbrace{J(W)}_{\text{原始损失（交叉熵）}} + \underbrace{\lambda \sum W_i^2}_{\text{L2 正则项}}\]

&lt;p&gt;其中 \(\lambda\) 是正则化系数，用于控制惩罚的强度。&lt;/p&gt;

&lt;p&gt;【正则化的作用】：&lt;/p&gt;

&lt;p&gt;1.从机器角度考虑：抑制 W 在分类正确情况下，按比例无限增大&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在训练集中，一旦模型找到了一个能够正确分类所有样本的权重 \(W\)，那么将 \(W\) 扩大任意倍数 \(c &amp;gt; 1\)（即 \(cW\)），模型损失（交叉熵）会更小。但是随着 \(W\) 无限增大，模型的损失会&lt;strong&gt;无限减小&lt;/strong&gt;。在数学上，梯度下降会推动 $W$ 不断增大，直到发生&lt;strong&gt;数值溢出&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;此时加入正则化就不同了。L2 正则项 \(\lambda \sum W_i^2\) 成为一个&lt;strong&gt;“约束”&lt;/strong&gt;。当 \(W\) 增大时，惩罚项也会&lt;strong&gt;二次方地快速增大&lt;/strong&gt;。这迫使优化器在减小原始损失 \(J(W)\) 的同时，必须&lt;strong&gt;付出越来越大的代价来增加 \(W\)&lt;/strong&gt;。最终，模型会找到一个 &lt;strong&gt;\(W\) 相对较小&lt;/strong&gt;的解，这个解既能正确分类，又能避免 \(W\) 无限膨胀和潜在的数值溢出问题。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2.减少测试集和训练集的差异性（提高泛化能力）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;过拟合的模型过度拟合了训练数据中的&lt;strong&gt;噪声&lt;/strong&gt;，导致其学习到的权重 \(W\) 过于复杂和极端。这种&lt;strong&gt;复杂的 \(W\) 使得决策边界在训练集上表现完美，但在测试集上表现糟糕&lt;/strong&gt;。&lt;/p&gt;

  &lt;p&gt;L2 正则化倾向于使所有权重 \(W_i\) 的值&lt;strong&gt;趋于零&lt;/strong&gt;。&lt;/p&gt;

  &lt;p&gt;小的 \(W_i\) 意味着模型对单个输入特征 \(x_i\) 的依赖程度减小(对噪声的依赖也会减小)。这使得模型的决策边界更加&lt;strong&gt;平滑&lt;/strong&gt;和&lt;strong&gt;简单&lt;/strong&gt;，降低了模型的方差。&lt;/p&gt;

  &lt;p&gt;L2 正则化强制模型关注那些对大多数样本都有效的&lt;strong&gt;核心特征&lt;/strong&gt;，忽略训练集中的噪声和异常值，从而&lt;strong&gt;提高模型在测试集上的泛化能力&lt;/strong&gt;，减小训练集和测试集性能的差异。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;3.破坏训练集的效果（引入偏差，降低方差）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;从统计学的偏差-方差权衡角度来看，正则化通过牺牲一点训练集的完美性，来换取测试集的稳定性。&lt;/p&gt;

  &lt;p&gt;正则化项 \(\lambda \sum W_i^2\) &lt;strong&gt;不是在帮助最小化训练误差&lt;/strong&gt;，它是在&lt;strong&gt;增加一个额外的损失&lt;/strong&gt;。为了让 \(\sum W_i^2\) 减小，模型必须将权重 \(W\) 往零的方向拉，即使这会导致原始的交叉熵损失 \(J(W)\) 略微增大。&lt;/p&gt;

  &lt;p&gt;因为模型被迫将 \(W\) 约束得很小，它可能无法完美拟合训练集中的每一个点，导致训练集上的损失（偏差）略微增加。&lt;/p&gt;

  &lt;p&gt;正则化使得模型对数据的微小变动（即测试集与训练集的差异）不再那么敏感，极大地降低了模型的方差。&lt;/p&gt;

  &lt;p&gt;最终的 \(J_{\text{reg}}(W)\) 追求的是 &lt;strong&gt;“训练误差”&lt;/strong&gt; 和 &lt;strong&gt;“模型复杂度”&lt;/strong&gt; 之间的最佳平衡点，这正是我们希望在&lt;strong&gt;测试集上得到最小总误差&lt;/strong&gt;所需的特性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;3l1-正则化lasso和-l2-正则化ridge&quot;&gt;3.L1 正则化（Lasso）和 L2 正则化（Ridge）&lt;/h1&gt;

&lt;p&gt;L2 正则项是权重W的平方和，乘以一个超参数 \(\lambda\)。损失函数变为 \(J_{\text{L2}}(W) = J(W) + \lambda \sum W_i^2\)。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 正则项是权重W&lt;strong&gt;绝对值&lt;/strong&gt;的和，乘以一个超参数 \(\lambda\)。损失函数变为 $$J_{\text{L1}}(W) = J(W) + \lambda \sum&lt;/td&gt;
      &lt;td&gt;W_i&lt;/td&gt;
      &lt;td&gt;$$。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;L2 惩罚项对&lt;strong&gt;大的权重&lt;/strong&gt;施加的惩罚更大（二次方增长）。在梯度下降时，权重 \(W\) 的每一次更新都会被拉向零点，但拉力与 \(W\) 的值成正比（梯度为 \(2W\)）。&lt;/p&gt;

&lt;p&gt;L1 惩罚项在 \(W\) 不为零时，施加的是一个&lt;strong&gt;恒定的惩罚&lt;/strong&gt;（梯度为 \(\pm 1\)）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/1*nrWncnoJ4V_BkzEf1pd4MA.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;L2约束区域是一个&lt;strong&gt;圆形&lt;/strong&gt;。相切点往往落在非坐标轴上，这意味着所有权重都会被缩小，但很少会精确地变为零。&lt;/p&gt;

&lt;p&gt;L1约束区域是一个&lt;strong&gt;菱形&lt;/strong&gt;（或正方形）。由于菱形的尖角位于坐标轴上，损失函数的等值线更容易与这些尖角相切，使得相切点的某些维度 \(W_i\) 恰好为零。所以L1 正则化的独特之处在于它能够产生&lt;strong&gt;稀疏解&lt;/strong&gt;，从而实现&lt;strong&gt;特征选择&lt;/strong&gt;。L1 倾向于将那些对模型贡献不大的特征所对应的权重&lt;strong&gt;直接压缩为零&lt;/strong&gt;。当数据集中包含大量冗余或不重要的特征时，L1 可以帮助我们自动识别并保留最重要的特征。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;如果目标是特征选择或降维&lt;/strong&gt;，L1 正则化通常是更好的选择。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;如果目标是确保所有特征都保留下来，只是希望减小它们的权重以提高模型的鲁棒性和稳定性&lt;/strong&gt;，L2 正则化是更常见的选择。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在实践中，人们常结合两者，使用 &lt;strong&gt;Elastic Net&lt;/strong&gt; 正则化来兼顾特征选择和模型平滑。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果不指定正则，一般也会默认用L2正则，因为要防止参数溢出。&lt;/p&gt;

  &lt;p&gt;互联网行业，一般特征比较多（维度爆炸），一般采用L1正则，为了降维。&lt;/p&gt;

  &lt;p&gt;特征比较少的情况（比如生物），可以只采用L2正则（这也是默认的）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;4归一化&quot;&gt;4.归一化&lt;/h1&gt;

&lt;p&gt;归一化（Normalization，或标准化）通常是指对输入特征 \(x\) 进行缩放，使其落入特定范围（如 \([0, 1]\)）或具有标准分布（如均值 \(0\)，方差 \(1\)）。&lt;/p&gt;

&lt;h2 id=&quot;41归一化的重要性&quot;&gt;4.1归一化的重要性&lt;/h2&gt;

&lt;p&gt;模型的梯度（参数更新方向）依赖于输入特征 \(x\)。&lt;/p&gt;

&lt;p&gt;回顾逻辑回归的梯度：https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9906/&lt;/p&gt;

&lt;p&gt;对于整个训练集（所有 \(N\) 个样本），总损失 \(L(\mathbf{w}) = \frac{1}{N} \sum_{i=1}^{N} L_i(\mathbf{w})\) 对 \(w_j\) 的梯度为：&lt;/p&gt;

\[\frac{\partial L}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial L_i}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}\]

&lt;p&gt;显然，这个梯度依赖于x，并且梯度更新时，模型会沿着大范围特征（如 \(x_1\)）对应的维度进行大步更新，而在小范围特征（如 \(x_2\)）对应的维度进行小步更新。&lt;/p&gt;

&lt;p&gt;这会导致&lt;strong&gt;梯度下降的等高线变得非常扁平狭长&lt;/strong&gt;，优化过程需要很小的学习率才能避免振荡，导致收敛速度非常慢。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251120233233215.png&quot; alt=&quot;image-20251120233233215&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;归一化使得所有特征 \(x_i\) 具有相似的尺度。&lt;/p&gt;

&lt;p&gt;这保证了所有权重 \(W_i\) 的梯度大致位于同一数量级，使得&lt;strong&gt;等高线更接近圆形&lt;/strong&gt;，梯度下降可以沿着最陡峭的方向平稳快速地收敛。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;归一化间接帮助控制模型内部的线性得分（\(z = W^T x\)），这对于激活函数的稳定性非常重要。&lt;/p&gt;

&lt;p&gt;Softmax 或 Sigmoid 函数的输出 \(f(x)\) 依赖于线性得分 \(z = W^T x\)​。如果输入特征 \(x\) 的值很大，即使 \(W\) 很小， \(z\) 也可能非常大或非常小。&lt;/p&gt;

&lt;p&gt;当 \(z\) 的绝对值过大时，Sigmoid函数会输出接近 0或 1的值(模型可能一开始就学不到新东西，梯度消失了，参数不更新，模型还没怎么训练或者训练其实不到位就认为已经收敛了)，此时函数的&lt;strong&gt;导数（梯度）会非常小&lt;/strong&gt;（进入饱和区）。归一化确保 \(x\) 的尺度适中，可以帮助控制 \(W^T x\) 的范围。&lt;/p&gt;

&lt;p&gt;配合&lt;strong&gt;正则化&lt;/strong&gt;（它限制了 \(W\) 的大小），可以共同确保线性得分 \(W^T x\) 不会过度膨胀，从而&lt;strong&gt;避免激活函数进入饱和区&lt;/strong&gt;，保证训练过程中梯度的有效性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251120232445510.png&quot; alt=&quot;image-20251120232445510&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如何选择归一化的方法？是Min-Max 归一化还是Z-score？&lt;/p&gt;

&lt;p&gt;【Min-Max 归一化】缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;对异常值（Outliers）非常敏感。&lt;/strong&gt; 如果数据中存在极大的或极小的异常值，它们会严重挤压其余数据的范围，导致大部分数据点集中在 \([0, 1]\) 范围的一小部分，失去区分度。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;【Z-score归一化】缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不将数据限制在特定的范围 \([0, 1]\) 内，缩放后的特征值可能超出此范围。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;【Min-Max 归一化】优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将所有特征值限制在固定的 \([0, 1]\) 范围内，易于解释和比较。&lt;/li&gt;
  &lt;li&gt;在特征数量很少，且特征间差异不大的情况下表现良好。&lt;/li&gt;
  &lt;li&gt;不改变数据的原始分布形状。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;【Z-score归一化】优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使得数据符合标准正态分布（但不会改变原始分布的形状，只是进行平移和缩放）。&lt;/li&gt;
  &lt;li&gt;鲁棒性好。算法使用均值和标准差进行缩放，受异常值的影响较小（尤其当数据集较大时）。&lt;/li&gt;
  &lt;li&gt;适用于&lt;strong&gt;要求数据符合正态分布&lt;/strong&gt;或&lt;strong&gt;不依赖于固定范围&lt;/strong&gt;的算法，例如梯度下降法（有助于收敛）和许多线性模型。&lt;/li&gt;
  &lt;li&gt;适用于不知道数据确切范围，但需要保证特征同尺度的场景。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Z-Score 归一化 (Standardization)&lt;/strong&gt; 通常是更优的选择，因为它确保了所有特征的尺度一致，有助于梯度下降算法更快、更稳定地收敛。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Min-Max 归一化&lt;/strong&gt; 也可使用，但在处理带有极端异常值的数据时，需要格外小心。&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9908/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9908/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙07】逻辑回归3到底好不好？模型评价指标 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;1逻辑回归指标&quot;&gt;1.逻辑回归指标&lt;/h1&gt;

&lt;h2 id=&quot;11混淆矩阵&quot;&gt;1.1混淆矩阵&lt;/h2&gt;

&lt;p&gt;混淆矩阵 (Confusion Matrix)是评估分类模型性能的基础。它的四个象限记录了模型预测结果与真实标签之间的四种组合：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;预测为正 (Positive, P)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;预测为负 (Negative, N)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;行总计&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;真实为正 (y=1)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;真阳性 (TP)&lt;/strong&gt; \(\text{①}\)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;假阴性 (FN)&lt;/strong&gt; \(\text{②}\)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;MP&lt;/strong&gt; (模型预测总正类)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;真实为负 (y=0)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;假阳性 (FP)&lt;/strong&gt; \(\text{③}\)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;真阴性 (TN)&lt;/strong&gt; \(\text{④}\)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;MN&lt;/strong&gt; (模型预测总负类)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;各项定义：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;① 真阳性 (True Positive, TP):&lt;/strong&gt; 真实值是 1，模型&lt;strong&gt;正确预测&lt;/strong&gt;为1。 (击中目标)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;② 假阴性 (False Negative, FN):&lt;/strong&gt; 真实值是 1，模型&lt;strong&gt;错误预测&lt;/strong&gt;为0。 (漏报)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;③ 假阳性 (False Positive, FP):&lt;/strong&gt; 真实值是0，模型&lt;strong&gt;错误预测&lt;/strong&gt;为1。 (误报)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;④ 真阴性 (True Negative, TN):&lt;/strong&gt; 真实值是0，模型&lt;strong&gt;正确预测&lt;/strong&gt;为0。 (正确拒绝)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;12准确率&quot;&gt;1.2准确率&lt;/h2&gt;

\[\text{准确率} = \frac{\text{正确预测的样本总数}}{\text{样本总数}}\]

\[\text{准确率} = \frac{\text{TP} + \text{TN}}{\text{MP} + \text{MN}}\]

&lt;p&gt;或者用预测总数表示：&lt;/p&gt;

\[\text{准确率} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FN} + \text{FP} + \text{TN}}\]

&lt;p&gt;【&lt;strong&gt;数据不平衡问题下准确率的欺骗性&lt;/strong&gt;】尽管准确率计算简单，但在数据不平衡的情况下，它具有欺骗性。如果一个模型总是预测 0，其准确率仍高达 99%，但它在识别关键的&lt;strong&gt;少数类1&lt;/strong&gt; 上的能力（即 &lt;strong&gt;TP&lt;/strong&gt; 和 &lt;strong&gt;FN&lt;/strong&gt;）极差。&lt;/p&gt;

&lt;p&gt;例如一个极度不平衡的二分类场景，总样本量为100个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;多数类（正样本）：&lt;/strong&gt; 95个正样本（通常记为 \(P\) 或 \(y=1\)）。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;少数类（负样本）：&lt;/strong&gt; 5个负样本（通常记为 \(N\) 或 \(y=0\)）。&lt;/li&gt;
&lt;/ul&gt;

\[\text{准确率} = \frac{\text{TP} + \text{TN}}{\text{样本总数}} = \frac{94 + 1}{100} = \frac{95}{100} = 95\%\]

&lt;p&gt;尽管这个模型的&lt;strong&gt;准确率高达 \(95\%\)&lt;/strong&gt;，但它在&lt;strong&gt;少数类&lt;/strong&gt;上的性能是极差的&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对多数类（95 个正样本）：&lt;/strong&gt; 模型表现极好，只错判了 1个 \((\text{FN}=1)\)。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对少数类（5 个负样本）：&lt;/strong&gt; 模型表现极差。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;真阴性 (TN) 只有 1 个&lt;/strong&gt;：在 5 个负样本中，模型只正确识别了 1 个负样本。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;假阳性 (FP) 有 4个&lt;/strong&gt;：模型将 4 个负样本错误地判断为正样本，&lt;strong&gt;识别错误率高达 \(80\%\)&lt;/strong&gt; (\(4/5\))。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;这个例子强有力地证明了，在数据不平衡的情况下，我们必须使用对少数类性能更敏感的指标，例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;召回率 (Recall)&lt;/strong&gt;：衡量模型发现所有少数类的能力（在这个例子中：\(\text{Recall} = \frac{\text{TP}}{\text{TP}+\text{FN}} = \frac{94}{95} \approx 98.9\%\)）。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;特异度 (Specificity)&lt;/strong&gt;：衡量模型正确识别负类的能力（在这个例子中：\(\text{Specificity} = \frac{\text{TN}}{\text{TN}+\text{FP}} = \frac{1}{1+4} = 20\%\)）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;低特异度 \(20\%\)&lt;/strong&gt; 清晰地揭示了这个模型的真正缺陷，而这是 \(95\%\) 准确率所掩盖的。&lt;/p&gt;

&lt;h2 id=&quot;13召回率&quot;&gt;1.3召回率&lt;/h2&gt;

&lt;p&gt;召回率，也称为&lt;strong&gt;查全率 (Sensitivity)&lt;/strong&gt;，衡量的是模型&lt;strong&gt;找出所有真正正样本的能力&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;召回率的计算公式是：&lt;/p&gt;

\[\text{召回率 (Recall)} = \frac{\text{真阳性}}{\text{真阳性} + \text{假阴性}}\]

\[\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TP (True Positive, 真阳性):&lt;/strong&gt; 模型正确地预测为正类的样本数。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FN (False Negative, 假阴性):&lt;/strong&gt; 模型错误地预测为负类的样本数（即&lt;strong&gt;漏掉的正样本&lt;/strong&gt;）。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;分母 \(\text{TP} + \text{FN}\):&lt;/strong&gt; 等于&lt;strong&gt;所有真实的正样本总数&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;召回率回答了这个问题：“在所有真正是正样本的数据中，模型成功找出了多少比例？”&lt;/p&gt;

&lt;p&gt;召回率在那些&lt;strong&gt;“漏报”的成本非常高&lt;/strong&gt;的场景中至关重要。这意味着我们宁愿多报一些假的正样本 (FP)，也不能漏掉任何一个真正的正样本 (FN)。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;应用场景&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;正类 (1) 的定义&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;召回率的重要性&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;医学诊断&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;患者患有某种疾病。&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;高召回率是必需的。&lt;/strong&gt; 漏诊（FN，把患病病人判为健康）可能导致生命危险。多报（FP，把健康人判为患病）只是增加进一步检查的成本。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;反欺诈/安全&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;交易是欺诈。&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;高召回率至关重要。&lt;/strong&gt; 漏掉欺诈交易（FN）会导致巨大的经济损失。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;推荐系统&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;用户对某商品感兴趣。&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;高召回率是基础。&lt;/strong&gt; 必须确保将用户可能感兴趣的所有商品都纳入推荐池中，以供用户选择。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;召回率通常与另一个重要指标&lt;strong&gt;查准率 (Precision)&lt;/strong&gt; 相互制约，二者构成著名的&lt;strong&gt;Precision-Recall 权衡&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;查准率 (Precision):&lt;/strong&gt; \(\frac{\text{TP}}{\text{TP} + \text{FP}}\)。衡量模型在所有预测为正的样本中，有多少是真正正确的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通常，提高召回率（找到更多的正样本）往往是以降低查准率（误报更多的负样本）为代价的。模型设计者需要根据具体业务需求（例如，是“宁愿不错杀一个”还是“宁愿不放过一个”）来调整阈值，在召回率和查准率之间找到最佳平衡点。&lt;/p&gt;

&lt;h2 id=&quot;14准确率和召回率和阀值有关&quot;&gt;1.4准确率和召回率和阀值有关&lt;/h2&gt;

&lt;p&gt;逻辑回归和 Softmax 回归的&lt;strong&gt;正向传播&lt;/strong&gt;（计算 \(h_{\theta}(x)\)）会给每个样本分配一个概率 \(\phi\)。默认情况下，这个阈值通常设定为0.5。&lt;/p&gt;

&lt;p&gt;改变这个阈值，会直接改变模型预测 \(\hat{y}=1\) 和 \(\hat{y}=0\) 的样本数量，从而改变混淆矩阵的四个值（TP, FN, FP, TN），最终影响所有指标。&lt;/p&gt;

&lt;p&gt;这种阈值对指标的影响，引出了 &lt;strong&gt;查准率-召回率权衡 (Precision-Recall Trade-off)&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;如果业务目标是“不放过任何一个”（高召回率）：&lt;/strong&gt; 应该选择一个&lt;strong&gt;较低的阈值&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;如果业务目标是“不误判任何一个”（高查准率）：&lt;/strong&gt; 应该选择一个&lt;strong&gt;较高的阈值&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在实际建模中，通常会绘制 &lt;strong&gt;ROC 曲线&lt;/strong&gt;或 &lt;strong&gt;Precision-Recall 曲线&lt;/strong&gt;，来可视化模型在所有可能的阈值下的表现，从而选择最符合业务需求的最佳阈值。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;另外，&lt;strong&gt;阈值与产品形态有关&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;不同的产品形态，其对错误类型的容忍度不同，从而要求模型在召回率和查准率之间做出不同的权衡，而这种权衡是通过调整阈值来实现的。&lt;/p&gt;

&lt;p&gt;模型算法提供的是&lt;strong&gt;概率&lt;/strong&gt;，而&lt;strong&gt;产品形态决定了你如何使用这个概率&lt;/strong&gt;，即决定了要设定一个什么样的阈值，来将模型的通用能力，转化为符合特定商业需求的决策。&lt;/p&gt;

&lt;p&gt;例如：反政治反动暴力黄色，追求的一定是高的召回率（不放过任何一个）。&lt;/p&gt;

&lt;p&gt;这种和产品形态有关的事情，应该由产品经理去做。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;准确和召回跟阈值有关，而阈值的选择依赖于产品形态。那么到底该用什么单纯评价模型的好坏呢？到底什么指标和阈值无关呢？&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;将模型的内在质量（与阈值无关）与应用时的决策（与阈值有关）区分开来。&lt;/strong&gt;当我们要&lt;strong&gt;单纯评价模型“好坏”&lt;/strong&gt;，即评价模型&lt;strong&gt;对概率的排序能力&lt;/strong&gt;时，确实需要使用与阈值无关的指标。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;评价模型&lt;strong&gt;内在质量&lt;/strong&gt;，即评价模型&lt;strong&gt;将正样本和负样本的概率正确分开的能力&lt;/strong&gt;，主要使用以下两个指标，它们基于模型在&lt;strong&gt;所有可能阈值&lt;/strong&gt;下的表现&lt;/p&gt;

&lt;h2 id=&quot;15auc-roc&quot;&gt;1.5AUC-ROC&lt;/h2&gt;

&lt;p&gt;AUC-ROC (Area Under the Receiver Operating Characteristic Curve)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;ROC是曲线，AUC是一个值，代表曲线下面的面积。不同模型的优劣可以通过比较AUC值判断。AUC越大，越优秀。AUC值衡量的是模型对正负样本的&lt;strong&gt;排序能力&lt;/strong&gt;。&lt;/p&gt;

  &lt;p&gt;AUC 的核心定义是：&lt;/p&gt;

\[AUC = \frac{\text{正样本比负样本预测分值大的组合数}}{\text{正反样本的组合数量}}\]

  &lt;p&gt;如果从数据集中随机抽取一个正样本和一个负样本，模型给正样本的预测概率（或分数）高于给负样本的预测概率的概率，就是 AUC 值。这个定义与任何具体的阈值无关，它只关心模型输出的概率分数是否能正确地将正样本排在负样本前面。在实际计算中，尤其是当预测分数是&lt;strong&gt;离散&lt;/strong&gt;的时，AUC 可以通过对 ROC 曲线下的&lt;strong&gt;梯形面积&lt;/strong&gt;求和来近似或精确计算&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251120210220924.png&quot; alt=&quot;image-20251120210220924&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;AUC-ROC 是评估 GLM 模型（如逻辑回归和 Softmax 回归）&lt;strong&gt;内在能力&lt;/strong&gt;的最佳指标。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ROC 曲线描绘了在所有可能的分类阈值下，模型的&lt;strong&gt;真阳性率 (True Positive Rate, TPR)&lt;/strong&gt; 和 &lt;strong&gt;假阳性率 (False Positive Rate, FPR)&lt;/strong&gt; 之间的关系。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;ROC 曲线是从 (0, 0) 到 (1, 1) 的一条曲线。一个优秀的模型应该使得曲线尽可能靠近左上角 (0, 1)，这意味着在很低的误报率（FPR）下，就能获得很高的召回率（TPR）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;计算方式：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251120194316982.png&quot; alt=&quot;image-20251120194316982&quot; style=&quot;zoom: 33%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TPR (召回率/敏感度):&lt;/strong&gt; \(\frac{\text{TP}}{\text{TP} + \text{FN}}\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FPR (特异度补充):&lt;/strong&gt; \(\frac{\text{FP}}{\text{FP} + \text{TN}}\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AUC-ROC&lt;/strong&gt; 就是这条曲线下的面积。&lt;/li&gt;
  &lt;li&gt;ROC 曲线绘制的是 &lt;strong&gt;真阳性率 (True Positive Rate, TPR)&lt;/strong&gt; 随 &lt;strong&gt;假阳性率 (False Positive Rate, FPR)&lt;/strong&gt; 变化的关系。ROC 曲线代表了分类器&lt;strong&gt;敏感度 (Sensitivity)&lt;/strong&gt; 和 &lt;strong&gt;特异度 (Specificity)&lt;/strong&gt; 之间的权衡。如果你想要提高召回率（TPR），你将不可避免地提高误报率（FPR）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;AUC-ROC 衡量的是模型将随机选择的一个正样本排在随机选择的一个负样本之前的概率。&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;值范围：&lt;/strong&gt; \([0.5, 1]\)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;值解释：&lt;/strong&gt; 0.5代表随机猜测；1.0代表完美分类。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ROC 曲线本身就是&lt;strong&gt;通过遍历所有阈值&lt;/strong&gt;得到的。因此，&lt;strong&gt;AUC-ROC 作为一个单一数值，总结了模型在所有阈值下的整体性能，与最终选定的特定阈值无关。&lt;/strong&gt; 它评价的是模型对概率的&lt;strong&gt;排序能力&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;auc的值一定越高越好吗不会过拟合吗&quot;&gt;AUC的值一定越高越好吗？不会过拟合吗？&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;AUC 的值是越高越好，但前提是这个高 AUC 是在&lt;/strong&gt; &lt;strong&gt;独立的测试集或验证集&lt;/strong&gt; &lt;strong&gt;上获得的。&lt;/strong&gt; 如果高 AUC 仅在训练集上取得，则很可能存在&lt;strong&gt;过拟合&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;高 AUC（接近 1.0）意味着模型能够更好地将正样本（高概率/分数）排在负样本（低概率/分数）之前。AUC 衡量的是模型概率分数本身的质量。AUC 越高，说明模型在所有可能的阈值下，其表现都越优秀。&lt;/p&gt;

&lt;p&gt;只有当测试集 AUC 较高时，我们才能确信这是一个既具有强大区分能力，又具有良好泛化能力的优秀模型。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;在训练集上，我们希望 AUC 尽可能高（趋近 1.0）。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;在测试集上，我们希望 AUC 尽可能高，并且与训练集 AUC 的差距尽可能小。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了防止过拟合，我们会在训练 Softmax 回归和逻辑回归等模型时，引入 &lt;strong&gt;L1 或 L2 正则化&lt;/strong&gt;（也称为权重衰减），通过惩罚过大的参数 \(\theta\) 来降低模型复杂度，从而提高模型的泛化能力和测试集 AUC。&lt;/p&gt;

&lt;h2 id=&quot;16auprc&quot;&gt;1.6AUPRC&lt;/h2&gt;

&lt;p&gt;AUPRC (Area Under the Precision-Recall Curve)&lt;/p&gt;

&lt;p&gt;PR 曲线描绘了在所有可能的分类阈值下，模型的&lt;strong&gt;查准率 (Precision)&lt;/strong&gt; 和 &lt;strong&gt;召回率 (Recall)&lt;/strong&gt; 之间的关系。&lt;/p&gt;

&lt;p&gt;AUPRC 衡量的是模型在不同召回水平下查准率的平均表现。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AUPRC 对数据不平衡问题更敏感。&lt;/strong&gt; 在数据高度不平衡时，AUPRC 的值能更准确地反映模型在少数类上的识别能力。如果产品形态涉及罕见事件，AUPRC 往往是比 AUC-ROC 更好的“纯模型”评估指标。&lt;/p&gt;

&lt;h1 id=&quot;2总结&quot;&gt;2.总结&lt;/h1&gt;

&lt;p&gt;1.正确率    问题：容易被两类不平衡所影响 指标被阈值所影响&lt;/p&gt;

&lt;p&gt;2.准确率和召回率 单独看一类预测结果的指标 问题：指标被阈值所影响&lt;/p&gt;

&lt;p&gt;3.ROC曲线 和auc值：真正反映了模型的能力，表达了正负样本分数的区分度
问题：推理过程不好理解  auc值一般是0.7～0.85之间（面试不要吹的太夸张）&lt;/p&gt;

</description>
        <pubDate>Thu, 20 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9907/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9907/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙06】逻辑回归2损失函数推到解析和特征选择优化 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;1逻辑回归回顾&quot;&gt;1.逻辑回归回顾&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# -*- encoding:utf-8 -*-
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_val_predict&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_loss&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;read_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;readlines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;curve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{},{}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# y_pred=model.predict_proba(X_test)
# print(y_pred)
#loss=log_loss(y_test,y_pred)
#print (&quot;KL_loss:&quot;,loss)
#loss=log_loss(y_pred,y_test)
#print (&quot;KL_loss:&quot;,loss)
&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
curve_results=curve(X_train,model.coef_.tolist()[0],model.intercept_.tolist()[0])
with open(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;train_with_splitline&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;) as f :
	f.writelines(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.join(curve_results))
&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&apos;&apos;&lt;/span&gt;

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251118170657789.png&quot; alt=&quot;image-20251118170657789&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict_proba&lt;/code&gt;是用来预测概率的。如果解开下面2行注释：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251118171146410.png&quot; alt=&quot;image-20251118171146410&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，应该是大于0.5就会有预测结果0，小于0.5预测结果就是1.&lt;/p&gt;

&lt;p&gt;且每一行求和其实是1，因为是/非  是互斥的事件。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;计算KL距离（即损失函数）的函数是&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log_loss&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251118172001139.png&quot; alt=&quot;image-20251118172001139&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;注意：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.fit(X_train, y_train)&lt;/code&gt;就是模型不断学习的过程。即学习合适的参数\(w\)。&lt;/p&gt;

&lt;h1 id=&quot;2逻辑回归的损失函数bce求导与梯度下降&quot;&gt;2.逻辑回归的损失函数BCE求导与梯度下降&lt;/h1&gt;

&lt;p&gt;二元逻辑回归（Binary Logistic Regression）的损失函数——&lt;strong&gt;二元交叉熵损失 (Binary Cross-Entropy Loss, BCE Loss)&lt;/strong&gt; ：&lt;/p&gt;

\[\mathcal{L}_{\text{BCE}}(P || Q) = - \sum_{k \in \{0, 1\}} P(Y=k) \log Q(Y=k) = - [y \log \hat{y} + (1 - y) \log (1 - \hat{y})]=\\ \sum_{i=1}^{n} \left[ y_i \log \frac{y_i}{f_i} + (1 - y_i) \log \frac{1 - y_i}{1 - f_i} \right]\]

&lt;p&gt;逻辑回归模型 (Hypothesis):&lt;/p&gt;

\[\hat{y}_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}\]

&lt;p&gt;其中 \(z_i\) 是线性得分：&lt;/p&gt;

\[z_i = \mathbf{w}^T \mathbf{x}_i + b\]

&lt;p&gt;（这里我们把偏置 \(b\) 视为 \(\mathbf{w}\) 中的 \(w_0\) 且 \(\mathbf{x}_i\) 扩展了 \(x_{i,0}\)=1维，简化为 \(\mathbf{w}^T \mathbf{x}_i\)。）&lt;/p&gt;

&lt;h2 id=&quot;21求导&quot;&gt;2.1求导&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;目标：&lt;/strong&gt; 求损失函数 $$\mathcal{L}_{\text{BCE}}(P&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Q)\(对权重向量\)\mathbf{w}\(中任一分量\)w_j\(的偏导数\)\frac{\partial L_i}{\partial w_j}$$。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;使用链式法则，从 \(L_i\) 逐步向 \(w_j\) 追溯：&lt;/p&gt;

\[\frac{\partial L_i}{\partial w_j} = \frac{\partial L_i}{\partial \hat{y}_i} \cdot \frac{\partial \hat{y}_i}{\partial z_i} \cdot \frac{\partial z_i}{\partial w_j}\]

&lt;p&gt;【步骤 A】: \(\frac{\partial L_i}{\partial \hat{y}_i}\) (损失对预测概率的导数)&lt;/p&gt;

\[\frac{\partial L_i}{\partial \hat{y}_i} = - \left[ y_i \cdot \frac{1}{\hat{y}_i} + (1 - y_i) \cdot \frac{1}{1 - \hat{y}_i} \cdot (-1) \right]\]

\[\frac{\partial L_i}{\partial \hat{y}_i} = - \left[ \frac{y_i}{\hat{y}_i} - \frac{1 - y_i}{1 - \hat{y}_i} \right]\]

\[\frac{\partial L_i}{\partial \hat{y}_i} = - \left[ \frac{y_i (1 - \hat{y}_i) - \hat{y}_i (1 - y_i)}{\hat{y}_i (1 - \hat{y}_i)} \right]\]

\[\frac{\partial L_i}{\partial \hat{y}_i} = - \left[ \frac{y_i - y_i \hat{y}_i - \hat{y}_i + y_i \hat{y}_i}{\hat{y}_i (1 - \hat{y}_i)} \right] = - \frac{y_i - \hat{y}_i}{\hat{y}_i (1 - \hat{y}_i)}\]

&lt;p&gt;【步骤 B】: \(\frac{\partial \hat{y}_i}{\partial z_i}\) (Sigmoid 函数对线性得分的导数)&lt;/p&gt;

&lt;p&gt;Sigmoid 函数 \(\sigma(z) = \frac{1}{1 + e^{-z}}\) 的导数有一个非常简洁的形式：&lt;/p&gt;

\[\frac{\partial \sigma(z_i)}{\partial z_i} = \sigma(z_i) (1 - \sigma(z_i)) = \hat{y}_i (1 - \hat{y}_i)\]

&lt;p&gt;【步骤 C】: \(\frac{\partial z_i}{\partial w_j}\) (线性得分对权重的导数)&lt;/p&gt;

\[z_i = w_0 x_{i,0} + w_1 x_{i,1} + \dots + w_j x_{i,j} + \dots\]

\[\frac{\partial z_i}{\partial w_j} = \frac{\partial}{\partial w_j} (\mathbf{w}^T \mathbf{x}_i) = x_{i,j}\]

&lt;p&gt;【步骤 D】: 合并结果 (最终梯度)&lt;/p&gt;

&lt;p&gt;将 A、B、C 三个结果相乘：&lt;/p&gt;

\[\frac{\partial L_i}{\partial w_j} = \left[ - \frac{y_i - \hat{y}_i}{\hat{y}_i (1 - \hat{y}_i)} \right] \cdot \left[ \hat{y}_i (1 - \hat{y}_i) \right] \cdot \left[ x_{i,j} \right]\]

&lt;p&gt;观察到中间两项相乘可以抵消：&lt;/p&gt;

\[\frac{\partial L_i}{\partial w_j} = - (y_i - \hat{y}_i) x_{i,j} = (\hat{y}_i - y_i) x_{i,j}\]

&lt;hr /&gt;

&lt;p&gt;所以总体损失梯度如下：&lt;/p&gt;

&lt;p&gt;对于整个训练集（所有 \(N\) 个样本），总损失 \(L(\mathbf{w}) = \frac{1}{N} \sum_{i=1}^{N} L_i(\mathbf{w})\) 对 \(w_j\) 的梯度为：&lt;/p&gt;

\[\frac{\partial L}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial L_i}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}\]

&lt;h2 id=&quot;22梯度下降逼近最优解&quot;&gt;2.2梯度下降逼近最优解&lt;/h2&gt;

&lt;p&gt;得到梯度后，就可以使用梯度下降法来迭代更新权重 \(\mathbf{w}\)，以最小化总体损失 \(L(\mathbf{w})\)。&lt;/p&gt;

&lt;p&gt;在每次迭代中，权重 \(\mathbf{w}\) 会沿着梯度的&lt;strong&gt;负方向&lt;/strong&gt;进行更新。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;更新公式:&lt;/p&gt;

\[\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \alpha \nabla_{\mathbf{w}} L(\mathbf{w}_{\text{old}})\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对单个权重 \(w_j\) 的更新:&lt;/p&gt;

\[w_{j, \text{new}} = w_{j, \text{old}} - \alpha \cdot \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中 \(\alpha\) 是&lt;strong&gt;学习率 (Learning Rate)&lt;/strong&gt;，它控制了每一步更新的步长。&lt;/p&gt;

&lt;p&gt;整个迭代过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;初始化：&lt;/strong&gt; 随机初始化权重向量 \(\mathbf{w}\)。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;迭代循环 (直到收敛)：&lt;/p&gt;

    &lt;p&gt;a. 计算预测值： 对于所有样本 \(i=1\) 到 \(N\)，计算线性得分 \(z_i = \mathbf{w}^T \mathbf{x}_i\)，并计算预测概率 \(\hat{y}_i = \sigma(z_i)\)。&lt;/p&gt;

    &lt;p&gt;b. 计算梯度： 计算每个权重 \(w_j\) 的平均梯度 \(\frac{\partial L}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}\)。&lt;/p&gt;

    &lt;p&gt;c. 更新权重： 使用更新公式 \(w_{j, \text{new}} = w_{j, \text{old}} - \alpha \cdot \frac{\partial L}{\partial w_j}\) 更新所有权重。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;收敛：&lt;/strong&gt; 当权重向量 \(\mathbf{w}\) 在后续迭代中的变化小于一个预设的阈值，或者达到最大迭代次数时，停止迭代。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这个梯度 \(\frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i) x_{i,j}\) 直观地表示了&lt;strong&gt;“平均预测误差”&lt;/strong&gt;乘以&lt;strong&gt;“输入特征”&lt;/strong&gt;，模型会根据这个误差调整权重，使预测概率 \(\hat{y}_i\) 更接近真实标签 \(y_i\)。&lt;/p&gt;

&lt;h1 id=&quot;3逻辑回归不用mse而用bce&quot;&gt;3.逻辑回归不用MSE而用BCE&lt;/h1&gt;

&lt;p&gt;MSE均方误差：\(L_{MSE}=\frac{1}{N}\sum_{i=1}^N(f_i-y_i)^2\)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;BCE二元交叉熵损失：$$\mathcal{L}_{\text{BCE}}(P&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Q) = - \sum_{k \in {0, 1}} P(Y=k) \log Q(Y=k) = - [y \log \hat{y} + (1 - y) \log (1 - \hat{y})]=\ \sum_{i=1}^{n} \left[ y_i \log \frac{y_i}{f_i} + (1 - y_i) \log \frac{1 - y_i}{1 - f_i} \right]$$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;MSE求导：对于逻辑回归，模型预测为 \(\hat{y}_i = \sigma(z_i)\)，代入 MSE 损失公式并对权重 \(w_j\) 求导：&lt;/p&gt;

\[L_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^N(\hat{y}_i-y_i)^2\]

\[\frac{\partial L_{\text{MSE}}}{\partial w_j} = \frac{2}{N} \sum_{i} (\hat{y}_i - y_i) \cdot \underbrace{\hat{y}_i (1 - \hat{y}_i)}_{\text{Sigmoid 导数}} \cdot x_{i,j}\]

&lt;p&gt;而BCE求导上面已经求过了：&lt;/p&gt;

\[\frac{\partial L_i}{\partial w_j} = - (y_i - \hat{y}_i) x_{i,j} = (\hat{y}_i - y_i) x_{i,j}\]

&lt;hr /&gt;

&lt;p&gt;为什么逻辑回归不用MSE而用BCE？&lt;/p&gt;

&lt;p&gt;【1】若用MSE，如果初始权重 \(\mathbf{w}\) 被设置得&lt;strong&gt;非常大&lt;/strong&gt;（在绝对值意义上），那么对于大多数样本 \(\mathbf{x}_i\)：&lt;/p&gt;

\[|z_i| = |\mathbf{w}^T \mathbf{x}_i + b|\]

&lt;p&gt;\(z_i\) 的绝对值也会变得&lt;strong&gt;非常大&lt;/strong&gt;。也就是说，当权重很大时，即使特征值 \(x_{i,j}\) 变化很小，也会导致得分 \(z_i\) 发生巨大的变化。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;补充：Sigmoid 函数 \(\sigma(z) = \frac{1}{1 + e^{-z}}\) 具有以下特性：&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251118191312829.png&quot; style=&quot;zoom: 33%;&quot; /&gt;&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;&lt;strong&gt;\(z_i\) 的值&lt;/strong&gt;&lt;/th&gt;
        &lt;th&gt;&lt;strong&gt;\(y_i=\frac{1}{1 + e^{-z_i}}\) 的值&lt;/strong&gt;&lt;/th&gt;
        &lt;th&gt;&lt;strong&gt;\(y_i\) 接近&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;strong&gt;非常大的正数&lt;/strong&gt; (\(\gg 0\))&lt;/td&gt;
        &lt;td&gt;趋近于 $\frac{1}{1 + 0}$&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;strong&gt;非常大的负数&lt;/strong&gt; (\(\ll 0\))&lt;/td&gt;
        &lt;td&gt;趋近于 $\frac{1}{1 + \infty}$&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;接近 0&lt;/td&gt;
        &lt;td&gt;趋近于 $\frac{1}{1 + 1} = 0.5$&lt;/td&gt;
        &lt;td&gt;0.5&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;由于初始权重 \(w\) 很大，使得大多数 \(z_i\) 的绝对值 $$&lt;/td&gt;
      &lt;td&gt;z_i&lt;/td&gt;
      &lt;td&gt;$$ 非常大：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;如果 \(z_i\) 是一个&lt;strong&gt;非常大的正数&lt;/strong&gt;（例如，100），则 \(\hat{y}_i \approx 1\)。&lt;/li&gt;
  &lt;li&gt;如果 \(z_i\) 是一个&lt;strong&gt;非常大的负数&lt;/strong&gt;（例如，-100），则 \(\hat{y}_i \approx 0\)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此，&lt;strong&gt;当初始权重非常大时，模型对大部分样本的预测会非常自信地给出接近 0 或接近 1 的概率。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;当 \(\hat{y}_i\) 非常接近 0 或 1 时，Sigmoid 导数项 \(\hat{y}_i (1 - \hat{y}_i)\) 的值会变得&lt;strong&gt;极小&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;即使模型预测错误（即 \((\hat{y}_i - y_i)\) 不为 0），由于 \(\hat{y}_i (1 - \hat{y}_i)\) 接近 0，&lt;strong&gt;整个梯度也会趋于 0&lt;/strong&gt;。这就是&lt;strong&gt;梯度消失问题&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;即：若初始化W非常大，此时还没有正确分类，模型就已经不太能够学到东西了。&lt;/p&gt;

&lt;p&gt;而且其实就算不考虑极端的情况，本身\(f_i\)就很小，范围就是0到1，根据\(L_{MSE}== \frac{2}{N} \sum_{i} (\hat{y}_i - y_i) \cdot \underbrace{\hat{y}_i (1 - \hat{y}_i)}_{\text{Sigmoid 导数}} \cdot x_{i,j}\)，即\({\hat{y}_i (1 - \hat{y}_i)}\)本身就很小，梯度更新本身就很小了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251118193101978.png&quot; alt=&quot;image-20251118193101978&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;【2】逻辑回归若用MSE，其曲线并不是一个凸函数，可能存在多个局部极小值的点，即非凸的损失曲面意味着它可能存在多个“小山谷”或“小坑”，梯度下降算法可能会收敛到一个局部极小值点，这个点的损失值大于全局最小值。此时就无法找到损失值最小的最佳模型了。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;问：多选几个初始点w能不能避免  存在多个局部极小值的问题呢？&lt;/p&gt;

  &lt;p&gt;其实在机器学习的前沿领域，很多研究都是在讨论如何选初始点，这个策略叫做 &lt;strong&gt;Multi-start Optimization&lt;/strong&gt;（多点启动优化）。但是达不到很好的效果。&lt;/p&gt;

  &lt;p&gt;而且维度如果很大，鞍点（Saddle Points）的数量急剧增加，成为主要的优化障碍。鞍点在某些方向是最小值，但在其他方向是最大值。SGD 可能会在鞍点附近停滞，因为所有方向的梯度都接近零，这比局部极小值更难逃逸。&lt;/p&gt;

  &lt;p&gt;而且在高维空间中，很多“足够好”的局部极小值点的损失值&lt;strong&gt;非常接近&lt;/strong&gt;全局最优解。研究发现，许多局部极小值在泛化能力（即在测试集上的性能）上与全局最优解几乎没有区别。&lt;/p&gt;

  &lt;p&gt;在现代 ML 中，初始化研究的意义在于&lt;strong&gt;确保训练过程稳定、高效&lt;/strong&gt;，并引导模型找到&lt;strong&gt;泛化能力强&lt;/strong&gt;的局部最优解，而不是徒劳地去寻找理论上存在的、但在高维空间中难以捕捉的绝对全局最优解。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;4多分类任务-ovr&quot;&gt;4.多分类任务-OVR&lt;/h1&gt;

&lt;p&gt;为每一个类别训练一个二元分类器，叫做&lt;strong&gt;One-vs-Rest (OvR)&lt;/strong&gt;或者&lt;strong&gt;One-vs-All (OvA)&lt;/strong&gt;。具体逻辑如下：&lt;/p&gt;

&lt;p&gt;假设有 \(N\) 个类别，OvR 方法会训练 \(N\) 个独立的逻辑回归分类器：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;训练 \(N\) 个分类器：&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;为每个类别 \(k \in \{1, 2, \ldots, N\}\) 训练一个二元分类器 \(C_k\)。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;目标：&lt;/strong&gt; \(C_k\) 的任务是判断一个样本&lt;strong&gt;是否属于类别 \(k\)&lt;/strong&gt;，或者说，输出一个概率。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;构造数据集：&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;在训练 \(C_k\) 时，所有属于类别 $k$ 的样本被标记为&lt;strong&gt;正类（Positive, \(y=1\)）&lt;/strong&gt;。&lt;/li&gt;
      &lt;li&gt;所有不属于类别 \(k\) 的样本（即剩下的 \(N-1\) 个类别的样本）都被标记为&lt;strong&gt;负类（Negative, \(y=0\)）&lt;/strong&gt;。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;预测：&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;当一个新样本 \(\mathbf{x}\) 输入时，它会被馈送到所有的 \(N\) 个分类器 \(C_1, C_2, \ldots, C_N\) 中。&lt;/li&gt;
      &lt;li&gt;每个分类器 \(C_k\) 都会输出一个概率 \(P(\mathbf{x} \in \text{Class } k)\)。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;最终决策：&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;样本 \(\mathbf{x}\) 被最终分配给 输出概率最高的那个类别。&lt;/p&gt;

\[\text{Class}(\mathbf{x}) = \underset{k}{\operatorname{argmax}} \left( P(\mathbf{x} \in \text{Class } k) \right)\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;OvR概念简单，易于并行化；可以使用任何二元分类器（如 SVM、决策树等）。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;这种方法适合工程，具有一定的开闭原则。&lt;/p&gt;

&lt;p&gt;开闭原则是面向对象设计（OOD）的五大原则之一，它要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;“软件实体（类、模块、函数等）应该是对扩展开放的，对修改封闭的。”&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这意味着：当需要添加新功能（例如，增加一个新类别）时，应该通过&lt;strong&gt;扩展&lt;/strong&gt;现有代码来实现，而不是&lt;strong&gt;修改&lt;/strong&gt;已经稳定运行的代码。&lt;/p&gt;

&lt;p&gt;OvR对扩展开放是因为：如果需要从 10 个类别增加到 11 个类别（比如新增一个“短裤”类别），只需要&lt;strong&gt;训练和部署第 11 个独立的二元分类器 \(C_{11}\)&lt;/strong&gt;。你不需要触碰或重新训练那 10 个已经存在的、稳定运行的分类器 \(C_1\) 到 \(C_{10}\)。&lt;/p&gt;

&lt;p&gt;而如果用softmax回归，必须&lt;strong&gt;修改&lt;/strong&gt;模型的输出层（从 10 维改为 11 维），并且必须&lt;strong&gt;使用所有数据&lt;/strong&gt;从头重新训练整个模型，因为所有权重都相互关联。&lt;/p&gt;

&lt;p&gt;因此，在需要&lt;strong&gt;频繁增加新类别&lt;/strong&gt;、追求&lt;strong&gt;模块化&lt;/strong&gt;和&lt;strong&gt;服务高可用性&lt;/strong&gt;的工程实践中，OvR 策略（尤其是使用轻量级模型时）确实展现出更高的工程价值和更好的可扩展性。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;举例：更加形象的理解&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251119143435260.png&quot; alt=&quot;image-20251119143435260&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于上面的多分类任务，可以用下面的方式解决：&lt;/p&gt;

&lt;p&gt;第一：三角形的分类器&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251119143518462.png&quot; alt=&quot;image-20251119143518462&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二：叉叉的分类器&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251119143604594.png&quot; alt=&quot;image-20251119143604594&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第三：正方形的分类器&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251119143628491.png&quot; alt=&quot;image-20251119143628491&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5多分类任务-softmax回归&quot;&gt;5.多分类任务-softmax回归&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251120155712073.png&quot; alt=&quot;image-20251120155712073&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Softmax 回归可以看作是&lt;strong&gt;逻辑回归的推广&lt;/strong&gt;（从二分类推广到多分类）。&lt;/p&gt;

&lt;h2 id=&quot;51原理&quot;&gt;5.1原理&lt;/h2&gt;

&lt;p&gt;Softmax 回归是假设多项分布的，多项分布可以理解为二项分布的扩展。投硬币是二项分布，掷骰子是多项分布。&lt;/p&gt;

&lt;p&gt;多分类任务中，\(y\) 有多个可能的分类：\(y \in \{1, 2, 3, \ldots, k\}\)，&lt;/p&gt;

&lt;p&gt;每种分类对应的概率：\(\phi_1, \phi_2, \ldots, \phi_k\)。由于 \(\sum_{i=1}^{k} \phi_i = 1\)，所以一般用 \(k-1\) 个参数 \(\phi_1, \phi_2, \ldots, \phi_{k-1}\)。其中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
\[p(y = i; \phi) = \phi_i\]
  &lt;/li&gt;
  &lt;li&gt;
\[p(y = k; \phi) = 1 - \sum_{i=1}^{k-1} \phi_i\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了将多项分布表达为指数族分布，做以下工作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;定义 \(T(y) \in \mathbb{R}^{k-1}\) 它不再是一个数而是一个变量&lt;/li&gt;
&lt;/ul&gt;

\[\begin{aligned} T(1) = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, &amp;amp; T(2) = \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, T(3) = \begin{bmatrix} 0 \\ 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \ldots, \\ &amp;amp; T(k-1) = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \\ 0 \end{bmatrix}, T(k) = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \end{aligned}\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;引入指示函数：\(\mathbb{I}\{True\} = 1, \mathbb{I}\{False\} = 0\)&lt;/p&gt;

    &lt;p&gt;$E(T(y)_i) = p(y = i) = \phi_i$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;为什么要将多项分布表达为指数族分布&quot;&gt;为什么要将多项分布表达为指数族分布？&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;指数族分布 (Exponential Family Distribution)&lt;/strong&gt; 是统计学中一类重要的概率分布家族（包括正态分布、伯努利分布、泊松分布、伽马分布等）。&lt;/p&gt;

&lt;p&gt;要注意，学习过的线性回归中，变量服从&lt;strong&gt;高斯分布（正态分布）&lt;/strong&gt;，它属于指数族。&lt;strong&gt;逻辑回归&lt;/strong&gt;变量服从&lt;strong&gt;伯努利分布&lt;/strong&gt;，它也属于指数族。&lt;/p&gt;

&lt;p&gt;现在对于&lt;strong&gt;多分类问题&lt;/strong&gt;，它的输出是 \(k\) 种可能中的一种，服从&lt;strong&gt;多项分布&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;指数族分布是&lt;strong&gt;广义线性模型（GLM）&lt;/strong&gt; 这个“模型大厦”的基石。 GLM 提供了一个&lt;strong&gt;统一的建模流程&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;线性部分&lt;/strong&gt;（熟悉的 \(\theta^T x\)）：所有模型都用它来计算“分数”。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;连接函数&lt;/strong&gt;（把分数转成概率或输出）：这个函数是根据分布的性质自动推导出来的。
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;线性回归&lt;/strong&gt;：连接函数是“恒等”（分数就是输出）。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;：连接函数是 &lt;strong&gt;Sigmoid&lt;/strong&gt; 函数。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Softmax 回归&lt;/strong&gt;：连接函数是 &lt;strong&gt;Softmax&lt;/strong&gt; 函数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过这种统一，不需要为 Softmax 回归设计一套全新的理论，只是在 GLM 框架下，将随机分量从伯努利（二元）换成了多项（多分类）。&lt;/p&gt;

&lt;p&gt;另外，在线性回归中用 &lt;strong&gt;MSE&lt;/strong&gt;，在逻辑回归中用 &lt;strong&gt;BCE&lt;/strong&gt;。这两种损失函数其实都是&lt;strong&gt;最大似然估计（MLE）&lt;/strong&gt; 的结果。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;MSE&lt;/strong&gt; 是高斯分布下 MLE 的结果。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;BCE&lt;/strong&gt; 是伯努利分布下 MLE 的结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;指数族分布有一个极好的数学性质：当用 MLE 的方法来构建&lt;strong&gt;损失函数&lt;/strong&gt;时，这个损失函数（即对数似然函数）通常是&lt;strong&gt;凸函数&lt;/strong&gt;（或凹函数）。这保证了在训练 Softmax 模型时，使用梯度下降等优化算法能够&lt;strong&gt;稳定、快速地找到最佳的模型参数&lt;/strong&gt;，而不用担心陷入局部最优解。&lt;/p&gt;

&lt;p&gt;所有指数族分布都可以写成统一的规范形式：&lt;/p&gt;

\[p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta))\]

&lt;p&gt;其中，\(\eta\) 是自然参数（或规范参数），\(T(y)\) 是充分统计量，\(a(\eta)\) 是对数配分函数（用于确保概率之和为 1）。&lt;/p&gt;

&lt;p&gt;一旦将多项分布（或任何其他分布）写成这种形式，就可以利用指数族分布的通用性质来推导其&lt;strong&gt;连接函数&lt;/strong&gt;（link function）、&lt;strong&gt;均值和方差&lt;/strong&gt;等，无需为每种分布从头开始推导。&lt;/p&gt;

&lt;p&gt;解释：&lt;/p&gt;

&lt;p&gt;【1】自然参数是什么？在所有 GLM 模型中，自然参数 \(\eta\) 总是由输入特征的线性组合得到的：&lt;/p&gt;

\[\eta = \theta^T x\]

&lt;p&gt;可以把它理解为&lt;strong&gt;数据特征 \(x\) 经过线性组合后得到的“原始分数”&lt;/strong&gt;，这个分数直接决定了数据服从的概率分布的形状。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;\(η=θ^Tx\) 的含义&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;原始参数 (μ 或 ϕ)&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;期望本身&lt;/strong&gt;。 \(\eta\) 直接等于我们想要预测的连续值 \(\mu\)（假设 \(\sigma^2=1\)）。&lt;/td&gt;
      &lt;td&gt;均值 \(\mu\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Log-Odds&lt;/strong&gt;。 \(\eta\) 是用来衡量 \(\frac{P(y=1)}{P(y=0)}\) 这个比率的对数。&lt;/td&gt;
      &lt;td&gt;概率 \(\phi\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Softmax 回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Log-Odds Ratio&lt;/strong&gt;。 \(\eta_i\) 是衡量第 \(i\) 类相对于基准类 \(k\) 的对数几率比。\(\eta = \begin{bmatrix} \log(\phi_1/\phi_k) \\ \log(\phi_2/\phi_k) \\ \vdots \\ \log(\phi_{k-1}/\phi_k) \end{bmatrix}\)&lt;/td&gt;
      &lt;td&gt;概率向量 \(\phi\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;【2】什么是响应函数 (\(h(\cdot)\))?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;将模型的原始分数 (\(\eta\))，转化为我们真正想预测和解释的概率、均值或计数（\(\mu\)）&lt;/strong&gt;。&lt;/p&gt;

\[\mu = h(\eta)\]

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;自然参数 η&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;响应函数 μ=h(η)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;最终输出 μ&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(\eta = \theta^T x\)&lt;/td&gt;
      &lt;td&gt;\(\mu = \eta\) &lt;strong&gt;(恒等函数)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;均值 \(\mu\)&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(\eta = \text{Logit}(\phi)\)&lt;/td&gt;
      &lt;td&gt;\(\mu = \frac{e^\eta}{1+e^\eta}\) &lt;strong&gt;(Sigmoid 函数)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;概率 \(\phi\)&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Softmax 回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(\eta_i = \text{Log-Odds Ratio}\)&lt;/td&gt;
      &lt;td&gt;\(\mu_i = \frac{e^{\eta_i}}{\sum e^{\eta_j}}\) &lt;strong&gt;(Softmax 函数)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;概率 \(\phi_i\)&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;简单来说，如果把 \(\theta^T x\) 看作原始的“火力值”，响应函数 \(h(\cdot)\) 就是将这个火力值转化为&lt;strong&gt;实际命中目标的概率&lt;/strong&gt;（逻辑/Softmax 回归），或者&lt;strong&gt;实际的数值&lt;/strong&gt;（线性回归）。&lt;/p&gt;

&lt;h3 id=&quot;指数族分布&quot;&gt;指数族分布&lt;/h3&gt;

&lt;p&gt;所有指数族分布都可以写成统一的规范形式：&lt;/p&gt;

\[p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta))\]

&lt;p&gt;其中，\(b(y)\) 被称为残余项（Base Measure Term）或基测度，\(\eta\) 是自然参数（或规范参数），是一个&lt;strong&gt;只依赖于原分布参数&lt;/strong&gt;（如 \(\mu\) 和 \(\sigma^2\)）的函数，它&lt;strong&gt;不依赖于数据 $y$&lt;/strong&gt;。\(T(y)\) 是充分统计量，只依赖于数据 \(y\)。\(a(\eta)\) 是对数配分函数（用于确保概率之和为 1）。在广义线性模型 (GLM) 中，主要通过&lt;strong&gt;最大似然估计（MLE）&lt;/strong&gt; 来学习参数 \(\theta\)（它隐藏在 \(\eta\) 中）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;\(b(y)\) 一般包含原始概率分布函数（PDF 或 PMF）中所有与 \(y\) 相关但与模型参数 \(\eta\) 无关的项。&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;对于连续分布（如高斯分布），\(b(y)\) 通常包含像 \(\frac{1}{\sqrt{2\pi}}\) 或 \(\exp(-\frac{y^2}{2\sigma^2})\) 这样的因子。&lt;/li&gt;
    &lt;li&gt;对于某些离散分布（如伯努利分布），\(b(y)\) 可能只等于 \(1\)。对于泊松分布，它可能包含 \(\frac{1}{y!}\)。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;以线性回归遵循的高斯分布为例&lt;/strong&gt;，通常假设响应变量 \(y\) 服从均值为 \(\mu\)、方差为 \(\sigma^2\) 的高斯分布，即 \(y \sim N(\mu, \sigma^2)\)。为了简化，在 GLM 的推导中，我们通常假设 \(\sigma^2\) 是固定的常数（或为 1）。&lt;/p&gt;

&lt;p&gt;高斯分布的概率密度函数 (PDF) 为：&lt;/p&gt;

\[p(y; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right)\]

&lt;p&gt;对上式进行代数展开，并&lt;strong&gt;将 \(\mu\) 作为参数&lt;/strong&gt;：&lt;/p&gt;

\[\begin{aligned} p(y; \mu) &amp;amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(y^2 - 2y\mu + \mu^2)\right) \\ &amp;amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{y^2}{2\sigma^2}\right) \cdot \exp\left(\frac{2y\mu}{2\sigma^2} - \frac{\mu^2}{2\sigma^2}\right) \\ &amp;amp;= \left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{y^2}{2\sigma^2}\right)\right) \cdot \exp\left(\left(\frac{\mu}{\sigma^2}\right)y - \left(\frac{\mu^2}{2\sigma^2}\right)\right)\end{aligned}\]

&lt;p&gt;通过与规范形式 \(p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta))\) 对比，我们可以确定各部分：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;strong&gt;充分统计量\(T(y)\)&lt;/strong&gt;只依赖于数据 \(y\)。而\(\eta\) 是一个&lt;strong&gt;只依赖于原分布参数&lt;/strong&gt;（如 \(\mu\) 和 \(\sigma^2\)）的函数，它&lt;strong&gt;不依赖于数据 \(y\)&lt;/strong&gt;。所以\(T(y) = y\)，\(\eta = \frac{\mu}{\sigma^2}\)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;部分&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;高斯分布 (N(μ,σ2)) 的对应项&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;充分统计量 \(T(y)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(T(y) = y\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;自然参数 \(\eta\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(\eta = \frac{\mu}{\sigma^2}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;对数配分函数 \(a(\eta)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(a(\eta) = \frac{\mu^2}{2\sigma^2} = \frac{(\eta\sigma^2)^2}{2\sigma^2} = \frac{1}{2}\sigma^2 \eta^2\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;残余项 \(b(y)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(b(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{y^2}{2\sigma^2}\right)\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;自然参数 \(\eta\)​ 和均值 \(\mu\)​ 的关系是 \(\mu = \sigma^2 \eta\)​。在高斯分布的标准 GLM 中，通常假设方差 \(\sigma^2\) 是常数，并且为了简化，我们常取 \(\sigma^2 = 1\)。&lt;/p&gt;

\[\text{如果}\ \sigma^2 = 1\ \text{，则}\ \eta = \frac{\mu}{1} = \mu\]

&lt;p&gt;所以，只有在这种特殊情况下（方差为 1），高斯分布的&lt;strong&gt;规范连接函数&lt;/strong&gt;才是 \(\eta = \mu\)（即恒等连接）。&lt;/p&gt;

&lt;p&gt;所以，在标准 GLM 中，响应函数就是 \(\mu = h(\eta) = \eta\)，这正是&lt;strong&gt;线性回归&lt;/strong&gt;的连接函数（恒等连接）。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;再说逻辑回归（伯努利分布）：响应变量 \(y\) 是二元的（0 或 1），服从均值为 \(\phi\) 的伯努利分布，即 \(y \sim \text{Bernoulli}(\phi)\)。&lt;/p&gt;

&lt;p&gt;伯努利分布的概率质量函数 (PMF) 为：&lt;/p&gt;

\[p(y; \phi) = \phi^y (1 - \phi)^{1-y}\]

&lt;p&gt;转化为指数族规范形式&lt;/p&gt;

&lt;p&gt;对上式进行代数重写：&lt;/p&gt;

\[\begin{aligned} p(y; \phi) &amp;amp;= \exp\left( \log\left(\phi^y (1 - \phi)^{1-y}\right) \right) \\ &amp;amp;= \exp\left( y \log(\phi) + (1-y) \log(1 - \phi) \right) \\ &amp;amp;= \exp\left( y \log(\phi) - y \log(1 - \phi) + \log(1 - \phi) \right) \\ &amp;amp;= \exp\left( y \log\left(\frac{\phi}{1 - \phi}\right) + \log(1 - \phi) \right)\end{aligned}\]

&lt;p&gt;通过与规范形式 $p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta))$ 对比，我们可以确定各部分：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;部分&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;伯努利分布 (Bernoulli(ϕ)) 的对应项&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;充分统计量 \(T(y)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(T(y) = y\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;自然参数 \(\eta\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(\eta = \log\left(\frac{\phi}{1 - \phi}\right)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;对数配分函数 \(a(\eta)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(a(\eta) = -\log(1 - \phi) = \log\left(\frac{1}{1 - \phi}\right)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;残余项 \(b(y)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(b(y) = 1\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;自然参数 \(\eta\) 与均值 \(\phi\) 的关系（连接函数）：&lt;/p&gt;

\[\eta = \log\left(\frac{\phi}{1 - \phi}\right)\]

    &lt;p&gt;这就是著名的 Log-Odds 或 Logit 函数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;响应函数（反向）：&lt;/p&gt;

    &lt;p&gt;将 \(\phi\) 解出来：&lt;/p&gt;

\[e^\eta = \frac{\phi}{1 - \phi} \implies e^\eta (1 - \phi) = \phi \implies e^\eta - \phi e^\eta = \phi\]

\[e^\eta = \phi (1 + e^\eta) \implies \phi = \frac{e^\eta}{1 + e^\eta}\]

    &lt;p&gt;这就是 Sigmoid 函数！&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;关键点：&lt;/strong&gt; &lt;strong&gt;逻辑回归&lt;/strong&gt;的理论基础，正是通过将伯努利分布转化为指数族规范形式，自动推导出了 \(\phi = \frac{e^\eta}{1 + e^\eta}\) 这个 Sigmoid 函数作为它的&lt;strong&gt;响应函数&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过这两个例子，可以看到，指数族分布的规范形式确实是&lt;strong&gt;统一各种回归模型的理论框架&lt;/strong&gt;。对于多项分布（Softmax 回归），其过程与伯努利分布的推导非常相似，最终会自动推导出 Softmax 函数作为它的响应函数。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;下面就是softmax回归的推导：&lt;/p&gt;

&lt;p&gt;为了将多项分布纳入广义线性模型 (GLM) 框架，通常需要将其写成指数族分布的标准形式：&lt;/p&gt;

\[p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta))\]

&lt;p&gt;在多分类问题中，单个数值 \(y\) 不足以直接作为 \(T(y)\)。因此，引入一个 \((k-1)\) 维的向量 $T(y)$，采用 &lt;strong&gt;“One-Hot Encoding”&lt;/strong&gt;（独热编码）的思想：&lt;/p&gt;

&lt;p&gt;如果 \(y=i\) (对于 \(i=1, \ldots, k-1\))，则 $T(y)$ 的第 \(i\) 个分量是1，其余分量是 0。即：&lt;/p&gt;

\[\begin{aligned} T(1) = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, &amp;amp; T(2) = \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, T(3) = \begin{bmatrix} 0 \\ 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \ldots, \\ &amp;amp; T(k-1) = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \\ 0 \end{bmatrix}, T(k) = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \end{aligned}\]

&lt;p&gt;如果 \(y=k\)，则 \(T(k)\) 是一个全零向量 \(\mathbf{0}\)。&lt;/p&gt;

&lt;p&gt;另外，引入指示函数和期望，&lt;strong&gt;指示函数&lt;/strong&gt; \(\mathbb{I}\{\cdot\}\) 用于将分类结果转化为数学表达式。&lt;/p&gt;

&lt;p&gt;\(\mathbb{I}\{y=i\}\) 的值：如果分类结果是 \(i\)，则为1；否则为0。&lt;/p&gt;

&lt;p&gt;期望 \(E(T(y)_i)\)：&lt;/p&gt;

&lt;p&gt;\(T(y)_i\) 代表向量 \(T(y)\) 的第 $i$ 个分量。&lt;/p&gt;

\[E(T(y)_i) = \sum_{y \in \{1, \ldots, k\}} T(y)_i \cdot p(y) = 1 \cdot p(y=i) + 0 \cdot p(y \ne i) = p(y=i)\]

&lt;p&gt;这验证了 \(T(y)\) 向量的第 \(i\) 个分量的期望，正是第 \(i\) 类发生的概率 \(\phi_i\)（对于 \(i=1, \ldots, k-1\)）。在 Softmax 回归的推导中，这个 \(E(T(y))\) 会被设为 \(\phi\)，并与线性预测 \(\eta\) 通过连接函数联系起来。&lt;/p&gt;

&lt;p&gt;多项分布 PMF（使用指示函数）：&lt;/p&gt;

\[p(y; \phi) = \phi_1^{\mathbb{I}\{y=1\}} \phi_2^{\mathbb{I}\{y=2\}} \cdots \phi_k^{\mathbb{I}\{y=k\}}\]

&lt;p&gt;其中，\(\mathbb{I}\{y=i\}\) 是指示函数，只有当 \(y=i\) 时取 1，否则取 0。&lt;/p&gt;

&lt;p&gt;由于 \(\phi_k = 1 - \sum_{i=1}^{k-1} \phi_i\)，且指示函数的和 \(\sum_{i=1}^{k} \mathbb{I}\{y=i\} = 1\)，所以 \(\mathbb{I}\{y=k\} = 1 - \sum_{i=1}^{k-1} \mathbb{I}\{y=i\}\)。&lt;/p&gt;

\[p(y; \phi) = \phi_1^{\mathbb{I}\{y=1\}} \phi_2^{\mathbb{I}\{y=2\}} \cdots \phi_k^{1 - \sum_{i=1}^{k-1} \mathbb{I}\{y=i\}}\]

&lt;p&gt;即\(p(y; \phi) = \phi_1^{T(y)_1} \phi_2^{T(y)_2} \cdots \phi_k^{1 - \sum_{i=1}^{k-1} T(y)_i}\)&lt;/p&gt;

&lt;p&gt;取对数并写成 \(\exp(\cdot)\) 形式：&lt;/p&gt;

\[\begin{aligned} p(y; \phi) &amp;amp;= \exp\left( T(y)_1 \log(\phi_1) + T(y)_2 \log(\phi_2) + \cdots + \left(1 - \sum_{i=1}^{k-1} T(y)_i\right) \log(\phi_k) \right) \\ &amp;amp;= \exp\left( \sum_{i=1}^{k-1} T(y)_i \log(\phi_i) + \log(\phi_k) - \sum_{i=1}^{k-1} T(y)_i \log(\phi_k) \right)\end{aligned}\]

&lt;p&gt;整理成指数族形式（引入 \(\phi_k\) 作为分母）：将所有 \(T(y)_i\) 对应的项归类，并提取 \(\log(\phi_k)\)：&lt;/p&gt;

\[\begin{aligned} p(y; \phi) &amp;amp;= \exp\left( \sum_{i=1}^{k-1} T(y)_i \left(\log(\phi_i) - \log(\phi_k)\right) + \log(\phi_k) \right) \\ &amp;amp;= \exp\left( \sum_{i=1}^{k-1} T(y)_i \log\left(\frac{\phi_i}{\phi_k}\right) + \log(\phi_k) \right) \\ &amp;amp;= \exp\left( \begin{bmatrix} T(y)_1 \\ \vdots \\ T(y)_{k-1} \end{bmatrix}^T \begin{bmatrix} \log(\phi_1/\phi_k) \\ \vdots \\ \log(\phi_{k-1}/\phi_k) \end{bmatrix} - (-\log(\phi_k)) \right)\end{aligned}\]

&lt;p&gt;与指数族规范形式 \(p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta))\) 进行比对，得到最终的模型参数：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;多项分布 (Softmax) 的对应项&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;解释&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;\(T(y)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(T(y) = \begin{bmatrix} \mathbb{I}\{y=1\} \\ \mathbb{I}\{y=2\} \\ \vdots \\ \mathbb{I}\{y=k-1\} \end{bmatrix}\)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;充分统计量&lt;/strong&gt;，一个 \((k-1)\) 维的独热编码向量。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;\(\eta\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(\eta = \begin{bmatrix} \log(\phi_1/\phi_k) \\ \log(\phi_2/\phi_k) \\ \vdots \\ \log(\phi_{k-1}/\phi_k) \end{bmatrix}\)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;自然参数&lt;/strong&gt;，一个 \((k-1)\) 维向量，每个分量是 Log-Odds Ratio。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;\(a(\eta)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(a(\eta) = -\log(\phi_k)\)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;对数配分函数&lt;/strong&gt;，确保概率总和为 1。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;\(b(y)\)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(b(y) = 1\)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;残余项&lt;/strong&gt;。对于多项分布（没有阶乘项），\(b(y)\) 恒为 1。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;softmax-回归的核心理论基础&quot;&gt;&lt;strong&gt;Softmax 回归&lt;/strong&gt;的核心理论基础&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;自然参数 \(\eta\) 即 Log-Odds Ratio&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;自然参数 \(\eta_i\) 被定义为第 \(i\) 类相对于基准类 \(k\) 的对数几率比 (Log-Odds Ratio)：&lt;/p&gt;

\[\eta_i = \log\left(\frac{\phi_i}{\phi_k}\right)\]

&lt;ol&gt;
  &lt;li&gt;推导出 Softmax 函数（响应函数）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从 \(\eta\) 的定义，我们可以反推出 Softmax 函数 \(\phi\)：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;步骤 1： 对 \(\eta_i\) 取指数：&lt;/p&gt;

\[e^{\eta_i} = \frac{\phi_i}{\phi_k} \quad \implies \quad \phi_i = \phi_k e^{\eta_i} \quad (\text{for } i=1, \ldots, k-1)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;步骤 2：&lt;/strong&gt; 第 \(k\) 类可以定义为 \(\phi_k = \phi_k e^{\eta_k}\)，其中 \(\eta_k\) 被隐含地设定为 \(0\)。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;步骤 3： 利用概率总和为 1 的约束：\(\sum_{j=1}^{k} \phi_j = 1\)&lt;/p&gt;

\[\sum_{j=1}^{k} \phi_j = \sum_{j=1}^{k} \phi_k e^{\eta_j} = \phi_k \sum_{j=1}^{k} e^{\eta_j} = 1\]

    &lt;p&gt;解出 \(\phi_k\)：&lt;/p&gt;

\[\phi_k = \frac{1}{\sum_{j=1}^{k} e^{\eta_j}}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;步骤 4： 将 \(\phi_k\) 代回 \(\phi_i\) 的表达式，即将\(\phi_i = \phi_k e^{\eta_i} \quad (\text{for } i=1, \ldots, k-1)\)带入求和项&lt;/p&gt;

    &lt;p&gt;\(\sum_{j=1}^{k-1} \phi_j + \phi_k = 1\)中，得到\(\sum_{j=1}^{k-1} (\phi_k e^{\eta_j}) + \phi_k = 1\)，从等式左边提取公因式 \(\phi_k\)​：\(\phi_k \left( \sum_{j=1}^{k-1} e^{\eta_j} + 1 \right) = 1\)&lt;/p&gt;

    &lt;p&gt;将括号里的项重新写成一个包含 \(k\) 个类别的求和：(注意，这里的 \(+1\) 其实代表了 \(e^{\eta_k}\)，因为我们隐含地设定了 \(\eta_k = 0\) (\(e^0 = 1\))。)&lt;/p&gt;

\[\phi_k \left( \sum_{j=1}^{k} e^{\eta_j} \right) = 1\]

    &lt;p&gt;现在，解出基准类别 $\phi_k$ 的表达式：&lt;/p&gt;

\[\phi_k = \frac{1}{\sum_{j=1}^{k} e^{\eta_j}} \quad\]

    &lt;p&gt;最后一步，我们回到关系式 （$\phi_i = \phi_k e^{\eta_i}$），将我们刚刚解出的表达式  代入 \(\phi_k\) 的位置：&lt;/p&gt;

    &lt;p&gt;得到著名的 Softmax 函数：&lt;/p&gt;

\[\phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^{k} e^{\eta_j}}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这一步是标准的归一化技巧：先用基准项（\(\phi_k\)）表示所有其他项（\(\phi_i\)），然后利用所有项的和等于 1 的约束，解出基准项，最后代回得到通用公式。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Log-Odds Ratio（对数几率比）&lt;/strong&gt; 是 Softmax 回归（和 Logit 回归）中 \(\eta\) 的核心意义。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Odds (几率)：&lt;/strong&gt; \(\frac{P(\text{事件发生})}{P(\text{事件不发生})} = \frac{\phi}{1-\phi}\)。几率告诉我们事件发生的可能性是不发生的可能性的多少倍。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Log-Odds (对数几率)：&lt;/strong&gt; \(\log(\frac{\phi}{1-\phi})\)。取对数是为了将范围 \([0, \infty)\) 映射到 \((-\infty, \infty)\)，从而可以与线性模型 \(\theta^T x\) 相匹配。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 Softmax 回归 中，面对 \(k\) 个类别，用的是 Ratio (比率)：&lt;/p&gt;

\[\eta_i = \log\left(\frac{\phi_i}{\phi_k}\right)\]

&lt;p&gt;这意味着：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;不再与“不发生”作比较，而是与&lt;strong&gt;一个选定的基准类别 \(k\)&lt;/strong&gt; 作比较。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;特殊意义：&lt;/strong&gt; \(\eta_i\) 的数值直接告诉我们，&lt;strong&gt;第 \(i\) 类发生的对数几率，相对于第 \(k\) 类发生的对数几率，高出了多少。&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果 \(\theta_i\) 是特征 \(x_j\) 对应的模型系数，那么 \(\theta_i\) 代表着：&lt;strong&gt;当 \(x_j\) 增加 1 个单位时，第 \(i\) 类相对于基准类 \(k\) 的对数几率比 \(\eta_i\) 增加 \(\theta_i\) 个单位。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这种 Log-Odds Ratio 的形式，在统计学上是处理多项分布参数的最自然、最简洁的方式，也是确保 Softmax 模型能够被完美纳入指数族和 GLM 框架的关键。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;注意：\(\eta_k = \log\left(\frac{\phi_k}{\phi_k}\right) = \log(1) = 0\)&lt;/p&gt;

&lt;p&gt;将 \(\eta_k\) 设为 \(0\) 意味着&lt;strong&gt;第 \(k\) 类被选定为所有比较的基准类别&lt;/strong&gt;。所有其他的线性预测器 \(\eta_1, \ldots, \eta_{k-1}\) 都被解释为相对于 \(\eta_k\) 的差异。&lt;/p&gt;

&lt;p&gt;这种设置与处理多项分布参数时使用 \(k-1\) 个参数（因为 \(\sum \phi_i = 1\)）的，最终只需要学习 \(k-1\) 组系数向量 \(\theta_1, \ldots, \theta_{k-1}\)。第 \(k\) 组系数 \(\theta_k\) 被隐含地设定为&lt;strong&gt;零向量&lt;/strong&gt;（或被吸收进截距项）。&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;概率表示： Softmax 回归预测的是在给定输入 \(x\) 和模型参数 \(\theta\) 的条件下，输出为类别 \(i\) 的概率 $$p(y=i&lt;/td&gt;
      &lt;td&gt;x; \theta)\(，记为\)\phi_i$$。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[p(y = i | x; \theta) = \phi_i\]

&lt;p&gt;响应函数： 根据指数族分布的推导（我们之前讨论过），概率 \(\phi_i\) 由自然参数 \(\eta_i\) 经 Softmax 函数转换得到：&lt;/p&gt;

\[\phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^{k} e^{\eta_j}}\]

&lt;p&gt;线性预测子： Softmax 回归作为广义线性模型（GLM），其自然参数 \(\eta_i\) 由输入 \(x\) 的线性组合构成：&lt;/p&gt;

\[\eta_i = \theta_i^T x\]

&lt;p&gt;注意：这里 \(\theta_i\) 是一个与类别 \(i\) 相关的参数向量，整个模型有 \(k\) 个这样的向量 \(\theta_1, \theta_2, \ldots, \theta_k\)。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最终假设函数:&lt;/strong&gt; 将线性预测子代入响应函数，得到了 Softmax 回归的最终假设函数 \(h_{\theta}(x)\)，它输出一个 \(k\) 维的概率向量：&lt;/p&gt;

\[h_{\theta}(x) = \begin{cases} P(y=1|x) = \frac{e^{\theta_1^T x}}{\sum_{j=1}^{k} e^{\theta_j^T x}}, &amp;amp; y=1 \\ P(y=2|x) = \frac{e^{\theta_2^T x}}{\sum_{j=1}^{k} e^{\theta_j^T x}}, &amp;amp; y=2 \\ \vdots \\ P(y=k|x) = \frac{e^{\theta_k^T x}}{\sum_{j=1}^{k} e^{\theta_j^T x}}, &amp;amp; y=k \end{cases}\]

&lt;p&gt;&lt;strong&gt;\(h_{\theta}(x)\) 的输出&lt;/strong&gt; 是一个向量 \([\phi_1, \phi_2, \ldots, \phi_k]^T\)，其中 \(\sum_{i=1}^k \phi_i = 1\)，且 \(0 \le \phi_i \le 1\)。&lt;/p&gt;

&lt;p&gt;Softmax 是一个&lt;strong&gt;归一化指数函数&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251120141218434.png&quot; alt=&quot;image-20251120141218434&quot; /&gt;&lt;/p&gt;

&lt;p&gt;假设我们有 \(k=3\) 个类别，输入 \(x\) 经过线性预测（即 \(\eta_i = \theta_i^T x\)）后，得到了 \(k=3\) 个原始分数 \(z_1, z_2, z_3\)：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;步骤&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;原始分数 (自然参数 η)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;转换操作&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;结果值 (指数项 eη)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;最终概率 ϕi&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;1.&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(z_1 = 3\)&lt;/td&gt;
      &lt;td&gt;\(e^{z_1}\)&lt;/td&gt;
      &lt;td&gt;\(e^3 \approx 20\)&lt;/td&gt;
      &lt;td&gt;\(y_1 = \frac{20}{23.75} \approx 0.88\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;2.&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(z_2 = 1\)&lt;/td&gt;
      &lt;td&gt;\(e^{z_2}\)&lt;/td&gt;
      &lt;td&gt;\(e^1 \approx 2.7\)&lt;/td&gt;
      &lt;td&gt;\(y_2 = \frac{2.7}{23.75} \approx 0.12\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;3.&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;\(z_3 = -3\)&lt;/td&gt;
      &lt;td&gt;\(e^{z_3}\)&lt;/td&gt;
      &lt;td&gt;\(e^{-3} \approx 0.05\)&lt;/td&gt;
      &lt;td&gt;\(y_3 = \frac{0.05}{23.75} \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Softmax 机制:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.&lt;strong&gt;指数化 (Exponentiation):&lt;/strong&gt; 对每个原始分数 \(z_i\)（即 \(\eta_i\)）取指数 \(e^{z_i}\)。将分数映射到 \((0, \infty)\) 的范围内，确保输出的概率是非负的。同时，指数函数会&lt;strong&gt;放大分数之间的差异&lt;/strong&gt;，分数越高，其指数值增长得越快。&lt;/li&gt;
  &lt;li&gt;2.&lt;strong&gt;求和 (Normalization Term):&lt;/strong&gt; 计算所有指数项的和 \(\sum_{j=1}^3 e^{z_j}\)，确保所有概率之和为 1 的&lt;strong&gt;归一化因子&lt;/strong&gt;（对应于指数族中的 \(e^{a(\eta)}\)）。&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;3.&lt;strong&gt;归一化 (Division):&lt;/strong&gt; 将每个指数项除以总和，得到最终的概率 $$y_i = P(C_i&lt;/td&gt;
          &lt;td&gt;x)$$。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;softmax输出的是概率分布，但其名称“Softmax”来源于它是一个&lt;strong&gt;平滑的、可微分的“argmax”函数&lt;/strong&gt;的近似。它会给最大的输入分数（比如上面的\(z_1=3\)）分配一个&lt;strong&gt;远大于&lt;/strong&gt;其他分数的概率（\(0.88\)），从而&lt;strong&gt;放大最大值&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;多分类交叉熵损失&quot;&gt;多分类交叉熵损失&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251120143158820.png&quot; alt=&quot;image-20251120143158820&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Softmax 回归的损失函数是通过最大化数据样本的&lt;strong&gt;对数似然函数&lt;/strong&gt;推导出来的，这个函数就是著名的&lt;strong&gt;多分类交叉熵损失 (Multiclass Cross-Entropy Loss)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;【1.似然函数 (Likelihood Function)】&lt;/p&gt;

&lt;p&gt;假设有一个包含 \(m\) 个独立同分布 (i.i.d.) 训练样本的数据集 \(\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})\}\), 其中 \(y^{(i)} \in \{1, \ldots, k\}\)。&lt;/p&gt;

&lt;p&gt;模型的参数是 \(\theta = \{\theta_1, \ldots, \theta_k\}\)。&lt;/p&gt;

&lt;p&gt;似然函数 \(\mathcal{L}(\theta)\) 是观察到整个数据集的概率，根据独立性原则，它是每个样本概率的乘积：&lt;/p&gt;

\[\mathcal{L}(\theta) = P(y^{(1)}, \ldots, y^{(m)} | x^{(1)}, \ldots, x^{(m)}; \theta) = \prod_{i=1}^{m} P(y^{(i)} | x^{(i)}; \theta)\]

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;对于单个样本 \((x, y)\)，我们知道 $$P(y=j&lt;/td&gt;
      &lt;td&gt;x) = \phi_j\(。由于\)y\(只有一个取值，我们可以使用我们之前定义的指示函数\)\mathbb{I}{y=j}\(或充分统计量\)T(y)_j$$ 来简洁地表示这个概率：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[P(y | x; \theta) = \prod_{j=1}^{k} \left( \phi_j \right)^{\mathbb{I}\{y=j\}}\]

&lt;p&gt;其中 \(\phi_j\) 是 Softmax 函数：\(\phi_j = \frac{e^{\theta_j^T x}}{\sum_{l=1}^{k} e^{\theta_l^T x}}\)。&lt;/p&gt;

&lt;p&gt;将这个概率代入似然函数：&lt;/p&gt;

\[\mathcal{L}(\theta) = \prod_{i=1}^{m} \left[ \prod_{j=1}^{k} \left( \phi_j^{(i)} \right)^{\mathbb{I}\{y^{(i)}=j\}} \right]\]

&lt;p&gt;【2.对数似然函数 (Log-Likelihood Function)】&lt;/p&gt;

&lt;p&gt;为了简化计算（将乘积转化为求和）并利用指数族分布的优点，我们取对数似然 \(\ell(\theta) = \log \mathcal{L}(\theta)\)：&lt;/p&gt;

\[\ell(\theta) = \log \left( \prod_{i=1}^{m} \prod_{j=1}^{k} \left( \phi_j^{(i)} \right)^{\mathbb{I}\{y^{(i)}=j\}} \right)\]

\[\ell(\theta) = \sum_{i=1}^{m} \sum_{j=1}^{k} \mathbb{I}\{y^{(i)}=j\} \log\left( \phi_j^{(i)} \right)\]

&lt;p&gt;【3.损失函数 (Loss Function)】&lt;/p&gt;

&lt;p&gt;在机器学习中，我们通常采用&lt;strong&gt;最小化损失函数&lt;/strong&gt;的方式来训练模型，而不是最大化对数似然函数。&lt;/p&gt;

&lt;p&gt;损失函数 \(J(\theta)\) 被定义为负的平均对数似然函数：&lt;/p&gt;

\[J(\theta) = - \frac{1}{m} \ell(\theta) = - \frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{k} \mathbb{I}\{y^{(i)}=j\} \log\left( \phi_j^{(i)} \right)\]

&lt;p&gt;【4.交叉熵的本质】&lt;/p&gt;

&lt;p&gt;这个函数 \(J(\theta)\) 就是&lt;strong&gt;多分类交叉熵损失 (Multiclass Cross-Entropy Loss)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;为什么叫交叉熵？在信息论中，交叉熵 \(H(p, q)\) 衡量的是用分布 \(q\)（模型预测的概率 \(\phi\)）来编码分布 \(p\)（真实标签 \(y\) 的概率分布）所需要的平均比特数。&lt;/p&gt;

&lt;p&gt;真实标签 \(y\) 的概率分布 \(p\) 是一个独热编码的分布，例如，如果真实标签是类别 3，那么 \(p\) 就是 \([0, 0, 1, 0, \ldots]^T\)。&lt;/p&gt;

&lt;p&gt;对于一个样本 \(i\)，其真实概率分布 \(p^{(i)}\) 是 \(p_j^{(i)} = \mathbb{I}\{y^{(i)}=j\}\)。&lt;/p&gt;

&lt;p&gt;样本 \(i\) 的交叉熵损失为：&lt;/p&gt;

\[J^{(i)}(\theta) = - \sum_{j=1}^{k} p_j^{(i)} \log\left( \phi_j^{(i)} \right)\]

&lt;p&gt;由于 \(p_j^{(i)}\) 只有在 \(j\) 等于真实标签 \(y^{(i)}\) 时才为1，其余为0 ，所以上式简化为：&lt;/p&gt;

\[J^{(i)}(\theta) = - \log\left( \phi_{y^{(i)}}^{(i)} \right)\]

&lt;p&gt;其中&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;\(\phi_{y^{(i)}}^{(i)}\)&lt;/strong&gt; 是模型预测的&lt;strong&gt;正确类别&lt;/strong&gt;的概率。&lt;/li&gt;
  &lt;li&gt;\(\log(\phi_{y^{(i)}}^{(i)})\) 随着正确概率的增大而增大（但仍为负数）。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;最小化&lt;/strong&gt; \(J(\theta)\) 等同于&lt;strong&gt;最大化&lt;/strong&gt;正确类别的预测概率 \(\phi_{y^{(i)}}^{(i)}\)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最大化多项分布的对数似然，等价于最小化&lt;strong&gt;多分类交叉熵损失&lt;/strong&gt;。这一损失函数具有&lt;strong&gt;凸性&lt;/strong&gt;（由于指数族分布的良好性质），保证了模型可以通过梯度下降等优化算法稳定地找到全局最优参数。&lt;/p&gt;

&lt;h3 id=&quot;损失函数的梯度&quot;&gt;损失函数的梯度&lt;/h3&gt;

&lt;p&gt;现在来推导 Softmax 回归损失函数（多分类交叉熵）相对于模型参数 \(\theta_j\) 的梯度。这个梯度是训练模型时使用梯度下降法的核心。&lt;/p&gt;

&lt;p&gt;为了简洁，我们只推导&lt;strong&gt;单个样本&lt;/strong&gt;的损失函数 \(J^{(i)}(\theta)\) 相对于&lt;strong&gt;某个特定类别 \(j\) 的参数向量 \(\theta_j\)&lt;/strong&gt; 的梯度。&lt;/p&gt;

&lt;p&gt;单个样本 \((x, y)\) 的损失函数（负对数似然）为：&lt;/p&gt;

\[J(\theta) = - \log\left( \phi_{y} \right)\]

&lt;p&gt;其中，\(\phi_{y}\) 是模型对真实类别 \(y\) 预测的概率。&lt;/p&gt;

&lt;p&gt;\(\phi_j\) 是 Softmax 函数：&lt;/p&gt;

\[\phi_j = \frac{e^{\eta_j}}{\sum_{l=1}^{k} e^{\eta_l}}\]

&lt;p&gt;并且 \(\eta_j = \theta_j^T x\)。&lt;/p&gt;

&lt;p&gt;我们的目标是计算损失函数 \(J(\theta)\) 对第 \(j\) 个类别的参数向量 \(\theta_j\) 的偏导数：&lt;/p&gt;

\[\nabla_{\theta_j} J(\theta) = \frac{\partial J(\theta)}{\partial \theta_j}\]

&lt;hr /&gt;

&lt;p&gt;\(J(\theta)\) 是关于 \(\phi_y\) 的函数，\(\phi_y\) 是关于 \(\eta_l\) 的函数，而 \(\eta_l\) 是关于 \(\theta_j\) 的函数，我们需要使用链式法则：&lt;/p&gt;

\[\frac{\partial J(\theta)}{\partial \theta_j} = \sum_{l=1}^{k} \left( \frac{\partial J(\theta)}{\partial \phi_l} \cdot \frac{\partial \phi_l}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j} \right)\]

&lt;p&gt;\(\frac{\partial J(\theta)}{\partial \phi_l}\) 只有在 \(l=y\) 时非零，所以简化为：&lt;/p&gt;

\[\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial J(\theta)}{\partial \phi_y} \cdot \frac{\partial \phi_y}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j}\]

&lt;p&gt;然后计算各部分偏导数：&lt;/p&gt;

&lt;p&gt;A. \(\frac{\partial J(\theta)}{\partial \phi_y}\) (损失函数对概率的导数)&lt;/p&gt;

\[J(\theta) = - \log(\phi_y)\]

\[\frac{\partial J(\theta)}{\partial \phi_y} = \frac{\partial (-\log(\phi_y))}{\partial \phi_y} = - \frac{1}{\phi_y}\]

&lt;p&gt;B. \(\frac{\partial \eta_j}{\partial \theta_j}\) (自然参数对参数的导数)&lt;/p&gt;

\[\eta_j = \theta_j^T x\]

\[\frac{\partial \eta_j}{\partial \theta_j} = x\]

&lt;p&gt;(注意 \(\frac{\partial}{\partial \theta_j}\) 意味着对向量 \(\theta_j\) 求梯度，结果是向量 \(x\)。)&lt;/p&gt;

&lt;p&gt;C. \(\frac{\partial \phi_y}{\partial \eta_j}\) (Softmax 对原始分数的导数)&lt;/p&gt;

&lt;p&gt;这是最复杂的一步，需要根据 \(j\) 是否等于真实类别 \(y\) 进行分情况讨论：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Case 1: \(j = y\) (计算 \(\phi_y\) 对 \(\eta_y\) 的导数)&lt;/strong&gt;&lt;/p&gt;

\[\phi_y = \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}}\]

&lt;p&gt;使用商法则 \(\left(\frac{u}{v}\right)&apos; = \frac{u&apos;v - uv&apos;}{v^2}\)，其中 \(u = e^{\eta_y}\)，\(v = \sum_{l=1}^{k} e^{\eta_l}\)。&lt;/p&gt;

\[\frac{\partial \phi_y}{\partial \eta_y} = \frac{(e^{\eta_y}) \sum_{l=1}^{k} e^{\eta_l} - e^{\eta_y} (e^{\eta_y})}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2}\]

\[= \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}} - \frac{e^{\eta_y} e^{\eta_y}}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2} = \phi_y - \phi_y \cdot \phi_y\]

\[\frac{\partial \phi_y}{\partial \eta_y} = \phi_y (1 - \phi_y)\]

&lt;p&gt;&lt;strong&gt;Case 2: \(j \ne y\) (计算 \(\phi_y\) 对 \(\eta_j\) 的导数)&lt;/strong&gt;&lt;/p&gt;

\[\frac{\partial \phi_y}{\partial \eta_j} = \frac{(0) \sum_{l=1}^{k} e^{\eta_l} - e^{\eta_y} (e^{\eta_j})}{\left(\sum_{l=1}^{k} e^{\eta_l}\right)^2}\]

\[= - \frac{e^{\eta_y}}{\sum_{l=1}^{k} e^{\eta_l}} \cdot \frac{e^{\eta_j}}{\sum_{l=1}^{k} e^{\eta_l}} = - \phi_y \cdot \phi_j\]

\[\frac{\partial \phi_y}{\partial \eta_j} = - \phi_y \phi_j\]

&lt;hr /&gt;

&lt;p&gt;现在，将 A, B, C 代回链式法则，同样分 \(j=y\) 和 \(j \ne y\) 两种情况。&lt;/p&gt;

\[\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial J(\theta)}{\partial \phi_y} \cdot \frac{\partial \phi_y}{\partial \eta_j} \cdot \frac{\partial \eta_j}{\partial \theta_j}\]

&lt;p&gt;&lt;strong&gt;Case 1: \(j = y\) (真实类别的参数梯度)&lt;/strong&gt;&lt;/p&gt;

\[\nabla_{\theta_y} J(\theta) = \underbrace{\left(- \frac{1}{\phi_y}\right)}_{\text{A}} \cdot \underbrace{\left(\phi_y (1 - \phi_y)\right)}_{\text{C1}} \cdot \underbrace{(x)}_{\text{B}}\]

\[= - (1 - \phi_y) x = (\phi_y - 1) x\]

&lt;p&gt;&lt;strong&gt;Case 2: \(j \ne y\) (非真实类别的参数梯度)&lt;/strong&gt;&lt;/p&gt;

\[\nabla_{\theta_j} J(\theta) = \underbrace{\left(- \frac{1}{\phi_y}\right)}_{\text{A}} \cdot \underbrace{\left(- \phi_y \phi_j\right)}_{\text{C2}} \cdot \underbrace{(x)}_{\text{B}}\]

\[= \phi_j x\]

&lt;blockquote&gt;
  &lt;p&gt;可以用一个统一的公式来表达梯度 \(\nabla_{\theta_j} J(\theta)\)：&lt;/p&gt;

\[\nabla_{\theta_j} J(\theta) = (\phi_j - \mathbb{I}\{y=j\}) x\]

  &lt;p&gt;其中，\(\mathbb{I}\{y=j\}\) 是指示函数：如果 \(j\) 是真实类别 \(y\)，则为1；否则为0。&lt;/p&gt;

  &lt;p&gt;【注意】括号内的项 \((\phi_j - \mathbb{I}\{y=j\})\) 是&lt;strong&gt;预测概率&lt;/strong&gt;和&lt;strong&gt;真实标签&lt;/strong&gt;之间的&lt;strong&gt;误差&lt;/strong&gt;。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;如果 \(j\) 是真实类别 \(y\)：&lt;/strong&gt; 误差是 \((\phi_y - 1)\)。由于 \(\phi_y &amp;lt; 1\)，误差是负的。梯度下降会向&lt;strong&gt;增加&lt;/strong&gt; \(\phi_y\) 的方向调整 \(\theta_y\)。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;如果 \(j\) 不是真实类别 \(y\)：&lt;/strong&gt; 误差是 \((\phi_j - 0) = \phi_j\)。梯度是正的。梯度下降会向&lt;strong&gt;减少&lt;/strong&gt; \(\phi_j\) 的方向调整 \(\theta_j\)。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;模型的参数 \(\theta_j\) 的更新量正比于：&lt;/p&gt;

\[\text{误差} \times \text{输入特征}\]

&lt;p&gt;&lt;strong&gt;这个公式与线性回归和逻辑回归的梯度公式形式惊人地相似，展现了 GLM 框架的统一性。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;有了这个梯度，我们就可以使用梯度下降（或其变体）来迭代更新所有 \(\theta_j\) 向量，直到损失函数收敛到最小值。&lt;/p&gt;

&lt;h2 id=&quot;案例&quot;&gt;案例&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.optim&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 1. 数据加载与预处理 ---
# 定义转换：将图像转换为张量，并归一化到 [0, 1]
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 将 PIL Image 或 NumPy ndarray 转换为 Tensor
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# PyTorch 的 ToTensor 默认会将像素值除以 255.0，实现归一化
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 加载 FashionMNIST 数据集
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;./data&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mnist_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;./data&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 数据读取
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 标签对应的服饰名字
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_text_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;t-shirt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;trouser&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;pullover&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dress,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;coat&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sandal&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shirt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sneaker&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;bag&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ankle boot&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# label 是一个 Tensor 或 NumPy 数组
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 2. 定义模型 ---
# 在 PyTorch 中，我们使用 nn.Module 来定义模型。
# Softmax 回归本质上是一个线性层。
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SoftmaxRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SoftmaxRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 784 (输入特征) -&amp;gt; 10 (输出类别)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# x 的形状是 (batch_size, 1, 28, 28)
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 我们需要将其展平为 (batch_size, 784)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;c1&quot;&gt;# 然后通过线性层 (这一步包含了 wx + b)
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 初始化模型
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 28 * 28
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SoftmaxRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 3. 定义损失函数和优化器 ---
# PyTorch 的 nn.CrossEntropyLoss 包含了 Softmax 运算和交叉熵计算，因此
# 模型的 forward 函数只需要输出未经 Softmax 的原始得分 (logits)。
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 损失函数: 交叉熵 (会自动应用 Softmax)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# 优化器: 随机梯度下降 (SGD)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 4. 辅助函数 ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;evaluate_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;acc_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 在测试阶段禁用梯度计算
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# net(X) 返回 logits (未经 Softmax 的得分)
&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# argmax(dim=1) 找到概率最高的类别
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;acc_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 5. 训练模型 ---
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;开始训练...&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_loss_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_acc_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 清除梯度
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
        
        &lt;span class=&quot;c1&quot;&gt;# 前向传播 (计算预测的 logits)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 计算损失
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 反向传播 (计算梯度)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 更新模型参数
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 统计训练指标
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;train_loss_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_acc_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 模型训练完之后进行测试
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;evaluate_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Epoch %d. Loss: %f, Train acc %f, Test acc %f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loss_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_acc_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;训练完成。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 6. 对新的样本进行标签预测 ---
# 取测试集前 9 个样本
# --- 6. 对新的样本进行标签预测 ---
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 从数据集中提取前 num_samples 个样本
# mnist_test[i] 返回的是 (Image Tensor, label scalar)
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 存储提取出的图像和标签
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;true_labels_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_scalar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;true_labels_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label_scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 将图像列表堆叠成一个大的 Tensor (batch_size, channels, height, width)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# 将标签列表转换为 Tensor
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_labels_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true labels:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 使用新的 label Tensor
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_text_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 禁用梯度，进行预测
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 确保模型使用的是展平的输入 (784)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;y_hat_logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;c1&quot;&gt;# 找到预测概率最高的类别
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;predicted_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_hat_logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;predicted labels:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_text_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251118205831477.png&quot; alt=&quot;image-20251118205831477&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;6逻辑回归的线性不可分&quot;&gt;6.逻辑回归的线性不可分&lt;/h1&gt;

&lt;p&gt;例子：有4个坐标(0,0),(0,1),(1,0),(1,1)，每个坐标对应了一个图形，能找得到一个边界区分下面的2类图形吗？&lt;/p&gt;

&lt;p&gt;答：找不到。这就是&lt;strong&gt;线性不可分&lt;/strong&gt;。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;实际工程的项目，一定是线性不可分的场景更加多。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251119004243945.png&quot; alt=&quot;image-20251119004243945&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;线性可分是指：在一个 $D$ 维特征空间中，如果存在一个 $D-1$ 维的超平面（例如，在二维空间中是一条直线），能够将两类样本完全、准确地划分开，那么称这两类样本是&lt;strong&gt;线性可分&lt;/strong&gt;的。&lt;/p&gt;

&lt;p&gt;上图是一个异或问题 (XOR)。这个问题的逻辑是：&lt;strong&gt;当且仅当 \(x_1\) 和 \(x_2\) 不同时，类别为 1 (三角形)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;此时用逻辑回归就不能解决这个问题了。逻辑回归（以及其他任何&lt;strong&gt;线性分类器&lt;/strong&gt;，如感知机 Perceptron）的核心是找到一个线性决策边界 \(\mathbf{w}^T \mathbf{x} + b = 0\)。由于 &lt;strong&gt;XOR 问题是线性不可分的&lt;/strong&gt;，这个模型无论如何优化权重 \(\mathbf{w}\) 和偏置 \(b\)，都无法找到一条直线来正确地对所有四个点进行分类。&lt;/p&gt;

&lt;p&gt;要解决像 XOR 这样的线性不可分问题，我们必须引入&lt;strong&gt;非线性&lt;/strong&gt;。主要有两种策略：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;策略 A：特征变换 (Feature Transformation)&lt;/li&gt;
  &lt;li&gt;策略 B：引入非线性模型 (多层神经网络)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;61策略a-特征变换&quot;&gt;6.1策略A-特征变换&lt;/h2&gt;

&lt;p&gt;特征变化是最直接的方法，目标是将数据映射到一个&lt;strong&gt;更高维的空间&lt;/strong&gt;，在这个新空间中，数据变得线性可分。&lt;/p&gt;

&lt;p&gt;核技巧的思路：&lt;strong&gt;原始特征：&lt;/strong&gt; \(\mathbf{x} = (x_1, x_2)\)，&lt;strong&gt;添加非线性特征，&lt;/strong&gt; 即引入一个乘积特征 \(x_3 = x_1 x_2\)，最终的&lt;strong&gt;新特征空间 \(\Phi(\mathbf{x})\)：&lt;/strong&gt; \((x_1, x_2, x_3)\)&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;x=(x1,x2)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;x3=x1x2&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Φ(x)=(x1,x2,x3)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;类别&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$(0, 0)$&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;$(0, 0, 0)$&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$(0, 1)$&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;$(0, 1, 0)$&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$(1, 0)$&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;$(1, 0, 0)$&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$(1, 1)$&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;$(1, 1, 1)$&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;特征由2维变成3维，在这个三维空间中，可以找到一个超平面（一个平面）来区分这两类点，从而解决线性不可分问题。&lt;/p&gt;

&lt;p&gt;与2维相同的思路，寻找一组新的权重 \(\mathbf{w}&apos; = (w_1, w_2, w_3)\) 和偏置 \(b&apos;\)。&lt;/p&gt;

\[\mathbf{w}&apos;^T \Phi(\mathbf{x}) + b&apos; = 0\]

&lt;p&gt;展开形式（即平面的方程）就是：&lt;/p&gt;

\[w_1 x_1 + w_2 x_2 + w_3 (x_1 x_2) + b&apos; = 0\]

&lt;p&gt;寻找最佳平面就是找到最优的参数集合 \((\mathbf{w}&apos;, b&apos;)\)，使得这个平面能够在新空间中将两类样本准确地分开。&lt;/p&gt;

&lt;p&gt;为了训练模型，需要定义一个损失函数 \(L(\mathbf{w}&apos;, b&apos;)\)。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于逻辑回归，使用&lt;strong&gt;二元交叉熵损失&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;对于线性 SVM，使用 &lt;strong&gt;Hinge Loss&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;用逻辑回归解决的代码如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.optim&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 字体设置 ---
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;font.sans-serif&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Noto Sans CJK SC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SimHei&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;axes.unicode_minus&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 1. 准备 XOR 数据 ---
# 原始的 XOR 数据点
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 对应的标签 (0: 圆形, 1: 三角形)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 2. 特征工程: 引入乘积特征 x1*x2 ---
# x3 = x1 * x2
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1_x2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 计算 x1*x2 并增加一个维度
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 拼接原始特征和新特征
# 新特征向量是 (x1, x2, x1*x2)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_extended&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1_x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;扩展后的特征 X_extended:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_extended&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 3. 定义逻辑回归模型 (使用扩展特征) ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LogisticRegressionExtended&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LogisticRegressionExtended&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 3 (输入特征: x1, x2, x1*x2) -&amp;gt; 1 (输出: 概率)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Sigmoid 激活函数通常在 BCEWithLogitsLoss 中隐式包含，
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 但为了 forward 明确输出概率，我们在这里加上
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 模型初始化
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_extended_inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_extended&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 3
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LogisticRegressionExtended&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_extended_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 4. 定义损失函数和优化器 ---
# nn.BCELoss 需要模型直接输出概率 (0到1之间)
# nn.BCEWithLogitsLoss 更稳定，它内部包含 Sigmoid，所以模型forward不需要Sigmoid
# 这里我们的模型forward已经有Sigmoid，所以直接用BCELoss
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BCELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 5. 训练模型 ---
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;开始训练...&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 训练更多轮次以确保收敛
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_extended&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;nf&quot;&gt;if &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Epoch [&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;], Loss: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;训练完成。&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 6. 获取训练后的模型参数 ---
# 提取权重和偏置
# net.linear.weight 是一个 (1, num_inputs) 的张量
# net.linear.bias 是一个 (1,) 的张量
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;最终学习到的权重 w = (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;最终学习到的偏置 b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 7. 可视化结果 ---
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 绘制原始数据点
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;150&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;blue&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edgecolor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;black&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;类别 0 (圆形)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;150&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edgecolor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;black&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;类别 1 (三角形)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 绘制决策边界曲线
# 定义一个网格来评估决策函数
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x2_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;meshgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 计算决策函数的值 (Logits)
# Z = w1*x1 + w2*x2 + w3*x1*x2 + b
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 绘制决策边界 (Z=0 的等高线)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;contour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;levels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;green&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linestyles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;XOR 问题：通过特征扩展实现的非线性决策边界&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$x_1$&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$x_2$&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;xticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;yticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linestyle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 8. 验证预测结果 ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_extended&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predicted_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;模型预测结果:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;真实标签:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;预测概率:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;预测类别:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/Users/apple/Library/Application Support/typora-user-images/image-20251119142409259.png&quot; alt=&quot;image-20251119142409259&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;虽然是三维的，但是画图，看到的是投影的边界，看上去像双曲线。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;以上的特征组合是\(x_1x_2\)，但是特征组合的方式多种多样，可以是\(x_1^2\)，也可以是\(x_2^2\)，…，等等。&lt;/p&gt;

&lt;p&gt;这就是特征工程的任务了，找到最好的某种特征的组合，得到最终的模型，使得分类任务完成的最好。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;另外，要注意，对于0.5的边界，其实也不一定合理，因为&lt;strong&gt;分类器有一定的概率预测错误&lt;/strong&gt;。比如癌症，建议边界卡的低一点。宁可让小概率得癌症的人多检查几次，也不要认为他没有得癌症回家了，这是需要编程自己根据实际情况调整边界的。&lt;/p&gt;

&lt;h2 id=&quot;62策略b-引入非线性模型-多层神经网络&quot;&gt;6.2策略B-引入非线性模型 (多层神经网络)&lt;/h2&gt;

&lt;p&gt;这是现代机器学习中最常用的方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;多层感知机 (MLP)：&lt;/strong&gt; 在输入层和输出层之间添加一个或多个&lt;strong&gt;隐藏层&lt;/strong&gt;，并在隐藏层中引入&lt;strong&gt;非线性激活函数&lt;/strong&gt;（如 ReLU, Sigmoid）。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;工作原理：&lt;/strong&gt; 每个隐藏层可以被视为一个自动学习特征变换 Φ(x) 的模块。通过堆叠多个非线性层，模型能够学习到任意复杂的非线性决策边界，轻松解决 XOR 等问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;7glm-模型的梯度更新形式都具有高度的统一性&quot;&gt;7.GLM 模型的梯度更新形式都具有高度的统一性&lt;/h1&gt;

&lt;p&gt;来详细对比 Softmax 回归、逻辑回归和线性回归的&lt;strong&gt;单个样本&lt;/strong&gt;的参数更新公式，看看它们的相似之处。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;广义线性模型（GLM）理论中最核心的洞察之一：&lt;strong&gt;所有 GLM 模型的梯度更新形式都具有高度的统一性。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有三种模型的参数更新都是基于 梯度下降法 的迭代更新规则：&lt;/p&gt;

\[\theta_{\text{new}} = \theta_{\text{old}} - \alpha \cdot \nabla_{\theta} J(\theta)\]

&lt;p&gt;其中 \(\alpha\) 是学习率，\(\nabla_{\theta} J(\theta)\) 是梯度（损失函数 \(J(\theta)\) 对参数 \(\theta\) 的偏导数）。&lt;/p&gt;

&lt;p&gt;我们关注的重点是&lt;strong&gt;梯度项 \(\nabla_{\theta} J(\theta)\)&lt;/strong&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Softmax 回归 (多分类)是多分类交叉熵损失 (负对数似然)。梯度公式如下：&lt;/p&gt;

\[\nabla_{\theta_j} J(\theta) = \underbrace{(\phi_j - \mathbb{I}\{y=j\})}_{\text{误差项}} \cdot \underbrace{x}_{\text{输入特征}}\]

&lt;p&gt;逻辑回归 (二分类)是 Softmax 回归在 \(k=2\) 时的特例。它只学习一个参数向量 \(\theta\)（通常 \(\theta_1\)）。损失函数是二元交叉熵损失 (BCE)。梯度公式如下：&lt;/p&gt;

\[\nabla_{\theta} J(\theta) = \underbrace{(\phi - y)}_{\text{误差项}} \cdot \underbrace{x}_{\text{输入特征}}\]

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;其中，\(\phi = h_{\theta}(x)\) 是模型预测 $$P(y=1&lt;/td&gt;
      &lt;td&gt;x)\(的概率，而\)y$$ 是真实标签（0 或 1）。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;线性回归 (连续值预测)的损失函数是MSE（均方误差），梯度公式是：&lt;/p&gt;

\[\nabla_{\theta} J(\theta) = \underbrace{(h_{\theta}(x) - y)}_{\text{误差项}} \cdot \underbrace{x}_{\text{输入特征}}\]

&lt;p&gt;其中，\(h_{\theta}(x) = \theta^T x\) 是模型的预测值，\(y\) 是真实值。&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;损失函数 J(θ)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;梯度 ∇θJ(θ)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;误差项 (预测−真实)&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;基础分布&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;MSE&lt;/td&gt;
      &lt;td&gt;\((h_{\theta}(x) - y) x\)&lt;/td&gt;
      &lt;td&gt;(预测值-真实值)&lt;/td&gt;
      &lt;td&gt;高斯分布&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;BCE&lt;/td&gt;
      &lt;td&gt;\((\phi - y) x\)&lt;/td&gt;
      &lt;td&gt;\((\text{预测概率} - \text{真实标签})\)&lt;/td&gt;
      &lt;td&gt;伯努利分布&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Softmax 回归&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;交叉熵&lt;/td&gt;
      &lt;td&gt;\((\phi_j - \mathbb{I}\{y=j\}) x\)&lt;/td&gt;
      &lt;td&gt;\((\text{预测概率} - \text{真实标签})\)&lt;/td&gt;
      &lt;td&gt;多项分布&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;所有 GLM 模型的梯度更新都遵循以下结构：&lt;/strong&gt;&lt;/p&gt;

\[\nabla_{\theta} J(\theta) \propto (\text{预测值} - \text{观测值}) \times \text{输入特征}\]

&lt;h1 id=&quot;8正向传播与反向传播&quot;&gt;8.正向传播与反向传播&lt;/h1&gt;

&lt;p&gt;其实表达式计算的过程就是正向传播，利用梯度下降反复更新参数（权重）就是反向传播。&lt;/p&gt;

&lt;p&gt;不论是线性回归、逻辑回归、softmax回归，其实都已经有了正向传播、反向传播的概念。&lt;/p&gt;

&lt;p&gt;在多层神经网络出现之前，这些模型就被称为&lt;strong&gt;“单层网络”&lt;/strong&gt;。它们的反向传播过程之所以看起来简单，是因为它们只有&lt;strong&gt;一个计算层&lt;/strong&gt;（线性组合 \(\theta^T x\)）和一个&lt;strong&gt;激活函数&lt;/strong&gt;（Sigmoid 或 Softmax）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;正向传播：&lt;/strong&gt; \(x \to \theta^T x \to h(\theta^T x) \to J(\theta)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;反向传播：&lt;/strong&gt; \(J(\theta) \to \nabla_{\theta} J\) &lt;strong&gt;(通过链式法则) \(\to \theta_{\text{new}}\)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这两种传播机制是所有基于梯度学习的模型（包括深度学习）的根本。&lt;/p&gt;

&lt;h1 id=&quot;9采样的重要性&quot;&gt;9.采样的重要性&lt;/h1&gt;

&lt;p&gt;如果数据集极度不平衡（例如，欺诈检测、罕见疾病诊断、广告点击率），如果不进行采样，模型在训练时会过度关注&lt;strong&gt;多数类 0&lt;/strong&gt;。模型会发现，只要它预测所有样本都是0，它就能达到一个&lt;strong&gt;看似很高&lt;/strong&gt;的准确率（例如 99.9%），因为0的样本占了绝大多数。&lt;/p&gt;

&lt;p&gt;虽然整体准确率高，但模型实际上失去了识别&lt;strong&gt;少数类 1&lt;/strong&gt;（例如，真正欺诈的交易、真正患病的人）的能力。这在关键业务场景中是不可接受的。&lt;/p&gt;

&lt;p&gt;所以需要采样，上采样（复制少数类样本）或下采样（减少多数类样本），有效提高少数类在损失计算中的权重，使其梯度能够充分指导模型的参数更新。&lt;/p&gt;

&lt;h1 id=&quot;总结重点&quot;&gt;总结重点&lt;/h1&gt;

&lt;p&gt;1.逻辑回归损失函数与梯度下降，手推数学公式，即求导。说明逻辑回归迭代的过程。&lt;/p&gt;

&lt;p&gt;2.KL距离（KL散度）与BCE之间的关系。并解释下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251118174428582.png&quot; alt=&quot;image-20251118174428582&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3.为什么逻辑回归不用MSE而用BCE？&lt;/p&gt;

&lt;p&gt;4.softmax&lt;/p&gt;

&lt;h1 id=&quot;补充-colab画图之显示中文字体&quot;&gt;补充-colab画图之显示中文字体&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.font_manager&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 1. 设置变量 ---
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FONT_FILENAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;NotoSansCJKsc-Regular.otf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;DOWNLOAD_URL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Regular.otf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# 找出 Matplotlib 的缓存/配置目录（兼容性写法）
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cache_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_cachedir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;AttributeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cache_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_configdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# 定义下载和目标路径
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FONT_FILENAME&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;font_destination&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FONT_FILENAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;目标缓存目录: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache_dir&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 2. 检查并下载字体 ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;正在重新下载 &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FONT_FILENAME&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 强制重新下载，覆盖可能存在的空文件
&lt;/span&gt;    &lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wget&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DOWNLOAD_URL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;O&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 检查下载是否成功
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 字体文件通常大于 1MB
&lt;/span&gt;    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;✅ 文件下载成功，大小：&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; MB&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# --- 3. 复制字体文件到目标路径 ---
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 使用 shutil.copy 以确保复制成功
&lt;/span&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shutil&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shutil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;copyfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;font_destination&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;✅ &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FONT_FILENAME&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; 已成功复制到目标目录。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# --- 4. 清理并重建 Matplotlib 缓存（关键） ---
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 清除旧的字体缓存文件（如果存在）
&lt;/span&gt;    &lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 强制 Matplotlib 重新扫描并构建缓存
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;_load_fontmanager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;try_read_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;✅ 字体管理器已强制重新加载和构建缓存。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;❌ 错误：下载失败或文件大小异常 (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; 字节)。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 验证字体是否被管理器识别
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Noto Sans CJK SC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fontManager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ttflist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;✅ 验证成功：Matplotlib 字体管理器已识别 &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Noto Sans CJK SC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;❌ 验证失败：字体管理器未识别该字体。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如果验证失败，可以重启会话之后再执行一次。&lt;/p&gt;

&lt;p&gt;如何还是显示“验证失败”，这通常是因为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;font_manager&lt;/code&gt; 模块在重新初始化时，并没有真正将复制到缓存目录的文件视为有效的字体文件，或者读取路径有问题。可以尝试运行下面代码：Python 代码强制指定 Matplotlib 使用这个字体文件，绕过自动识别过程。（注意，最好还是重启下）&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.font_manager&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 1. 定义字体文件路径 ---
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FONT_FILENAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;NotoSansCJKsc-Regular.otf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 找出 Matplotlib 的缓存/配置目录
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cache_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_cachedir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;AttributeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cache_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_configdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
&lt;span class=&quot;c1&quot;&gt;# 目标路径就是我们复制到的路径
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FONT_FILENAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 2. 尝试将字体手动添加到字体管理器 ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 使用 fontManager.addfont 明确告诉 Matplotlib 这个文件是一个字体
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fontManager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;addfont&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;✅ 字体文件 &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FONT_FILENAME&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; 已通过 addfont 明确添加到管理器。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;❌ 字体手动添加失败: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 3. 强制设置参数 (使用文件名作为字体名称) ---
# Matplotlib 有时会使用文件名作为字体名，我们使用正确的字体名 &apos;Noto Sans CJK SC&apos;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Noto Sans CJK SC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# 检查字体是否已被识别 (重新检查，确保 addfont 有效)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;font_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fontManager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ttflist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;✅ 验证成功：Matplotlib 字体管理器已识别 &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Noto Sans CJK SC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 强制设置字体参数
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;font.sans-serif&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SimHei&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 将其设置为首选
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;axes.unicode_minus&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;✅ Matplotlib 默认字体已设置为 &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# --- 4. 运行一个快速测试图 ---
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;中文显示测试&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;测试文本：逻辑回归&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;va&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;❌ 最终验证失败：字体管理器未识别 &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;。请确保安装代码运行完整。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如果没有问题会显示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251119142127271.png&quot; alt=&quot;image-20251119142127271&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以画sigmoid函数为例：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.font_manager&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 【关键修复部分：强制刷新字体管理器】 ---
# 这一步尝试强制 Matplotlib 重新加载字体缓存，以确保新安装的字体生效。
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 确保 mpl 模块被正确导入
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpl&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 尝试重建字体缓存（如果 Matplotlib 版本支持）
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 强制重新加载字体管理器，不使用缓存
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;_load_fontmanager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;try_read_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ℹ️ Matplotlib 字体管理器已强制刷新。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 打印错误，但允许代码继续运行，以防版本兼容问题
&lt;/span&gt;    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;⚠️ 字体管理器刷新失败: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 【设置中文字体】 ---
# 使用思源黑体作为首选，并保留其他常用字体作为备用
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;font.sans-serif&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Noto Sans CJK SC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;WenQuanYi Zen Hei&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SimHei&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;axes.unicode_minus&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# 解决负号显示的问题
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ℹ️ 当前设置的字体列表: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;font.sans-serif&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# --- 【验证字体是否可用】 ---
# 尝试查找首选字体，如果失败，会发出警告
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;findfont&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;font.sans-serif&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fallback_to_default&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;✅ 中文字体验证通过。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;❌ 警告：首选中文字体仍未找到，可能显示乱码。&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# --- 【绘图逻辑】 ---
# 生成 x 值范围
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 计算 Sigmoid 函数
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 计算 Sigmoid 的导数：σ&apos;(x) = σ(x)*(1-σ(x))
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 绘图
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sigmoid 函数: σ(x) = 1/(1+e^{-x})&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;blue&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sigmoid 导数: σ&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;(x) = σ(x)(1-σ(x))&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linestyle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 添加标题和标签
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sigmoid 函数及其导数&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 添加网格、图例、零轴线
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linestyle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y=0.5&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linestyle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linestyle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 设置坐标轴范围
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 显示图像
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251118191312829.png&quot; alt=&quot;image-20251118191312829&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9906/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/20/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9906/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
      <item>
        <title>【AI思想启蒙05】逻辑回归1猛将起于卒伍，工业环境下的分类模型 </title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251117162736025.png&quot; alt=&quot;image-20251117162736025&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;逻辑回归是线性回归的扩展：&lt;/strong&gt; 它在线性回归的输出上应用了 Sigmoid 变换。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;逻辑回归是最简单的单层神经网络：&lt;/strong&gt; 它相当于一个&lt;strong&gt;只有输入层和输出层（带 Sigmoid 激活函数）&lt;/strong&gt;，并使用&lt;strong&gt;二元交叉熵 (BCE) 损失&lt;/strong&gt;的神经网络。&lt;/p&gt;

&lt;p&gt;当神经网络只有一层（没有隐藏层）时，如果输出层是 Sigmoid 激活函数，它就是逻辑回归；如果输出层是恒等激活函数，它就是线性回归。&lt;/p&gt;

&lt;p&gt;深度学习通过引入&lt;strong&gt;隐藏层&lt;/strong&gt;和使用 &lt;strong&gt;ReLU、Tanh&lt;/strong&gt; 等&lt;strong&gt;非线性激活函数&lt;/strong&gt;，能够学习和建模数据中高度复杂的&lt;strong&gt;非线性&lt;/strong&gt;关系，这是线性回归和逻辑回归作为单层模型所无法实现的。&lt;/p&gt;

&lt;h1 id=&quot;1回顾线性回归&quot;&gt;1.回顾线性回归&lt;/h1&gt;

&lt;p&gt;线性回归是最基础的回归模型，它假设输入特征 \(X\) 和输出目标 \(Y\) 之间存在&lt;strong&gt;线性关系&lt;/strong&gt;。&lt;/p&gt;

\[Y = W^T X + b\]

&lt;p&gt;线性回归是&lt;strong&gt;最简单的神经网络&lt;/strong&gt;。如果一个神经网络&lt;strong&gt;只有输入层和输出层&lt;/strong&gt;，并且&lt;strong&gt;输出层没有激活函数&lt;/strong&gt;（即使用恒等函数 \(f(z)=z\)），那么它执行的操作就是线性回归。&lt;/p&gt;

&lt;p&gt;线性回归的核心在于预测连续数值。建立输入特征（即自变量 \(X\)）与连续输出目标（未知的“另一个坐标”，即因变量 \(Y\)）之间的线性关系。其目标是找到一条最佳拟合的直线或平面，使得模型可以根据输入的 \(X\) 值，直接预测出一个具体、连续的数值，例如预测房价、气温或股票价格，解决的是&lt;strong&gt;回归问题&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;而&lt;strong&gt;逻辑回归的核心在于确定相对位置以进行分类。&lt;/strong&gt; 逻辑回归：知道完整的坐标，计算和直线的相对位置，这就是逻辑回归的分类本质。这里的“直线”是模型的&lt;strong&gt;决策边界&lt;/strong&gt;。逻辑回归首先通过线性计算确定数据点在线性空间中的位置（知道完整的坐标），然后通过 Sigmoid 函数将数据点与决策边界的相对位置转换为一个 \([0, 1]\) 之间的概率值。这个概率值决定了数据点被分到某一类别的可能性，从而解决了&lt;strong&gt;分类问题&lt;/strong&gt;，实现了对离散类别的预测。&lt;/p&gt;

&lt;h1 id=&quot;2逻辑回归&quot;&gt;2.逻辑回归&lt;/h1&gt;

&lt;h2 id=&quot;21原理&quot;&gt;2.1原理&lt;/h2&gt;

&lt;p&gt;线性 + Sigmoid 非线性 = 逻辑回归&lt;/p&gt;

&lt;p&gt;逻辑回归主要用于&lt;strong&gt;二分类&lt;/strong&gt;任务。它在&lt;strong&gt;线性组合&lt;/strong&gt;的基础上，增加了一个 &lt;strong&gt;Sigmoid (S型)&lt;/strong&gt; 激活函数，将输出压缩到 \([0, 1]\) 之间，表示概率。&lt;/p&gt;

\[P(Y=1|X) = \sigma(W^T X + b)\]

&lt;p&gt;其中 \(\sigma(z) = \frac{1}{1 + e^{-z}}\) 是 Sigmoid 函数。&lt;/p&gt;

&lt;p&gt;sigmoid函数图像如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251117173055512.png&quot; alt=&quot;image-20251117173055512&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;为什么是sigmoid函数（或类似的 S 型曲线）？&lt;/p&gt;

  &lt;p&gt;1.虽然逻辑回归本身是线性分类器，但 Sigmoid 函数作为&lt;strong&gt;非线性激活函数&lt;/strong&gt;，使得它能够将线性模型的输出转化为非线性概率。更重要的是，在&lt;strong&gt;深度神经网络&lt;/strong&gt;中，正是非线性激活函数的堆叠（如 Sigmoid）赋予了网络学习和拟合复杂非线性关系的能力。如果没有非线性激活函数，无论堆叠多少层，神经网络仍然只会是一个线性模型。补充：sigmoid作为神经网络的激活函数并不常见。因为在曲线平坦的区域，Sigmoid 的导数（梯度）接近于零。这意味着当模型的输出得分过大或过小时，通过反向传播计算出的梯度会非常小，导致权重 \(W\) 的更新非常缓慢，使得模型训练&lt;strong&gt;停滞不前&lt;/strong&gt;。这也是在现代深度学习中，ReLU 及其变体更常被用作隐藏层激活函数的原因。&lt;/p&gt;

  &lt;p&gt;2.整个曲线是&lt;strong&gt;平滑且可导&lt;/strong&gt;的（没有尖锐的拐角或跳变）。Sigmoid 函数处处可导，其导数（即梯度）也是连续的。这对于使用&lt;strong&gt;梯度下降法&lt;/strong&gt;及其变体（如反向传播）来训练模型至关重要，因为优化算法需要平滑的梯度来稳定地更新模型的权重 \(W\)。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;sigmoid导数图像如下：\(\sigma&apos;(z) = \sigma(z) \cdot (1 - \sigma(z))\)，其中\(\sigma(z) = \frac{1}{1 + e^{-z}}\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251117173138335.png&quot; alt=&quot;image-20251117173138335&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sigmoid 导函数的图像是一个&lt;strong&gt;对称的钟形曲线&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;kl散度&quot;&gt;KL散度&lt;/h3&gt;

&lt;p&gt;线性回归的损失函数是MSE，逻辑回归的损失函数能不能也是MSE？&lt;/p&gt;

&lt;p&gt;不能。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;KL 距离，正式名称为 &lt;strong&gt;Kullback-Leibler 散度&lt;/strong&gt;，是一种衡量&lt;strong&gt;两个概率分布之间差异&lt;/strong&gt;的非对称度量。它源自信息论。&lt;/p&gt;

&lt;p&gt;KL 散度的定义如下：&lt;/p&gt;

\[\mathcal{L}_{KL} = \sum_{i=1}^{N} P(x_i) \cdot \log \frac{P(x_i)}{Q(x_i)}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;\(P(x_i)\) (或图中的 \(y_i\))&lt;/strong&gt;: 真实概率分布（或参考分布 \(P\)）中事件 \(x_i\) 发生的概率。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(Q(x_i)\) (或图中的 \(f(x_i)\))&lt;/strong&gt;: 预测概率分布（或近似分布 \(Q\)）中事件 \(x_i\) 发生的概率。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(N\)&lt;/strong&gt;: 离散事件或状态的总数（在分类中通常是类别数）。&lt;/li&gt;
  &lt;li&gt;衡量使用分布 \(Q\) 来近似分布 \(P\) 时，所&lt;strong&gt;损失的信息量&lt;/strong&gt;或引入的&lt;strong&gt;信息增益&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举例说明：&lt;/p&gt;

&lt;p&gt;有2枚硬币P和Q，抛硬币向上和向下的概率如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;P&lt;/th&gt;
      &lt;th&gt;Q&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;正面朝上&lt;/td&gt;
      &lt;td&gt;\(\frac{1}{3}\)&lt;/td&gt;
      &lt;td&gt;\(\frac{1}{4}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;反面朝上&lt;/td&gt;
      &lt;td&gt;\(\frac{2}{3}\)&lt;/td&gt;
      &lt;td&gt;\(\frac{3}{4}\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;计算硬币 \(\mathbf{P}\) 相对于 \(\mathbf{Q}\) 的 KL 散度：$$D_{KL}(P&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Q)$$。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[D_{KL}(P || Q) = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)}=P(x_1) \log \frac{P(x_1)}{Q(x_1)}+P(x_2) \log \frac{P(x_2)}{Q(x_2)} = \frac{1}{3} \ln \left( \frac{1/3}{1/4} \right) + \frac{2}{3} \ln \left( \frac{2/3}{3/4} \right)\]

&lt;p&gt;KL 散度的&lt;strong&gt;性质:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;非对称性:&lt;/strong&gt; $$D_{KL}(P&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;Q) \neq D_{KL}(Q&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;P)\(。它衡量的是\)P\(相对于\)Q$$ 的散度。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;非负性:&lt;/strong&gt; $$D_{KL}(P&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;Q) \geq 0\(。只有当\)P\(和\)Q$$ 完全相同时，$D_{KL}(P&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;Q) = 0$。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;一个很重要且有难度的结论：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;散度表达式非对称导致拟合倾向不同&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251117212909037.png&quot; alt=&quot;image-20251117212909037&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;**$$KL(P&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Q)\(** 倾向于让\)Q\(**覆盖**\)P\(的所有模式（**零回避**），因此\)Q\(会尽可能匹配\)P\(的**所有大值区域**，避免在\)P$$ 存在的地方 $Q$ 为零。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;**$$KL(Q&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;P)\(** 倾向于让\)Q\(**聚焦**于\)P\(的主要模式（**零强制**），因此\)Q\(会忽略\)P\(的长尾或不重要的区域，致力于完美匹配\)P\(的**少数几个峰值**，避免在\)P\(不存在的区域\)Q$$ 仍有值。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;损失函数bce&quot;&gt;损失函数BCE&lt;/h3&gt;

&lt;p&gt;逻辑回归的标准损失函数：二元交叉熵 (BCE)&lt;/p&gt;

&lt;p&gt;逻辑回归的训练目标是最大化似然函数，这等价于最小化&lt;strong&gt;二元交叉熵 (Binary Cross Entropy, BCE)&lt;/strong&gt; 损失函数：&lt;/p&gt;

\[\mathcal{L}_{\text{BCE}}(P || Q) = - \sum_{i} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;真实分布 \(P\) (True Distribution):&lt;/strong&gt; 由真实标签 \(y_i\) 定义，它是一个伯努利分布，概率质量集中在 \(y_i\)上。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;预测分布 \(Q\) (Predicted Distribution):&lt;/strong&gt; 由模型输出 \(\hat{y}_i\) 定义，它也是一个伯努利分布，正类概率为 $\hat{y}_i$，负类概率为 \(1 - \hat{y}_i\)。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;KL 散度 $$D_{KL}(P&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Q)$$ 定义为：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[D_{KL}(P || Q) = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)} = \sum_{i} P(x_i) \log P(x_i) - \sum_{i} P(x_i) \log Q(x_i)\]

&lt;p&gt;在二分类问题中，对于单个样本：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;真实分布 \(P\) 的熵 \(H(P)\)：&lt;/p&gt;

\[H(P) = - \sum_{k \in \{0, 1\}} P(Y=k) \log P(Y=k) = - [y \log y + (1 - y) \log (1 - y)]\]

    &lt;p&gt;由于 \(y\) 是真实标签，只能取 0 或 1，所以 \(H(P) = 0\)（确定分布的熵为零）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;BCE 损失 $$\mathcal{L}_{\text{BCE}}(P&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;Q)$$：&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

\[\mathcal{L}_{\text{BCE}}(P || Q) = - \sum_{k \in \{0, 1\}} P(Y=k) \log Q(Y=k) = - [y \log \hat{y} + (1 - y) \log (1 - \hat{y})]=\\ \sum_{i=1}^{n} \left[ y_i \log \frac{y_i}{f_i} + (1 - y_i) \log \frac{1 - y_i}{1 - f_i} \right]\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;KL 散度 $$D_{KL}(P&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;Q)$$：&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

\[\mathbf{D_{KL}(P || Q)} = \mathcal{L}_{\text{BCE}}(P || Q) - H(P)\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由于在逻辑回归中，目标是真实标签 \(y_i\)，其分布 \(P\) 是确定性的（熵 \(H(P)=0\)），所以：&lt;/p&gt;

\[\mathbf{D_{KL}(P || Q)} = \mathcal{L}_{\text{BCE}}(P || Q)\]

&lt;p&gt;因此，&lt;strong&gt;最小化逻辑回归的 BCE 损失，在数学上完全等价于最小化真实分布 \(P\) 和预测分布 \(Q\) 之间的 KL 散度。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;22牛客习题1&quot;&gt;2.2牛客习题1&lt;/h2&gt;

&lt;p&gt;https://www.nowcoder.com/practice/3718cf46430740c7bbb6cd31fc433b88?tpId=390&amp;amp;tqId=11507519&amp;amp;sourceUrl=%2Fexam%2Foj%2Fta%3Fpage%3D1%26tpId%3D37%26type%3D390%26channelPut%3Dw25post&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decimal&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Decimal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ROUND_HALF_UP&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 1. Sigmoid 函数 ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;计算 Sigmoid 激活函数值&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# np.clip 避免指数溢出
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# --- 2. 预测函数 ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;计算预测概率和标签&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 线性组合：Z = X @ W (包含偏置项的矩阵乘法)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 概率：Y_hat = sigmoid(Z)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 预测标签：概率 &amp;gt;= 0.5 为 1，否则为 0
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# --- 3. 损失函数 (平均交叉熵 + L2 正则) ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compute_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;计算平均交叉熵损失和 L2 正则化项&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 避免 log(0)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 交叉熵损失
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;cross_entropy_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# L2 正则化项 (通常不正则化偏置项 W[0])
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 注意：W[1:] 对应 w1, w2, w3
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# --- 4. 批量梯度下降训练 ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;使用批量梯度下降训练逻辑回归模型&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 初始化所有权重 W 为零 (包含 w0, 即 bias)
&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 前向传播：计算预测概率
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 反向传播：计算梯度 (包含正则项)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 梯度 for 所有权重 (包括偏置 W[0])
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 添加 L2 正则项的梯度 (只对 W[1:] 添加)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;l2_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;l2_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_grad&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 权重更新
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 计算新的损失并检查收敛
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;new_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compute_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 检查收敛条件
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# print(f&quot;Converged at iteration {i+1}. Loss change: {abs(current_loss - new_loss):.6f}&quot;)
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_loss&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# --- 5. 主程序 ---
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 读取训练参数 (n max_iter alpha lam tol)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 读取训练数据
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;raw_train_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;raw_train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())))&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_train_data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Handle case where n &amp;gt; 0 but data is missing
&lt;/span&gt;            &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Error: Missing training data.&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 转换为 NumPy 数组
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 提取特征 X 和标签 Y
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X_train_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# age, inc, dur
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# label
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# 兼容 n=0 的情况
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;X_train_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# --- 特征标准化 (Z-Score Normalization) ---
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 记录训练集的均值和标准差，用于标准化测试集
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# 避免除以零：将标准差为零的特征的标准差设为 1 (它们不会被缩放)
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;X_train_normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# n=0 时，训练数据为空，标准化系数无关紧要
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X_train_normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 添加偏置项 (Intercept/Bias Term)
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normalized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# --- 训练模型 ---
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# 特殊情况：max_iter=0，权重保持初始值 W=0
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# n=0，权重 W 仍然为零
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# n&amp;gt;0, max_iter=0, W=0 已经在上面处理
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# --- 读取测试数据 ---
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;line_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;raw_test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;raw_test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())))&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X_test_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# 使用训练集的均值和标准差标准化测试集
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;X_test_normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# 添加偏置项
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test_normalized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# --- 进行预测 ---
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# --- 输出结果 ---
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# 按照要求，概率保留四位小数，四舍五入
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;prob_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Decimal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 使用 Decimal 实现精确的四舍五入
&lt;/span&gt;                &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob_str&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# print(f&quot;An error occurred: {e}&quot;, file=sys.stderr)
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 示例环境中通常需要安静失败
&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;__main__&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;23牛客习题2&quot;&gt;2.3牛客习题2&lt;/h2&gt;

&lt;p&gt;https://www.nowcoder.com/practice/d9c4bcf3bc5e426b8a11e690f65ba601?tab=note&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dataSet.csv&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;labels.csv&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#补全 sigmoid 函数功能
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#code start here
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#code end here
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gradientDescent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataMatIn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 学习率，也就是题目描述中的 α
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;iteration_nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 迭代次数，也就是for循环的次数
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;dataMatrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataMatIn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;labelMat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 返回dataMatrix的大小。m为行数,n为列数。
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;weight_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#初始化权重矩阵
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#iteration_nums 即为循环的迭代次数
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#请在代码完善部分注意矩阵乘法的维度，使用梯度下降矢量化公式
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#code start here
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration_nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#数据矩阵与权重矩阵点积，是预测值
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labelMat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#梯度
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;weight_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#权重更新
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_mat&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#code end here
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;__main__&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataMat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labelMat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;gradientDescent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataMat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labelMat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3多分类&quot;&gt;3.多分类&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://wechat01.oss-cn-hangzhou.aliyuncs.com/img/image-20251117163645461.png&quot; alt=&quot;image-20251117163645461&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;使用&lt;strong&gt;逻辑回归的思想&lt;/strong&gt;来处理多分类问题，这种方法通常被称为 &lt;strong&gt;Softmax 回归 (Softmax Regression)&lt;/strong&gt; 或&lt;strong&gt;多项逻辑回归 (Multinomial Logistic Regression)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;对于输入特征向量 \(X\)，模型会为每个类别 \(k\) 计算一个&lt;strong&gt;分数&lt;/strong&gt;（或称为 &lt;strong&gt;logit&lt;/strong&gt;）。这个分数是通过输入 \(X\) 和该类别对应的权重向量 \(W_k\) 进行线性组合得到的。&lt;/p&gt;

\[z_k = W_k^T X + b_k\]

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(k = 1, 2, 3, 4\)（对应图中的 \(P_1\) 到 \(P_4\)）。&lt;/li&gt;
  &lt;li&gt;\(z_k\) 是输入 \(X\) 属于类别 \(k\) 的未归一化分数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这对应了图中 &lt;strong&gt;X&lt;/strong&gt; 经过 &lt;strong&gt;W&lt;/strong&gt; 矩阵变换后的中间结果。&lt;/p&gt;

&lt;p&gt;为了将这些分数 \(z_k\)  转换为符合概率要求的输出（即所有概率值在 \([0, 1]\) 之间，且总和为 1），模型使用 &lt;strong&gt;Softmax 函数&lt;/strong&gt;。Softmax 函数是 &lt;strong&gt;Sigmoid 函数&lt;/strong&gt;（用于二分类逻辑回归）在多分类上的泛化。&lt;/p&gt;

&lt;p&gt;样本 \(X\) 属于类别 \(k\) 的概率 \(P_k\) 为：&lt;/p&gt;

\[P_k = P(Y=k|X) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}\]

&lt;p&gt;这对应了图中 \(P_1, P_2, P_3, P_4\) 的输出，这些输出满足 \(\sum_{k=1}^{4} P_k = 1\)。&lt;/p&gt;

&lt;p&gt;在 Softmax 回归中，我们通常使用&lt;strong&gt;交叉熵损失 (Cross Entropy Loss)&lt;/strong&gt; 来衡量预测概率分布 \(P\) 与真实标签分布 \(Y\) 之间的差异，并指导模型的训练。&lt;/p&gt;

\[\mathcal{L}_{CE} = - \sum_{k=1}^{K} Y_k \log(P_k)\]

&lt;ul&gt;
  &lt;li&gt;\(Y_k\) 是真实标签的 One-hot 编码（如果 \(X\) 属于类别 \(k\)，则 \(Y_k=1\)，否则为 0）。&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate>
        <link>https://kirsten-1.github.io/2025/11/17/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9905/</link>
        <guid isPermaLink="true">https://kirsten-1.github.io/2025/11/17/AI%E6%80%9D%E6%83%B3%E5%90%AF%E8%92%9905/</guid>
        
        <category>AI思想启蒙</category>
        
        
      </item>
    
  </channel>
</rss>
